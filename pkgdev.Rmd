#  (PART) Package Development & Submission {-}


# Package Development {#pkgdev}

The current chapter should be considered an extension of the corresponding
["Guide for Authors"](https://devguide.ropensci.org/guide-for-authors.html) in
rOpenSci's "Dev Guide". The principles for package development described there
also apply to statistical packages, with this chapter describing additional
processes and practices for packages intended for submission to the statistical
software peer review system.

Package developers are expected to sequentially follow the processes described
in the four sub-sections of this chapter. The first step is to determine
whether a package is likely to be considered within the scope of our review
system. The second sub-section describes our [`autotest`
tool](https://github.com/ropenscilabs/autotest), which is intended to be used
through the entire process of package development. The third sub-section
describes how to align software with our [general and category-specific
standards](#standards) for statistical software, and the final sub-section
describes the final step of preparing a "*Life Cycle Statement*". 

Statistical software to be submitted to our peer-review system will need to
have applied the tools described in the second and third sub-sections, and will
contain complete documentation of how and where the software aligns with each
applicable standard. These tools collectively aim to free the review process as
much as possible from technical details, and to facilitate reviews that focus
more on the substantive, qualitative aspects of software. The tools described
in this chapter enable a wealth of technical detail on software performance and
alignment to standards to be extracted at any stage during and beyond review,
while the reviews themselves will then hold an enduring record of the
qualitative aspects of the software.


## Scope

The first important task prior to submitting a package is to estimate whether
a package is likely to be considered within our scope for statistical software.
As described in the [Overview](#overview-scope), packages are generally
considered in scope if they fit one or more of the [categories listed
there](#overview-categories), and in the corresponding standards.

Developers are encouraged to contact us at any point prior to, or during,
development, to ask about whether a package might be in scope, or which
categories it might fit within. Categorisation of packages may not always be
straightforward, and we particularly encourage developers who are unsure about
whether a package belongs in a particular category or not to contact us for
discussion. An initial judgement of whether or not a package belongs in
a particular category may be gained by examining the respective standards. Any
package for which a large number of standards from a particular category may be
considered applicable (regardless of whether or not they would actually be
checked) is likely to fit within that category. Once you have determined that
your package is likely to fit into one or more of our in-scope categories,
you'll need to apply our two primary development tools described in the
following two sub-sections.

## The [`autotest` package](https://github.com/ropenscilabs/autotest) {#pkgdev-autotest}

We have developed an automated assessment tool,
[`autotest`](https://github.com/ropenscilabs/autotest), which all packages are
expected to pass in order to be accepted for submission. The package implements
a form of "mutation testing," by examining the types of all input parameters,
implementing type-specific mutations, and examining the response of each
function in a package to all such mutations. This kind of mutation testing is
a very effective way to uncover any unexpected behaviour which developers
themselves might not necessarily pre-empt. The purpose of using
[`autotest`](https://github.com/ropenscilabs/autotest) to prepare packages is
to avoid as much as possible the common situation of reviewers discovering bugs
when they attempt to use software in ways that differ from typical uses
envisioned by developers themselves. Reviews of software prepared with the help
of [`autotest`](https://github.com/ropenscilabs/autotest) should be less
burdened by discussions of what are often minor technical details, and more
able to focus on "higher level" aspects of software quality.


Full documentation of how to use
[`autotest`](https://github.com/ropenscilabs/autotest) in package development
is provided on the [package website](https://ropenscilabs.github.io/autotest/),
and we particularly encourage any developers intending to develop packages for
submission to our peer review system to step through the [main `autotest`
vignette](https://ropenscilabs.github.io/autotest/articles/autotest.html), and
to apply [`autotest`](https://ropenscilabs.github.io/autotest/) continuously
throughout package development, to ensure that `autotest_package()` returns
clean (`NULL`) results when the package is first submitted.


## Assessment Against Standards {#pkgdev-srr}

Once a package has been sufficiently developed to begin alignment with our
standards, and once all issues revealed by
[`autotest`](https://ropenscilabs.github.io/autotest/) have been addressed,
developers will need to use our second tool, the [`ssr` (**s**oftware
**r**eveiw **r**oclets) package](https://ropenscilabs.github.io/srr/) to insert
both general and category-specific standards into their code, and to begin the
process of documenting within the code itself how and where the code adheres to
the individual standards. The `srr` package can be installed locally with

```{r srr-install, eval = FALSE, echo = TRUE}
remotes::install_github("ropenscilabs/srr")
```

The procedures for using the package are again described in detail on the [main
website](https://ropenscilabs.github.io/srr/), and in particular in the
[package vignette](https://ropenscilabs.github.io/srr/articles/srr-stats.html).
Developers are first encouraged to obtain a local copy of the [source code for
that
vignette](https://github.com/ropenscilabs/srr/blob/main/vignettes/srr-stats.Rmd),
and to step through each line in order to understand how the procedure works.
Having done that, you may then insert standards into your own package by
running from within the local directory of your package,

```{r ssr-standards, eval = FALSE, echo = TRUE}
srr_stats_roxygen (category = c ("<category-1>", "<category-2>"))
```

That will insert a new file into the `R/` directory of your package called (by
default) `srr-stats-standards.R`. All standards initially have
a [`roxygen2`](https://roxygen2.r-lib.org) tag of `@srrstatsTODO`, to indicate
that these standards are yet to be addressed. These tags are processed by the 
[`srr` roclet](https://ropenscilabs.github.io/srr/) which needs to be connected
with your package by modifying the `Roxygen` line of your `DESCRIPTION` file to
the following form:

```{r srr-DESC, eval = FALSE, echo = TRUE}
Roxygen: list (markdown = TRUE, roclets = c ("namespace", "rd", "srr::srr_stats_roclet"))
```

You do not need to add the `srr` package anywhere else in your `DESCRIPTION`
file, nor do you need to retain this line when submitting packages to CRAN (or
elsewhere). You should nevertheless retain the line at all other times, and you
can easily disable the roclet output by including `#' @srrVerbose FALSE`
somewhere within your documentation. Note that `srr` documentation lines are
used only to produce on-screen output triggered by running
[`roxygen2::roxygensise()`](https://roxygen2.r-lib.org/reference/roxygenize.html),
or the equivalent function,
[`devtools::document()`](http://devtools.r-lib.org/reference/document.html),
and do not appear in actual package documentation.

The [`srr` roclet](https://ropenscilabs.github.io/srr/) recognises and process
three tags:

1. `@srrstatsTODO` to flag standards yet to be addressed;
2. `@srrstats` to flag standards which have been addressed, and followed by
   descriptions of how your code addresses those standards; and
3. `@srrstatsNA` to flag standards which you deem not to be applicable to your
   code, followed by explanations of why you deem those standards not
   applicable.

The file generated by
[`srr_stats_roxygen()`](https://ropenscilabs.github.io/srr/reference/srr_stats_roxygen.html)
initially contains two [`roxygen2`](https://roxygen2.r-lib.org) blocks, the
first containing every standard potentially applicable to your package, tagged
with `@srrstatsTODO`, and the second with a title of `MA_standards`, to
document standards deemed not applicable. The first task after having generated
this file is to move standards to approximate locations within your package
where they are likely to be addressed. For example, standards concerning tests
should be moved somewhere within the `tests/` directory, standards concerning
documentation to the main `README.Rmd` file, or within a vignette file. The
[package
skeleton](https://ropenscilabs.github.io/srr/reference/srr_stats_pkg_skeleton.html)
includes code demonstrating how to include roclet tags within `.Rmd` files.
This will break down an initially large single list of standards into more
manageable groups of standards dispersed throughout your code. As each standard
is addressed, it should be moved to one or more locations in your code as near
as possible to relevant code, and the tag changed from `@srrstatsTODO` to
`@srrstats`, and a brief description appended to explain how that standard is
addressed. Standards deemed not to be applicable to your package should all be
grouped together within a single [`roxygen2`](https://roxygen2.r-lib.org) block
with a title of `NA_standards`, each with a tag of `@srrstatsNA`, and a brief
description of why those standards are deemed not to be applicable.

Software to be submitted for review must contain no `@srrstatsTODO` tags --
that is, all standards must have been addressed, either by documenting how and
where software adheres to each standard through moving it to the appropriate
location within your code and changing the tag to `@srrstats`, or by explaining
why certain standards are not applicable, changing tags to `@srrstatsNA`, and
moving to a [`roxygen2`](https://roxygen2.r-lib.org) block named
`NA_standards`. A summary report with hyperlinks to locations within your code
at which all standards are placed can be generated by running
[`srr_report()`](https://ropenscilabs.github.io/srr/reference/srr_report.html).
This is also the document which reviewers can use to assess how well software
complies with standards. Full details of the `srr` procedure are given on the
[package website](https://ropenscilabs.github.io/srr).

## Badges and Software Life Cycle Statement {#pkgdev-badges}

All statistical software which is recommended for acceptance by reviews is
entitled to display an rOpenSci badge, as for software reviewed under the
[current peer-review
system](https://devguide.ropensci.org/building.html#readme). The badge for
statistical software ~~includes~~ will include an additional section on the far
right indicating that the software was reviewed under the statistical software
system, rather than the standard system. This additional section ~~appears~~
will appear by default in the same colour as the left side of the current
badge:

[![](https://badges.ropensci.org/103_status.svg)](https://badges.ropensci.org/103_status.svg)

In addition to this "standard" badge, the statistical software peer review
system features the following two additional grades of badges, which ~~are~~
will be reflected in the colouring of this additional section on the right side
of the badge.

- **gold** for software which complies with *all* standards which reviewers
  have deemed potentially applicable. This is the highest standard, and
  requires developers to ensure compliance with the largest number of
  standards, while minimising the number of standards deemed non-applicable.
-   **silver** for software for which developers aspire to reach gold standard,
  but which is not yet sufficiently compliant.

These silver and gold badges will generally only be applicable to software
which developers aim to actively develop and progressively improve beyond the
initial review phase. Software which developers see as stable and having little
need for future development will generally not be able to attain silver or gold
badges. A silver badge should be viewed as a statement of intent, and not as
a fixed category in its own right. At the end of the review process, most
software will attain either the default or silver badges, with the latter
indicating an intention for package development to continue to enhance
alignment with our statistical standards until the gold standard has been
reached.

Note also that the standards for statistical software contained within this
book are versioned, and that both silver and gold badges in particular are
assessed against specific versions of standards. As these standards are
upgraded, current versions may increasingly diverge from versions against which
badges were assessed. In the absence of ongoing software maintenance, gold
badges may eventually be downgraded to silver, and silver to the default badge.
The [`srr` package](https://ropenscilabs.github.io/srr/) ~~features~~ will
feature an `srr_release()` function, analogous to
[`devtools::release()`](http://devtools.r-lib.org/reference/release.html), to
facilitate ongoing compliance, and to guide the process of progressively
complying with standards.

Intentions to achieve a specific category of badge are conveyed via a "*Life
Cycle Statement*", which must be included in a `lifecycle.md` file within
a package's repository at the time of submission. The statement can be
completed by downloaded the template from the main
[`statistical-software-review`
repository](https://github.com/ropenscilabs/statistical-software-review/blob/main/lifecycle.md),
and saving it either in the root directory of your repository, or in the
`inst/` folder. The content of that file is shown in the following sub-section.
Once an initial `lifecycle.md` file has been created, and it's location
specified in the `.Rbuildignore`, the `srr_release()` function may be called at
any time to update that statement.


### Software Life Cycle Statement

1. The \<package name\> package has been under development for \<N\> \<months/years\>.

2. The \<package name\> package has \<N\> primary and \<N\> secondary developers.

3. The primary users of the \<package name\> package are envisioned to be \<list
  envisioned user community\>, possibly of the order of
  \<tens/hundreds/thousands\> of users.

4. Following review, the \<package name\> package will

    - \<Be in a stable state of development, with minimal subsequent development
      envisioned\>
    - \<Be in a stable state of development, with active subsequent development
      primarily in response to user feedback.\>
    - \<Be in a stable state of development, with some degree of active
      subsequent development as envisioned by the primary developers.\>
    - \<Be in an initially stable state of development, with a great deal of
      active subsequent development envisioned.\>

5. Following review,
    - \<The primary developer(s) envision being able to maintain the package for
      at least \<N\> years.\>
    - \<The primary developers can envision a potential need to transfer
      package maintenance after \<N\> years.\>

(It may be appropriate to retain versions of both of the preceding statements.)

6. The main factors likely to eventually make the package obsolete or redundant
  are \<list factors here\>. This may happen sometime within \<N\> years following
  review. \<Please elaborate if desired.\>

7. We, the developers of \<package name\>, hope to attain a \<standard, silver,
  gold\> badge at the end of review.

(Note that silver or gold badges require ongoing active maintenance, for which
either the third or fourth items of point 4 must be checked. Note also that
silver should reflect a statement of intent to eventually attain a gold badge,
for which purpose the following statement is also to be completed.)

8. We aim for a silver badge at the end of review, and then aim to attain a
   gold badge via a subsequent round of review \<N\> \<months/years\> later.

(This last item can be left until negotiation with reviewers during the review
process.)
