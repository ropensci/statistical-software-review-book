[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rOpenSci Statistical Software Peer Review",
    "section": "",
    "text": "1 Welcome\nWelcome to rOpenSci’s system for peer-review of statistical software in R packages and beyond. This system extends our existing system for software peer review, through expanding the scope to include explicitly statistical software. As such, it is a direct extension of rOpenSci Packages: Development, Maintenance, and Peer Review. This book provides guidelines for authors on how to develop statistical software, and for editors and reviewers on our processes for peer review of statistical software.\nYou are invited to contribute to this project by participating in our GitHub Discussions, or filing suggestions as issues in the book’s GitHub repository. Feedback on standards is particularly welcome, for which the Discussions have dedicated pages for each category of standards.\nSubmissions of statistical software for our peer-review system are currently handled by the following team of expert editors:\nThis project has developed to its current state largely through the support of a great advisory and editorial committee. We are grateful to all of the following former members:\nThis work was largely supported by the Sloan Foundation and organized under an R Consortium Working Group.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "index.html#citation",
    "href": "index.html#citation",
    "title": "rOpenSci Statistical Software Peer Review",
    "section": "\n1.1 Citation",
    "text": "1.1 Citation\nYou can cite this book by its its Zenodo DOI, or by copying the following BibTeX entry:\n@software{mark_padgham_2021_5556756,\n  author       = {mark padgham and\n                  Maëlle Salmon and\n                  Noam Ross and\n                  Jakub Nowosad and\n                  Rich FitzJohn and\n                  yilong zhang and\n                  Christoph Sax and\n                  Francisco Rodriguez-Sanchez and\n                  François Briatte and\n                  Leonardo Collado-Torres},\n  title        = {ropensci/statistical-software-review-book:\n                   Official first standards versions\n                  },\n  month        = oct,\n  year         = 2021,\n  publisher    = {Zenodo},\n  version      = {v0.1.0},\n  doi          = {10.5281/zenodo.5556756},\n  url          = {https://doi.org/10.5281/zenodo.5556756},\n}",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "rOpenSci Statistical Software Peer Review",
    "section": "\n1.2 Contributors",
    "text": "1.2 Contributors\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the allcontributors specification. Contributions of any kind are welcome!\n\n1.2.1 Content\n\n\n\n mpadge\n\n\n maelle\n\n\n osorensen\n\n\n helske\n\n\n cforgaci\n\n\n\n\n Rekyt\n\n\n Nowosad\n\n\n richfitz\n\n\n elong0527\n\n\n christophsax\n\n\n\n\n Pakillo\n\n\n briatte\n\n\n grwhumphries\n\n\n lcolladotor\n\n\n rkillick\n\n\n\n1.2.2 Issue Authors\n\n\n\n elimillera\n\n\n IndrajeetPatil\n\n\n jsta\n\n\n noamross\n\n\n gilbertocamara\n\n\n\n\n n-kall\n\n\n KlausVigo",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome</span>"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "2  Overview of the Project and of this Book",
    "section": "",
    "text": "2.1 Project Motivation\nThis book is the main document for rOpenSci’s project to expand peer review to include explicitly statistic software. It is intended to aid both software developers intending to submit statistical software for peer-review, and for reviewers of statistical software. An additional aim of the project, and of this documentation, is to serve as a blueprint for future adoption and adaptation in other areas, including other computer languages.\nThe present book should be considered an extension of rOpenSci’s guide to software Development, Maintenance, and Peer Review (the “Dev Guide”). The guidelines and expectations for software as presented in the Dev Guide also apply to statistical software under the newly expanded system, with this document describing additional guides and expectations for explicitly statistical software. The Dev Guide ought thus be considered essential reading prior to the current book.\nThis chapter summarises overall project aims, the scope of statistical software we are currently able to consider, and provides a brief overview of the structure and purpose of this book. It consists of the following sections:\nThe official description of R declares it to be a “software environment for statistical computing and graphics”, yet rOpenSci previously deemed explicitly statistical packages out of scope, owing among other factors to the perceived difficulty of devising an appropriate system for assessment and review. R is nevertheless an explicitly statistical computing environment, and so rOpenSci developed this project to expand our peer review system to include statistical software.\nIn doing so, the project also offered an opportunity to reconsider and potentially improve aspects of rOpenSci’s current system for peer review, which had operated for five years by the time this project began, and had already reviewed over &gt;200 packages, primarily in areas of data life cycle management. The form of these packages continues to be strongly influenced by the Dev Guide, which presents sets of “guidelines” which packages are expected to “meet”. These guidelines are nevertheless necessarily general, and were largely developed in ongoing response to successive developments in technology to support software development, such as continuous integration services. Although our Dev Guide effectively provides a set of “standards” to which software is expected to adhere, the alignment of software to these standards it itself not necessarily systematic, and in particular there is no direct way to ascertain the standards to which a given piece of software adheres, and those from which it may diverge.\nThe present project reflects a more systematic alignment of software with standards, one which enables automated and ongoing identification of those standards with which a given piece of software complies. The following sets of standards for statistical software are thus far more extensive that our previous “guidelines”, and provide ongoing assurance for users of the standard of software accepted within our system, including systematic identification of ways by which software may diverge from standards, and explanations of why.\nSuch assurance is important in many areas of scientific research, notably including those subject to regulation such as pharmaceutical trials. Software used in such trials must be “validated”, generally through a process of identifying any risks associated with using that software. In such contexts, our system fosters confidence in the use of software assessed according to our standards. For developers, the system provides a system of graded “badges” able to be used to identify and publicise the assessment of their software as meeting or exceeding the standards set by our system.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview of the Project and of this Book</span>"
    ]
  },
  {
    "objectID": "overview.html#overview-scope",
    "href": "overview.html#overview-scope",
    "title": "2  Overview of the Project and of this Book",
    "section": "\n2.2 Scope of Statistical Software Review",
    "text": "2.2 Scope of Statistical Software Review\n\n2.2.1 The R Language\nThe present project represents a direct expansion of rOpenSci’s current scope to include specifically statistical software, while retaining the restriction to software in the form of R packages. Nevertheless, this does not necessarily mean that the primary language of a package needs to be R. Many R packages include code from a variety of other languages, with the following table summarising statistics for the top ten languages from all 15,948 CRAN packages as of 09 Jul 2025 (including only code from the /R, /src, and /inst directories of each package).\n\n\n\nProportion of code lines in different languages in all CRAN packages.\n\nlanguage\nlines\nproportion\n\n\n\nR\n22,921,103\n0.422\n\n\nC/C++ Header\n7,630,317\n0.141\n\n\nHTML\n5,733,628\n0.106\n\n\nC\n5,266,867\n0.097\n\n\nC++\n4,855,078\n0.089\n\n\nJavaScript\n1,414,470\n0.026\n\n\nXML\n1,048,611\n0.019\n\n\nJSON\n939,453\n0.017\n\n\nFortran 77\n839,144\n0.015\n\n\nCSS\n640,580\n0.012\n\n\n\n\n\nClose to one half of all code in all R packages to date has been written in the R language, clearly justifying a primary focus upon that language. Collating all possible ways of packaging and combining C and C++ code yields 17,752,262 lines or code or 33% of all code, indicating that 75% of all code has been written in either R or C/C++. We anticipate the large majority of submissions to be coded in one of these primary languages, and will cultivate a community of reviewers with expertise in these languages. R packages may nevertheless incorporate algorithms coded in a number of other languages (such as Rust), and no package will be considered out-of-scope on the basis of computer language alone. Developers using less common languages may nevertheless face longer processing times to allow for finding reviewers with appropriate skills in those languages.\n\n2.2.2 Categories of Statistical Software\nThe scope of statistical software able to be submitted for peer review is primarily defined by the following list of categories. Any software which fits in to one or more of these categories may be deemed in-scope, and submitted for review, while software which can not be described by any of these categories will generally be deemed out of scope. While the categories themselves are primarily defined by the corresponding standards given in detail in Chapter 6, this chapter provides brief descriptions of the categories to aid developers in initially estimating whether or not software may be in scope.\nEmpirical analyses described in Appendix A.2 were devised to identify sub-domains within statistical software, from which we have to date developed standards for the following categories:\n\nBayesian and Monte Carlo Routines\nRegression and Supervised Learning\nDimensionality Reduction, Clustering, and Unsupervised Learning\nExploratory Data Analysis (EDA) and Summary Statistics\nTime Series Analyses\nMachine Learning\nSpatial Analyses\nProbability Distributions\n\nEach of these categories is represented by a set of standards, as briefly described in the following sub-section. We anticipate that submissions will commonly fit into, or be described by, multiple categories, and the standards have also been devised to be as inter-compatible as possible. Moreover, alignment with specific categories may not always be straightforward, and we anticipate that some submissions will require negotiation between developers and editors to identify appropriate categories prior to full submission.\nWe also intend to expand the system to include the additional three categories of:\n\nWrapper Packages\nNetwork Analysis Software\nWorkflow Support\n\nWhile software in these latter three categories is beyond the scope of current standards, we invite any software developers interested in submitting software within one or more of these categories to contact us directly to enquire about the status of associated standards, and the possibility of submitting. Finally, we anticipate our sets of standards to expand further over time, and openly invite any form of discussion on the possibility of expanding our definition to include additional categories.\nThe following sub-sections provide brief descriptions of each of our chosen categories in terms of their general characteristics and inter-relationships with other categories within our empirical analyses. The standards of Chapter 6 necessarily consider each category separately. There is nevertheless some degree of overlap between categorical definitions which is it important to appreciate. The following brief descriptions attempt to state some of the potentially problematic or confounding areas of overlap and ambiguity between categorical definitions. Titles of each sub-section link directly to the corresponding standards in Chapter 6.\n\n2.2.2.1 Bayesian and Monte Carlo Routines\n\nBayesian and Monte Carlo software centres on quantitative estimation of components of Baye’s theorem, particularly on estimation or application of prior and/or posterior probability distributions. The procedures implemented to estimate the properties of such distributions are commonly based on random sampling procedures, hence referred to as “Monte Carlo” routines in reference to the random yet quantifiable nature of casino games.\nPackages implementing or otherwise relying on Bayesian or Monte Carlo routines are amongst the most common of our selected categories. Although roughly equal in frequency to several other categories, this category represents the central “hub” of all categories discerned in our empirical analyses. This indicates that software in this category is more likely than most others to also be described by additional categories.\n\n2.2.2.2 Regression and Supervised Learning\n\nRegression Software implements algorithms which aim to construct or analyse one or more mappings between two defined data sets (for example, a set of “independent” data, \\(X\\), and a set of “dependent” data, \\(Y\\)). In contrast, the analogous category of Unsupervised Learning Software aims to construct or analyse one or more mappings between a defined set of input or independent data, and a second set of “output” data which are not necessarily known or given prior to the analysis.\nCommon purposes of Regression Software are to fit models to estimate relationships or to make predictions between specified inputs and outputs. Regression Software includes tools with inferential or predictive foci, Bayesian, frequentist, or probability-free Machine Learning (ML) approaches, parametric or or non-parametric approaches, discrete outputs (such as in classification tasks) or continuous outputs, and models and algorithms specific to applications or data such as time series or spatial data. In many cases other standards specific to these subcategories may apply.\nThis category represents the most important intermediate node in the emprical network between Bayesian/Monte Carlo and Machine Learning (ML) algorithms, as well as being strongly connected to several other nodes. While many regression or interpolation algorithms are developed as part of general frameworks within these contexts, there are nevertheless sufficiently many examples of regression and interpolation algorithms unrelated to these contexts to warrant the existence of this distinct category. That said, algorithms within this category share very little in common, and each implementation is generally devised for some explicit applied purpose which may be difficult to relate to any other implementations in this category.\n\n2.2.2.3 Dimensionality Reduction, Clustering, and Unsupervised Learning\nSoftware in this category is distinguished from Regression Software though the latter aiming to construct or analyse one or more mappings between two defined data sets (for example, a set of “independent” data, \\(X\\), and a set of “dependent” data, “Y”), whereas Unsupervised Learning Software aims to construct or analyse one or more mappings between a defined set of input or independent data, and a second set of “output” data which are not necessarily known or given prior to the analysis. A key distinction in Unsupervised Learning Software and Algorithms is between that for which output data represent (generally numerical) transformations of the input data set, and that for which output data are discrete labels applied to the input data. Examples of the former type include dimensionality reduction and ordination software and algorithms, and examples of the latter include clustering and discrete partitioning software and algorithms. One of the primary problems presented by algorithms in this category is that they are constrained to yield a result independent on any measure of correctness of accuracy (Estivill-Castro 2002). This can make assessment of the accuracy or reliability of such algorithms difficult.\nThe node representing dimensionality reduction in our empirical network is almost as central as the Bayesian/Monte Carlo category, indicating the software in this category is also likely to be described by additional categories.\n\n2.2.2.4 Exploratory Data Analysis (EDA) and Summary Statistics\nExploration is a part of all data analyses, and Exploratory Data Analysis (EDA) is not something that is entered into and exited from at some point prior to “real” analysis. Exploratory Analyses are also not strictly limited to Data, but may extend to exploration of Models of those data. The category could thus equally be termed, “Exploratory Data and Model Analysis”, yet we opt to utilise the standard acronym of EDA in this document.\nEDA is nevertheless somewhat different to many other categories included here, primarily because,\n\nEDA software often has a strong focus upon visualization, which is a category which we have otherwise explicitly excluded from the scope of the project at the present stage.\nThe assessment of EDA software requires addressing more general questions than software in most other categories, notably including the important question of intended audience(s).\n\nOur empirical analyses revealed a strong connection between EDA and visualisation software, but EDA software nevertheless differed in also being connected with calculation and presentation of summary statistics, and with network relationships reflecting inter-relationships between data components.\n\n2.2.2.5 Time Series Analyses\nThe category of Time Series software is arguably easier to define that the preceding categories, and represents any software the primary input of which is intended to be temporally structured data. Importantly, while “temporally structured” may often imply temporally ordered, this need not necessarily be the case. The primary definition of temporally structured data is that they possess some kind of index which can be used to extract temporal relationships.\n\n2.2.2.6 Machine Learning\nMachine Learning (ML) routines play a central role in modern statistical analyses, and the ML node in the empirical network diagram is roughly equally central, and equally connected, to the Bayesian and Monte Carlo node. Machine Learning algorithms represent perhaps some of the most difficult algorithms for which to develop standards and methods of comparison. Both input and output data can be categorically different or even incomparable, while even where these may be comparable, the abiding aims of different ML algorithms can differ sufficiently to make comparison of outputs to otherwise equivalent inputs largely meaningless. The general ecosystem of ML software within R nevertheless offers a number of tools which may be adapted for specific stages of many ML workflows, and which may accordingly provide useful contexts for both of aligning and reviewing software against standards, even if only as “benchmark” comparisons. Divided into three main steps of input -&gt; processing -&gt; output, useful tools include:\nInput Data The vtreat package “prepares messy real world data for predictive modeling in a reproducible and statistically sound manner.” The routines in this package perform a series of tests for general sanity of input data, and may prove generally useful as part of a recommended ML workflow.\nAlgorithms The aforementioned diversity of ML algorithms has fostered the developed of several packages offering unified interfaces. As for input data, standards do not suggest that any particular package use any of these, but they should at least be considered as comparative benchmarks against which to assess packages. Both the mlr3 and tidymoels collection of packages reflect unified ML workflows with modular and extensible interfaces to a range of ML routines.\nOutput Data There are several extant packages for (post-)processing data output from ML algorithms. Many, perhaps even most, of these primarily aim to derive insightful visualisations of output, whether in interactive (JavaScript-based) form, as with the modelStudio or modelDown packages, or more static plots using internal graphical routines from R, as in the iml (Interpretable Machine Learning) package. The latter package offers a host of additional functionality useful in interpreting the output of ML algorithms, and which may prove useful in general standards-based contexts.\n\n2.2.2.7 Spatial Analyses\nSpatial analyses have a long tradition in R, as summarised and reflected in the CRAN Task Views on Spatial and Spatio-Temporal data and analyses. Those task views also make immediately apparent that the majority of development in both of these domains has been in representations of spatial data, rather than in statistical analyses per se. Spatial statistical analyses have nevertheless been very strong in R, notably through the spatstat and gstat packages, first published in 2002 and 2003, respectively.\nSpatial analyses entail a number of aspects which, while not necessarily unique in isolation, when considered in combination offer sufficiently unique challenges for this to warrant its own category. Some of these unique aspects include:\n\nA generally firm embeddedness in two dimensions\nFrequent assumptions of continuous rather than discrete processes (point-pattern processes notwithstanding)\nA pervasive decrease in statistical similarity with increasing distance - the so-called “First Law of Geography” - which is the observe of pervasive difficulties arising from auto-correlated observations.\nA huge variety of statistical techniques such as kriging and triangulation which have been developed for almost exclusive application in spatial domains.\nThe unique challenges arising in the domain of Spatial Temporal Analyses.\n\n2.2.2.8 Probability Distributions\n(Not yet in scope) The category of probability distributions is an outlier in the preceding network diagram, connected only to ML and regression/interpolation algorithms. It is nevertheless included here as a distinct category because we anticipate software which explicitly represents or relies on probability distributions to be subject to distinct standards and assessment procedures, particularly through enabling routines to be tested for robustness against a variety of perturbations to assumed distributional forms.\nPackages which fall within this category include:\n\n\nunivariateML which is, “an R package for maximum likelihood estimation of univariate densities,” which support more than 20 different forms of probability density.\n\nkdensity which is, “An R package for kernel density estimation with parametric starts and asymmetric kernels.” This package implements an effectively non-parametric approach to estimating probability densities.\n\noverlapping, which is, “a R package for estimating overlapping in empirical distributions.”\n\nThe obverse process from estimating or fitting probability distributions is arguably drawing samples from defined distributions, of which the humanleague package is an example. This package has a particular application in synthesis of discrete populations, yet the implementation is quite generic and powerful.\n\n2.2.2.9 Wrapper Packages\n(Not yet in scope) “Wrapper” packages provide an interface to previously-written software, often in a different computer language to the original implementation. While this category is reasonably unambiguous, there may be instances in which a “wrapper” additionally offers extension beyond original implementations, or in which only a portion of a package’s functionality may be “wrapped.” Rather than internally bundling or wrapping software, a package may also serve as a wrapper thorough providing access to some external interface, such as a web server. Examples of potential wrapper packages include the following:\n\nThe greta package (with accompanying JOSS article) “for writing statistical models and fitting them by MCMC and optimisation” provides a wrapper around google’s TensorFlow library. It is also clearly a workflow package, aiming to provide a single, unified workflow for generic machine learning processes and analyses.\nThe nse package (with accompanying JOSS paper) which offers “multiple ways to calculate numerical standard errors (NSE) of univariate (or multivariate in some cases) time series,” through providing a unified interface to several other R packages to provide more than 30 NSE estimators. This is an example of a wrapper package which does not wrap either internal code or external interfaces, rather it effectively “wraps” the algorithms of a collection of R packages.\n\nKey Considerations: For many wrapper packages it may not be feasible for reviewers (or authors) to evaluate the quality or correctness of the wrapped software, so review could be limited to the interface or added value provided, or the statistical routines within.\nWrapper packages include the extent of functionality represented by wrapped code, and the computer language being wrapped. - Internal or External: Does the software internally wrap of bundle previously developed routines, or does it provide a wrapper around some external service? If the latter, what kind of service (web-based, or some other form of remote access)? - Language: For internally-bundled routines, in which computer language e the routines written? And how are they bundled? (For R packages: In ./src? In ./inst? Elsewhere?) - Testing: Does the software test the correctness of the wrapped component? Does it rely on tests of the wrapped component elsewhere? - Unique Advances: What unique advances does the software offer beyond those offered by the (internally or externally) wrapped software?\n\n2.2.2.10 Networks\n(Not yet in scope) Network software is a particular area of application of what might often be considered more generic algorithms, as in the example described above of the grapherator package, for which this category is appropriate only because the input data are assumed to represent a particular form of graphical relationship, while most of the algorithms implemented in the package are not necessarily specific to graphs. That package might nevertheless be useful in developing standards because it, “implements a modular approach to benchmark graph generation focusing on undirected, weighted graphs”. This package, and indeed several others developed by its author Jakob Bossek, may be useful in developing benchmarks for comparison of graph or network models and algorithms.\nCases of software which might be assessed using such generic graph generators and benchmarks include:\n\n\nmcMST, which is “a toolbox for the multi-criteria minimum spanning tree problem.”\n\ngwdegree, which is a package for, “improving interpretation of geometrically-weighted degree estimates in exponential random graph models.” This package essentially generates one key graph statistic from a particular class of input graphs, yet is clearly amenable to benchmarking, as well as measures of stability in response to variable input structures.\n\nNetwork software which is likely more difficult to assess or compare in any general way includes:\n\n\ntcherry is a package for “Learning the structure of tcherry trees,” which themselves are particular ways of representing relationships between categorical data. The package uses maximum likelihood techniques to find the best tcherry tree to represent a given input data set. Although very clearly a form of network software, this package might be considered better described by other categories, and accordingly not directly assessed or assessable under any standards derived for this category.\n\nBNLearn is a package “for learning the graphical structure of Bayesian networks.” It is indubitably a network package, yet the domain of application likely renders it incomparable to other network software, and difficult to assess in any standardised way.\n\n2.2.2.11 Workflow Support\n(Not yet in scope) “Workflow” software may not implement particular methods or algorithms, but rather support tasks around the statistical process. In many cases, these may be generic tasks that apply across methods. These include:\n\nClasses (whether explicit or not) for representing or processing input and output data;\nGeneric interfaces to multiple statistical methods or algorithms;\nHomogeneous reporting of the results of a variety of methods or algorithms; and\nMethods to synthesise, visualise, or otherwise collectively report on analytic results.\n\nMethods and Algorithms software may only provide a specific interface to a specific method or algorithm, although it may also be more general and offer several of the above “workflow” aspects, and so ambiguity may often arise between these two categories. We note in particular that the “workflow” node in the interactive network diagram mentioned above is very strongly connected to the “machine learning” node, generally reflecting software which attempts to unify varied interfaces to varied platforms for machine learning.\nAmong the numerous examples of software in this category are:\n\nThe mlr3 package (with accompanying JOSS paper), which provides, “A modern object-oriented machine learning framework in R.”\nThe fmcmc package (with accompanying JOSS paper), which provides a unified framework and workflow for Markov-Chain Monte Carlo analyses.\nThe bayestestR package (with accompanying JOSS paper) for “describing effects and their uncertainty, existence and significance within the Bayesian framework. While this packages includes its own algorithmic implementations, it is primarily intended to aid general Bayesian workflows through a unified interface.\n\nWorkflows are also commonly required and developed for specific areas of application, as exemplified by the tabular package (with accompanying JOSS article for “Analysis, Seriation, and visualisation of Archaeological Count Data”.\nKey Considerations: Workflow packages are popular and add considerable value and efficiency for users. One challenge in evaluating such packages is the importance of API design and potential subjectivity of this. For instance, mlr3 as well as tidymodels have similar uses of providing a common interface to multiple predictive models and tools for automating processes across these models. Similar, multiple packages have different approaches for handling MCMC data. Each package makes different choices in design and has different priorities, which may or may not agree with reviewers’ opinions or applications. Despite such differences, it may be possible to evaluate such packages for internal cohesion, and adherence to a sufficiently clearly stated design goal. Reviewers may be able to evaluate whether the package provides a more unified workflow or interface than other packages - this would require a standard of relative improvement over the field rather than baseline standards.\nThese packages also often contain numerical routines (cross-validation, performance scoring, model comparison), that can be evaluated for correctness or accuracy.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview of the Project and of this Book</span>"
    ]
  },
  {
    "objectID": "overview.html#overview-prior-art",
    "href": "overview.html#overview-prior-art",
    "title": "2  Overview of the Project and of this Book",
    "section": "\n2.3 Prior Art",
    "text": "2.3 Prior Art\n\n2.3.1 rOpenSci\nrOpenSci’s current software peer-review process, detailed in our developer guide, is based on a blend of practices from peer review of academic practices and code review in open-source projects. Review takes place via an issue thread in our “software-review” repository on GitHub. The review process is entirely open, with each issue thread used to manage the entire process, coordinated by rOpenSci’s editors. After initial screening for scope and minimal qualification by editors, two reviewers provide comments and feedback on software packages. After one or more rounds of revisions, packages reach a point of approval, at which point they are “accepted” by rOpenSci, symbolized both through a badge system, and (generally) through transferring the software from the authors’ private domain to the github.com/ropensci domain.\n\n2.3.2 The Journal of Open Source Software\nThe Journal of Open Source Software (JOSS) was based on rOpenSci and follows a similar approach, with greater automation and broader scope. The Journal of Statistical Software conducts a closed review of both manuscript and software, with fewer prescriptive standards. In reviewing packages for acceptance into its repository, BioConductor conducts an open review primarily aimed at maintaining minimum standards and inter-compatibility.\n\n2.3.3 The Debian System\nThe development of software for the open-source Debian Operating System is guided by Debian Developers and Debian Maintainers. Expressed roughly, maintainers are individuals responsible for the maintenance of particular pieces of software, while developers engage with activities supporting the development of the operating system as a whole. The submission and review process for Debian is almost entirely automated, based on tools such as their own software checker, lintian. Debian differs fundamentally from the system proposed here in being centred around the trust and verification of people rather than software. Submission of software to Debian is largely automatic, and bug-free software may often progress automatically through various stages towards acceptance. Software may, however, only be submitted by official Debian Maintainers or Developers. People can only become developers or maintainers through being sponsored by existing members, and are then subject to review of the potential contribution they may be able to make to the broader Debian community. (Details can be seen in this chapter of the Debian handbook.)\nWhile the general process for software submission and acceptance in Debian may not be of direct relevance, their versioning policy provides a useful basis for our own versioning system. The ongoing development of both the Debian system and all associated packages proceeds in accordance with a versioned policy manual. All new packages must comply to the current standards at the time of submission, and are labelled with the latest version of the standards to which they comply, noting that,\n\nFor a package to have an old Standards-Version value is not itself a bug … It just means that no-one has yet reviewed the package with changes to the standards in mind.\n\nEach new version of the standards is accompanied by a simple checklist of differences, explicitly indicating differences with and divergences from previous versions. As long as software continues to pass all tests, upgrading to current standards remains optional. Failing tests in response to any upgrading of standards serve as a trigger for review of software. The nominated standards version may only be updated once review has confirmed compliance with current standards. The present project adapts some of these aspects of the Debian system, as described below.\n\n2.3.4 Other Potential Models\nThe Linux Core Infrastructure Initiative provides badges to projects meeting development best practices. Badges are graded (passing/silver/gold), and awarded by package authors self-certifying that they have implemented items on a checklist.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview of the Project and of this Book</span>"
    ]
  },
  {
    "objectID": "overview.html#overview-use-of-book",
    "href": "overview.html#overview-use-of-book",
    "title": "2  Overview of the Project and of this Book",
    "section": "\n2.4 Use of this Book",
    "text": "2.4 Use of this Book\nThis book is primarily intended to be used by the two primary audiences of software developers and reviewers. As mentioned above, it is also intended to serve as a “blueprint” to be adopted and adapted to other areas, including other computer languages, and other domains of application. The book has two primary entry points for these two primary audiences, with the following chapter providing extensive guidelines for package development, submission, and maintenance, and a subsequent chapter providing guidelines for reviewers of software submissions. Both audiences will need to refer to the actual standards, both general and category-specific. The book also includes important guidelines for our editors, in particular to instruct them on the capabilities of our automated ropensci-review-bot, and associated automatic package checking routines.\nImportantly, the entire project strives to cultivate diverse, inclusive, and geographically expansive communities, in terms both of software itself, and associated communities of developers, reviewers, and users. Note that while these aspects of community are not explicitly addressed throughout any of the remainder of this document, it is important that future revisions return to this point, and ensure that each of the following sections are appropriately modified to ensure effective consideration and incorporation of the representativeness and inclusiveness of communities cultivating and surrounding our software.\n\n\n\n\nEstivill-Castro, Vladimir. 2002. “Why so Many Clustering Algorithms: A Position Paper.” ACM SIGKDD Explorations Newsletter 4 (1): 65–75. https://doi.org/10.1145/568574.568575.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Overview of the Project and of this Book</span>"
    ]
  },
  {
    "objectID": "pkgdev.html",
    "href": "pkgdev.html",
    "title": "3  Guide for Authors",
    "section": "",
    "text": "3.1 Benefits of statistical software peer review\nThe current chapter should be considered an extension of the corresponding “Guide for Authors” in rOpenSci’s “Dev Guide”. The principles for package development described there also apply to statistical packages, with this chapter describing additional processes and practices for packages intended for submission to the statistical software peer review system.\nThe major additional process is the documentation of how a package complies with the general and category-specific standards given in Chapter 6. Statistical software packages must document compliance with the General Standards, as well as at least one set of category-specific standards. Authors need to document within the software itself every point at which it complies with every general and every applicable category-specific standard. The process of doing so is facilitated by the srr package, as described in detail in Sub-section 3.4, below.\nDocumenting compliance with our standards will generally entail more work than for non-statistical software submitted to our general peer review system. The first of the following sub-sections thus presents some of the benefits authors may expect from submitting their statistical software for peer-review.\nThe chapter then proceeds with a description of the scope of statistical software able to be considered for review, followed by descriptions of two tools intended to be used through the entire process of package development. The first tool is the pkgcheck package which should be used to confirm whether software is ready for submission or not, and enables authors to locally run the suite of checks which are automatically run on package submission. The subsequent sub-section describes our autotest tool, which is intended to be used through the entire process of package development. The third sub-section describes how use the srr package to address the major task of aligning software with our general and category-specific standards for statistical software, and the final sub-section describes the final step of specifying which grade of badge authors are aiming for.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Guide for Authors</span>"
    ]
  },
  {
    "objectID": "pkgdev.html#benefits-of-statistical-software-peer-review",
    "href": "pkgdev.html#benefits-of-statistical-software-peer-review",
    "title": "3  Guide for Authors",
    "section": "",
    "text": "First, feedback from your reviewers and editors will increase the quality and usability of your software. Both authors and reviewers consistently report that review improves code and makes them better programmers through the feedback and interaction with each other.\nBringing your software into compliance with our standards will make your software easier to maintain, as many of our standards, such as those for testing and continuous integration, are designed to make sure software continues to function well as it is updated.\nAs our standards and peer-review process aim to improve the quality and completeness of documentation, going through peer-review should make your software more understandable and reduce the support you need to provide to users.\nApproval by rOpenSci will make it easier to submit your software to repositories and journals. Packages approved by rOpenSci generally are CRAN-ready. They are eligible for expedited review at the Journal of Open Source Software, and the Journal of Statistical Software (JStatSoft) already recommends that developers refer to our standards. In our experience code peer-review is looked on favorably by reviewers of software papers at many journals.\nIn regulated environments and other fields where strong demonstration of quality and compliance is needed, approval by peer-review provides strong evidence of software quality rarely available for statistical software. Our approach to annotating within code how software meets standards makes demonstration of compliance clear and granular. We are actively working with the R Validation Hub to make approval under our system of review and standards a component of validation in regulated environments such as pharmaceutical or clinical research.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Guide for Authors</span>"
    ]
  },
  {
    "objectID": "pkgdev.html#scope",
    "href": "pkgdev.html#scope",
    "title": "3  Guide for Authors",
    "section": "\n3.2 Scope",
    "text": "3.2 Scope\nThe first important task prior to submitting a package is to estimate whether a package is likely to be considered within our scope for statistical software. As described in the Overview, packages are generally considered in scope if they fit one or more of the categories listed there. Prior to submission, authors must choose one or more of these categories, and document how their software aligns with the corresponding standards given in Chapter 6, according to the procedures described below. Any software which can be aligned with one or more sets of category-specific standards will by definition be considered in scope.\nAuthors are encouraged to contact us at any point prior to, or during, development, to ask about whether a package might be in scope, or which categories it might fit within. Categorisation of packages may not always be straightforward, and we particularly encourage authors who are unsure about whether a package belongs in a particular category or not to contact us for discussion. An initial judgement of whether or not a package belongs in a particular category may be gained by examining the respective standards. Any package for which a large number of standards from a particular category may be considered applicable (regardless of whether or not they would actually be checked) is likely to fit within that category. Once you have determined that your package is likely to fit into one or more of our in-scope categories, you’ll need to apply our three primary development tools described in the following two sub-sections.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Guide for Authors</span>"
    ]
  },
  {
    "objectID": "pkgdev.html#the-pkgcheck-package",
    "href": "pkgdev.html#the-pkgcheck-package",
    "title": "3  Guide for Authors",
    "section": "\n3.3 The pkgcheck package\n",
    "text": "3.3 The pkgcheck package\n\nThe pkgcheck package can be used to confirm whether software is ready for submission or not. The checks implemented within this package are also automatically run upon submission, and packages are expected to successfully pass all checks prior to initial submission. Packages may only be submitted once the main pkgcheck() function indicates such, through clearly stating,\n\nThis package may be submitted\n\nThis function accepts a single argument of the local path to the package being checked, and returns a detailed list of checks and associated results. The return object has a summary method which prints a formatted result to the console indicating whether a package is ready for submission or not. See the main package website for more details.\nThe pkgcheck() function is also applied to all packages upon initial submission, in response to which our ropensci-review-bot will print the results in the issue. In the unlikely circumstances that a package is unable to pass particular checks, explanations should be given upon submission about why those checks fail, and why review may proceed in spite of such failures.\nAn example result of the pkgcheck() function may be seen by applying it to the skeleton srr (Software Review Roclets) package:",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Guide for Authors</span>"
    ]
  },
  {
    "objectID": "pkgdev.html#pkgdev-autotest",
    "href": "pkgdev.html#pkgdev-autotest",
    "title": "3  Guide for Authors",
    "section": "\n3.4 The autotest package\n",
    "text": "3.4 The autotest package\n\nThe autotest package is an automated assessment tool which all packages are expected to pass in order to be accepted for submission. The package implements a form of “mutation testing,” by examining the types of all input parameters, implementing type-specific mutations, and examining the response of each function in a package to all such mutations. This kind of mutation testing is a very effective way to uncover any unexpected behaviour which authors themselves might not necessarily pre-empt. The purpose of using autotest to prepare packages is to avoid as much as possible the common situation of reviewers discovering bugs when they attempt to use software in ways that differ from typical uses envisioned by authors themselves. Reviews of software prepared with the help of autotest should be less burdened by discussions of what are often minor technical details, and more able to focus on “higher level” aspects of software quality.\nFull documentation of how to use autotest in package development is provided on the package website, and we particularly encourage any authors intending to develop packages for submission to our peer review system to step through the main autotest vignette, and to apply autotest continuously throughout package development, to ensure that autotest_package() returns clean (NULL) results when the package is first submitted.\nNote that use of autotest is recommended yet not strictly required. The package is intended to aid package development and preparation for submission, yet it is still experimental, and it may not be possible for all packages to generate clean (NULL) results.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Guide for Authors</span>"
    ]
  },
  {
    "objectID": "pkgdev.html#pkgdev-srr",
    "href": "pkgdev.html#pkgdev-srr",
    "title": "3  Guide for Authors",
    "section": "\n3.5 The srr package\n",
    "text": "3.5 The srr package\n\nOnce a package has been sufficiently developed to begin alignment with our standards, and once all issues revealed by autotest have been addressed, authors will need to use our third tool, the ssr (software review roclets) package to insert both general and category-specific standards into their code, and to begin the process of documenting within the code itself how and where the code adheres to the individual standards. The srr package can be installed locally by running either one of the following two lines.\n\nremotes::install_github(\"ropensci-review-tools/srr\")\npak::pkg_install(\"ropensci-review-tools/srr\")\n\nsrr procedures are described in detail on the package website, and in particular in the main vignette. Authors are first encouraged to obtain a local copy of the source code for that vignette, and to step through each line in order to understand how the procedure works. Having done that, you may then insert standards into your own package by running the following line from within the local directory of your package,\n\nsrr_stats_roxygen (category = c (\"&lt;category-1&gt;\", \"&lt;category-2&gt;\"))\n\nThat will insert a new file into the R/ directory of your package called (by default) srr-stats-standards.R. All standards initially have a roxygen2 tag of @srrstatsTODO, to indicate that these standards are yet to be addressed. These tags are processed by the srr roclet which needs to be connected with your package by modifying the Roxygen line of your DESCRIPTION file to the following form:\n\nRoxygen: list (markdown = TRUE, roclets = c (\"namespace\", \"rd\", \"srr::srr_stats_roclet\"))\n\nYou do not need to add the srr package anywhere else in your DESCRIPTION file, nor do you need to retain this line when submitting packages to CRAN (or elsewhere). You should nevertheless retain the line at all other times, and you can easily disable the roclet output by including #' @srrVerbose FALSE somewhere within your documentation. Note that srr documentation lines are used only to produce on-screen output triggered by running roxygen2::roxygensise(), or the equivalent function, devtools::document(), and do not appear in actual package documentation.\nThe srr roclet recognises and process three tags:\n\n\n@srrstatsTODO to flag standards yet to be addressed;\n\n@srrstats to flag standards which have been addressed, and followed by descriptions of how your code addresses those standards; and\n\n@srrstatsNA to flag standards which you deem not to be applicable to your code, followed by explanations of why you deem those standards not applicable.\n\nThe file generated by srr_stats_roxygen() initially contains two roxygen2 blocks, the first containing every standard potentially applicable to your package, tagged with @srrstatsTODO, and the second with a title of NA_standards, to document standards deemed not applicable. The first task after having generated this file is to move standards to approximate locations within your package where they are likely to be addressed. For example, standards concerning tests should be moved somewhere within the tests/ directory, standards concerning documentation to the main README.Rmd file, or within a vignette file. The package skeleton includes code demonstrating how to include roclet tags within .Rmd files.\nMoving different standards to more appropriate locations within your code will break down an initially large single list of standards into more manageable groups dispersed throughout your code. As each standard is addressed, it should be moved to one or more locations in your code as near as possible to relevant code, the tag changed from @srrstatsTODO to @srrstats, and a brief description appended to explain how that standard is addressed. Standards deemed not to be applicable to your package should all be grouped together within a single roxygen2 block with a title of NA_standards, each with a tag of @srrstatsNA, and a brief description of why those standards are deemed not to be applicable.\nSoftware to be submitted for review must contain no @srrstatsTODO tags – that is, all standards must have been addressed by modifying every tag to either @srrstats or @srrstatsNA, as described above. Two useful functions to aid package alignment with standards are:\n\nThe srr_stats_pre_submit() function, which confirms that all standards have been addressed prior to submission.\nThe srr_report() function, which generates a summary report with hyperlinks to locations within your code at which all standards are placed.\n\nThe output of both of these functions are included in the result of the pkgcheck() function, both when run locally, and as run upon initial package submission. The srr_stats_pre_submit() function can be used locally to confirm that,\n\n\n✔ All applicable standards have been documented in this package\n\n\nwhile the result of the srr_report() function may be accessed through the link given in the pkgcheck output, or it can be viewed by calling that function directly.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Guide for Authors</span>"
    ]
  },
  {
    "objectID": "pkgdev.html#pkgdev-badges",
    "href": "pkgdev.html#pkgdev-badges",
    "title": "3  Guide for Authors",
    "section": "\n3.6 Gold, Silver, and Bronze Badges",
    "text": "3.6 Gold, Silver, and Bronze Badges\nAll statistical software which is recommended for acceptance by reviewers is entitled to display an rOpenSci badge. This badge is a modified version of the badge for the current peer-review system, with an additional section on the far right indicating the version of that standards against which the software was assessed, coloured according to the “grade” of the badge. The three possible badges look like this:\n bronze for software which is sufficiently or minimally compliant with standards to pass review.\n silver for software for which complies with more than a minimal set of applicable standards, and which extends beyond bronze in least one notable way, as explained below.\n gold for software which complies with all standards which reviewers have deemed potentially applicable.\nThe submission template for statistical software submissions requires authors to identify the grade they wish to attain from the review process. These standards are not static, and it is always possible to elevate a badge to a higher grade subsequent to review. Badge grades may also be downgraded for code which is not continuously aligned with ongoing developments in standards. The following sub-sections provide further clarification of each grade.\n\n3.6.1 Bronze \n\nSoftware which is sufficiently or minimally compliant with standards will receive a bronze badge. One common reason for this badge is software which authors do not intend to develop further following review. This commonly arises for software produced from research projects which have been completed, leaving no funding to further develop the software. Another reason might be that software has been developed for a particular use case, with authors unable to align it with additional standards in order to expand its general utility. A bronze badge need not signify any weakness or inadequacy in software, rather it will generally signify software which has been developed for one particular use case, and which will not be subject to significant further development.\n\n3.6.2 Silver \n\nSilver badges are granted to software which extends beyond the minimal requirements of bronze in at least one the following four aspects:\n\nCompliance with a good number of standards beyond those identified as minimally necessary. This will require reviewers and authors to agree on identification of both a minimal subset of necessary standards, and a full set of potentially applicable standards. This aspect may be considered fulfilled if at least one quarter of the additional potentially applicable standards have been met, and should definitely be considered fulfilled if more than one half have been met.\nDemonstrating excellence in compliance with multiple standards from at least two broad sub-categories. Sub-categories are distinguished in the Standards Chapter by three numbers, so that the General Standards have five sub-categories numbered 6.1.1 to 6.1.5. This aspect would require software to extend notably beyond the requirements of two or more standards in at least two sub-categories (regardless of whether general or category-specific standards). For example, software which might otherwise be assessed at bronze grade, yet which is both excellently documented, and has an outstanding test suite, may be considered to fulfil this aspect.\nHave a demonstrated generality of usage beyond one single envisioned use case. Software is frequently developed for one particular use case envisioned by the authors themselves. Generalising the utility of software so that it is readily applicable to other use cases, and satisfactorily documenting such generality of usage, represents another aspect which may be considered sufficient for software to attain a silver grade.\nInternal aspects of package structure and design. Many aspects of the internal structure and design of software are too variable to be effectively addressed by standards. Packages which are judged by reviewers to reflect notably excellent design choices, especially in the implementation of core statistical algorithms, may also be considered worthy of a silver grade.\n\n3.6.3 Gold \n\nTo attain a gold badge, software must comply with all applicable standards, and must also fulfil at least three of the four aspects described above for silver-grade badges. Both the applicability of standards, and fulfilment of these three aspects, will ultimately determined by reviewers. Moreover, compliance with all grades is assessed against current standards, meaning that a gold badge must be actively maintained as standards themselves are revised and updated.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Guide for Authors</span>"
    ]
  },
  {
    "objectID": "pkgsubmission.html",
    "href": "pkgsubmission.html",
    "title": "4  Guide for Editors",
    "section": "",
    "text": "4.1 Editor-in-Chief\nAs for rOpenSci’s current peer-review system, packages are submitted directly to the ropensci/software-review repository on GitHub, with submissions handled initially by the rotating Editor-in-Chief (EiC). All statistical software submissions should begin with a pre-submission enquiry, although direct submissions may be accepted at the EiC’s discretion. The EiC should perform the following duties on initial pre-submission enquiries of statistical software:\nFull submissions generate a detailed report from our ropensci-review-bot, as described below. Examples can be seen by looking at any recent submissions. The EiC need only purvey the summary checks within the initial section of the report to confirm that all the submission passes all checks, and that the report concludes with a statement that,\nIn response to that statement, the sole tasks for an EiC prior to delegating a handling editor are to check the following single item:\nAnd to choose one the following two items:\nAdditional effort by the EiC will only be required in “edge cases” where a package may be unable to pass one of the checks, as in this sample automated check, which concludes with the statement that,\nIn these cases, submitting authors must explain why these checks may fail, and the EiC must then determine whether these failures are acceptable. Such cases ought nevertheless be rare, and it may be expected in the majority of cases that the sole tasks of the EiC are to confirm a positive bot response, to complete the two checklist items given above, and to allocate a handling editor. This latter step is done by calling @ropensci-review-bot assign &lt;name&gt; as editor.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Guide for Editors</span>"
    ]
  },
  {
    "objectID": "pkgsubmission.html#editor-in-chief",
    "href": "pkgsubmission.html#editor-in-chief",
    "title": "4  Guide for Editors",
    "section": "",
    "text": "Check that the appropriate submission template has been used, and ask authors to edit and update data if not.\nAsk authors to respond to any comments issued by the ropensci-review-bot about template format and content.\nCall @ropensci-review-bot check srr to generate a summary of compliance with our “srr” (Software Review Roclets) system for documenting standards compliance.\nConfirm from the report generated by check srr that the software complies with at least half of all applicable standards,\nPerform other EiC responsibilities described in our main Dev Guide.\n\n\n\nThis package may be submitted\n\n\n\nThe categories nominated by the submitting authors are appropriate for this package\n\n\n\nThe package does not fit within any additional categories of statistical software.\n\nThe package could potentially be described by the following additional categories of statistical software:\n\n&lt;list categories here&gt;\n\n\n\n\n\nAll failing checks above must be addressed prior to proceeding",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Guide for Editors</span>"
    ]
  },
  {
    "objectID": "pkgsubmission.html#pkgsub-handling-editor",
    "href": "pkgsubmission.html#pkgsub-handling-editor",
    "title": "4  Guide for Editors",
    "section": "\n4.2 Handling Editor",
    "text": "4.2 Handling Editor\nThe Handling Editor should use the summary report generated by the opening of the issue to perform an initial assessment and to guide assignment of reviewers. The contents of these automated checks are described at the end of this chapter. The EiC need only consider the initial summary checklist, but handling editors should consider all details contained within the automated report. General procedures for Handling Editors, including explanations of relevant ropensci-review-bot commands, are described in the Editor’s Chapter of the general Dev Guide. This section describes specific editorial requirements for statistical submissions.\n\n4.2.1 Handling Editor Checklist\nHaving considered the automated package report, and addressed all issues raised within that report, Handling Editors should check the following items before assigning reviewers.\n\nAny issues raised during initial processing by EiC have been resolved (or there were none).\nIssues raised in the goodpractice checks, statistical anomalies, and other details of the report do not appear to be major problems and can be directed to reviewers for further scrutiny.\nEither (i) this package is aiming for a bronze badge, or, (ii) for packages aiming for silver or gold badges, the authors have clarified which of the four aspects listed in the “Guide for Authors” section on silver badges they intend to fulfil.\n\nIssues flagged by the handling editor may require iteration with submitting authors. As stated in the Dev Guide:\n\nIf authors believe changes might take time, apply the holding label to the submission.\nIf the package raises a new issue for rOpenSci policy, start a conversation in Slack or open a discussion on the rOpenSci forum to discuss it with other editors (example of policy discussion).\n\nOnce all items have been checked, Handling Editors may proceed to find and assign two reviewers. As for general software reviews, start by commenting @ropensci-review-bot seeking reviewers. A email template to invite prospective reviewers of statistical packages is in the appendix. Once reviewers have agreed, use the command @ropensci-review-bot add &lt;@GITHUB_USERNAME&gt; to reviewers, and generally follow the procedure given in the Dev Guide. Handling editors may provide guidance to reviewers on issues that they think may be worth looking into based on their initial review of the package and automated checks.\n[Note that the Dev Guide is rapidly iterating as new capabilities are added to the review bot, and you may wish to refer to the draft version at &lt;devdevguide.netlify.app/&gt;.]\n\n4.2.2 Re-generating package check results\nThe handling editor may update the initial package check results at any time with the following command:\n@ropensci-review-bot check package\nThis is likely to be necessary following each review, to ensure any issues identified from the initial checks have been satisfactorily addressed.\n\n4.2.3 Disagreements on Badge Grades\nHandling editors are responsible for resolving any disagreements between authors’ stated or desired grade of badge and reviewers’ recommendations. Click here to jump to the corresponding recommendations for reviews. The views of reviewers should generally be prioritized in such cases. Grades as declared by authors are contained in the opening comment of the issue. These may be extracted by calling:\n@ropensci-review-bot stats grade\nEditors may modify these grades by editing the opening comment, and changing the value of the \"statsgrade\" variable.\n\n4.2.4 Approval\nAfter having completed a checklist and ensuring agreement on badge grade, the handling editor may approve a submission with the following command:\n@ropensci-review-bot approve\nThe bot will identify that this is a statistical software issue, extract the appropriate grade, and attach a corresponding badge which will also label the latest version of our statistical standards.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Guide for Editors</span>"
    ]
  },
  {
    "objectID": "pkgsubmission.html#automated-checks",
    "href": "pkgsubmission.html#automated-checks",
    "title": "4  Guide for Editors",
    "section": "\n4.3 Automated Checks",
    "text": "4.3 Automated Checks\nUpon initial submission, the ropensci-review-bot performs a suite of tests and checks, and will upload a report into the GitHub issue thread. This report is the primary source of information used to inform initial editorial decisions. All editors must familiarise themselves with the structure and contents of these automated reports, an example of which can be seen by clicking here. A similar report can be reproduced locally by running the code within the following sub-section.\n\nPackage report code here (click to see).\n\n\n\nlibrary (srr)\nlibrary (pkgcheck)\npath &lt;- srr_stats_pkg_skeleton ()\nsrr_stats_roxygen (category = \"regression\",\n                   filename = file.path (path, \"R\", \"srr-standards.R\"))\ncheck &lt;- pkgcheck (path)\nmd &lt;- checks_to_markdown (check, render = TRUE)\n\n\nThe crucial section for statistical packages is the first, which describes checks conducted by the srr (Software Review Roclets) package. This check confirms that all statistical standards have been documented within the code, and all packages must pass this check. The report linked to in that section is primarily intended to aid reviewers, and may be ignored by handling editors.\nThe second section describes the “Statistical Properties” of the package being submitted, and should be considered by handling editors. In particular, this section contains information which identifies any statistically noteworthy properties of the package. The example report illustrates how this report immediately identifies that the package has very little code, very few functions, and very few tests. Handling editors should consider these statistical details, and particularly any noteworthy aspects (defined by default as lying within upper or lower fifth percentiles in comparison with all current CRAN packages). Any aspects which seem concerning should be explicitly raised with submitting authors prior to proceeding. The measures currently considered include various metrics for:\n\nSize of code base, both overall and in sub-directories\nNumbers of files in various sub-directories\nNumbers of functions\nNumbers of documentation lines per function\nNumbers of parameters per function\nNumbers of blank lines\n\nA final metric, fn_call_network_size, quantifies the number of inter-relationships between different functions. In R directories, these are function calls, while relationships may be more complex within src or inst directories. Small network sizes indicate packages which either construct few objects (functions), or in which internal objects have no direct relationships.\nThe third and final section of the automated report contains details of goodpractice checks including:\n\nCode coverage estimates for each file (from the covr package).\nCode style reports from the lintr package.\nCyclomatic complexity reports from the cyclocomp package.\nAny errors, warnings, or notes raised by running R CMD check (from the rcmdcheck package.\n\nAny aspects of these goodpractice reports which do not pass the initial checklist (such as warnings or errors from R CMD check, or test coverage &lt; 75%) should be clarified with authors prior to proceeding with review.\nFinally, the initial Statistical Description includes details of computer languages used in a package, and should be used to ensure reviewers have appropriate experience and abilities with the language(s) in which a package is written.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Guide for Editors</span>"
    ]
  },
  {
    "objectID": "pkgreview.html",
    "href": "pkgreview.html",
    "title": "5  Guide for Reviewers",
    "section": "",
    "text": "5.1 Tools for Reviewing Statistical Software\nThe current chapter should be considered an extension of the corresponding “Guide for Reviewers” in rOpenSci’s “Dev Guide”. The principles for reviewing packages described there also apply to statistical packages, with this chapter describing additional processes and practices for review of packages submitted to the statistical software peer review system. Reviews of statistical software should first assess compliance against our standards, and then proceed to a more general review, as described in the following two sub-sections. The template to be used for reviews of statistical software is included in the final sub-section of this chapter. Prior to describing the review process, the following sub-section describes several tools which can be used to aid review.\nUpon initial submission, the ropensci-review-bot generates an automated report summarising aspects of package structure and functionality intended to inform the review process, an example of which can be seen here.\nThe elements of these reports are described in the Guide for Editors. While the aspects reported on there are primarily intended to help editors initially identify potential issues best addressed prior to review, they nevertheless include a number of insights into package structure which may usefully inform the review process.\nComponents of these reports intended to aid reviews include a complete report of standards compliance, generated with the srr package, and an interactive diagram of inter-relationships between package functions (and other objects), generated with the pkgstats package. These can be recreated locally by first installing these two packages by running either,\nremotes::install_github (\"ropensci-review-tools/pkgstats\")\nremotes::install_github (\"ropensci-review-tools/srr\")\nor,\npak::pkg_install (\"ropensci-review-tools/pkgstats\")\npak::pkg_install (\"ropensci-review-tools/srr\")\nWithin a local clone of the package being reviewed, the report on statistical standards can be generated by running srr::srr_report(), with the sample report linking to a version of that report which may be viewed here, and the detailed statistical properties of the package and associated interactive diagram of package structure generated by running,\nlibrary (pkgstats)\nx &lt;- pkgstats () # 'x' has lots of detail on package structure\nplot_network (x)\nThis network, the sample version of which may be viewed here, provides immediate visual insight into the relationships between all objects constructed within a package in all languages used, both R itself and any languages used in src/ code such as C or C++. The following section describes the srr report in more detail, and its intended use in assessing compliance with our standards.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Guide for Reviewers</span>"
    ]
  },
  {
    "objectID": "pkgreview.html#assessment-against-standards",
    "href": "pkgreview.html#assessment-against-standards",
    "title": "5  Guide for Reviewers",
    "section": "\n5.2 Assessment Against Standards",
    "text": "5.2 Assessment Against Standards\nThe entire system for peer review of statistical software is based on sets of general and category-specific standards given in Chapter 6 of this book. The process of assessing software against standards is facilitated by the srr (software review roclets) package which both authors and reviewers need to install as shown above.\nThis package is primarily intended to aid authors in documenting both how and where their software complies with each of the relevant general and category-specific standards. The function of the package used to aid reviewers is srr_report(), the output of which is linked from the initial package report described above, and can also be generated locally by simply running that function within a local clone of the package being reviewed. The report contains hyperlinks to all places in the code at which each standard is addressed.\nUsing this report, reviewers must assess their agreement with every statement either of compliance with, or non-applicability of, standards as reflected in roclet tags of:\n\n\n@srrstats for standards with which software complies;\n\n@srrstatsNA for standards which authors have deemed not to be applicable to their software.\n\nThe srr_report() is divided into two main sections containing links to locations in the code where these two types of tags are documented. No action need be taken on standards with which reviewers agree, whether because software complies and has a tag of @srrstats, or because a standard is not applicable and has a tag of @srrstatsNA. Reviewers are only asked to note any standards with which they disagree, primarily either because of:\n\nDisagreement in standards compliance, where authors have used a tag of @srrstats but a reviewer judges either the explanation or associated code to be insufficient for compliance; or\nDisagreement about non-applicability of a standard, where authors have used a tag of @srrstatsNA, but a reviewer believes that standard ought to apply to the software.\n\nThe srr_report() function returns the same content in markdown format, and may be used by reviewers as an initial checklist against which to assess compliance. All standards with which reviewers agree with authors statements of compliance may simply be removed, hopefully reducing initially extensive checklists down to a manageable few items with which reviews might disagree.\nThe following sub-section describes additional procedures required when assessing standards compliance of packages aiming for either silver or gold badges. The general srr procedure is described in the main package vignette, which reviewers are also encouraged to read to familiarise themselves with how the srr package is used to document compliance with standards. The main srr vignette includes code which can be stepped through to generate an example report.\n\n5.2.1 Review for Silver and Gold Badges\nThis system for peer-review of statistical software features badges in three categories of bronze, silver, and gold. As described in the corresponding Guide for Authors, a silver badge is granted to software which complies with more than a minimal set of applicable standards, and which extends beyond bronze in least one notable aspect while a gold badge is granted to software which complies with all standards which reviewers have deemed potentially applicable, and which extends beyond bronze in several notable aspects. These notable aspects by which software may fulfil the requirements of silver or gold badges are:\n\nCompliance with a sufficient number of additional standards beyond the minimal number necessary for bronze compliance;\nDemonstrated excellence in compliance with at least two standards from two distinct sub-sections;\nHaving a demonstrated generality of usage beyond a single use case; or\nDemonstrated excellence in internal aspects of package design and structure.\n\nThe authors will have identified in their initial submission which of these aspects they intend to fulfil. For packages which claim to comply with more than a minimal number of necessary standards, reviewers must additionally consider both which of the standards with which software complies might be considered minimally necessary, as well as whether any standards which authors have identified as not applicable (through @ssrstatsNA tags) could indeed be deemed applied. Not all standards may be able to be applied to a given piece of software. For example, software designed to accept sparse matrix inputs from the Matrix package will be unable to conform with many of the standards for general rectangular input forms.\nThese three categories of necessary, currently, and potentially applicable standards can then be used by reviewers to roughly assess the quantitative degree by which compliance exceeds a minimally required level. As stated in the Guide for Authors, the first of these four items may be considered to be fulfilled for software which meets at least one quarter of all potentially applicable standards beyond those minimally required. A useful example of minimally required standards may often be identified as those which would be required for the software to meet one specific use case. Any aspects of the software which generalise its usage beyond that single use case may be considered in the second category of potentially yet not necessarily applicable. Judgement of such categorical distinction, and of precise amounts, is left to the discretion of reviewers.\nPackages aiming for gold badges at the end of review will need to comply with all potentially applicable standards, and will also need to fulfil at least three of the four aspects listed above, and described in more detail in the Guide for Authors.\n\n5.2.2 Disagreement with Authors’ Intentions\nAuthors must state on submission the grade of badge they are aiming for. Reviewers may subsequently deem a package to be compliant with a different grade of badge. The review template includes the following two items:\n\nThis package complies with a sufficient number of standards for a (bronze/silver/gold) badge\nThis grade of badge is the same as what the authors wanted to achieve\n\nThe first item is intended to specify the grade of badge (bronze, silver, gold) which reflects the reviewer’s judgement, and need not necessarily reflect the authors intentions. The second item may be checked when a reviewer agrees with the authors that a package is indeed sufficient to achieve their desired badge. Where reviewers do not agree with authors’ beliefs on package package compliance, the second item should be left unchecked in the submitted review. The editor will then ask authors for their response, and will inform whether additional rounds of development and review are necessary to obtain the grade of badge desired by the authors.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Guide for Reviewers</span>"
    ]
  },
  {
    "objectID": "pkgreview.html#general-package-review",
    "href": "pkgreview.html#general-package-review",
    "title": "5  Guide for Reviewers",
    "section": "\n5.3 General Package Review",
    "text": "5.3 General Package Review\nFrom a reviewer’s perspective, one of the primary aim of our standards-based system is to provide a highly structured system for addressing the technical aspects of review, leaving the general review process comparably free of technical details, and therefore more able to consider broader aspects of package design, functionality, and usage.\nFollowing assessment of compliance with standards, reviewers should accordingly proceed with a general descriptive review by following the processes established in rOpenSci’s general software review system, for which the best source of information is provided by reviews themselves, along with the Guide for Reviewers. In formulating a general review of statistical software, we ask reviewers to explicitly consider the following aspects, some of which loosely correspond to sub-sections of the General Standards for Statistical Software:\n\n\nDocumentation: Is the documentation sufficient to enable general use of the package beyond one specific use case? Do the various components of documentation support and clarify one another?\n\nAlgorithms How well are algorithms encoded? Is the choice of computer language appropriate for that algorithm, and/or envisioned use of package? Are aspects of algorithmic scaling sufficiently documented and tested? Are there any aspects of algorithmic implementation which could be improved?\n\nTesting Regardless of actual coverage of tests, are there any fundamental software operations which are not sufficiently expressed in tests? Is there a need for extended tests, or if extended tests exists, have they been implemented in an appropriate way, and are they appropriately documented?\n\nVisualisation (where appropriate) Do visualisations aid the primary purposes of statistical interpretation of results? Are there any aspects of visualisations which could risk statistical misinterpretation?\n\nPackage Design Is the package well designed for its intended purpose? We ask reviewers to consider the follow two aspects of package design:\n\n\nExternal Design: Do exported functions and the relationships between them enable general usage of the package? Do exported functions best serve inter-operability with other packages?\n\nInternal design: Are algorithms implemented appropriately in terms of aspects such as efficiency, flexibility, generality, and accuracy? Could ranges of admissible input structures, or form(s) of output structures, be expanded to enhance inter-operability with other packages?\n\n\n\nAs algorithms form the core of statistical software, we ask reviewers to pay particular attention to the assessment of algorithmic quality. Most category-specific standards include a central “Algorithmic Standards” component which can be used to provide starting points for more general considerations of algorithmic quality. The General Standard G1.1 also requires all similar algorithms or implementations to be documented within the software, so reviewers should also have access to a list of comparable implementations.\nMost of the above considerations are explicitly included in the reviewers’ template which follows.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Guide for Reviewers</span>"
    ]
  },
  {
    "objectID": "pkgreview.html#pkgrev-template",
    "href": "pkgreview.html#pkgrev-template",
    "title": "5  Guide for Reviewers",
    "section": "\n5.4 Review Template",
    "text": "5.4 Review Template\nThe following template is to be used for reviews of statistical software. All checkbox items should be retained, and checked where appropriate, while other lines, notably including questions in the General Review section, may be modified or removed as appropriate.\n## Package Review\n\n*Please check off boxes as applicable, and elaborate in comments below.  Your review is not limited to these topics, as described in the reviewer guide*\n\n- Briefly describe any working relationship you may have (had) with the package authors (or otherwise remove this statement)\n\n- [ ] As the reviewer I confirm that there are no [conflicts of interest](https://devguide.ropensci.org/policies.html#coi) for me to review this work (If you are unsure whether you are in conflict, please speak to your editor _before_ starting your review).\n\n---\n\n### Compliance with Standards\n\n- [ ] This package complies with a sufficient number of standards for a (bronze/silver/gold) badge\n- [ ] This grade of badge is the same as what the authors wanted to achieve\n\nThe following standards currently deemed non-applicable (through tags of `@srrstatsNA`) could potentially be applied to future versions of this software: (Please specify)\n\nPlease also comment on any standards which you consider either particularly well, or insufficiently, documented.\n\nFor packages aiming for silver or gold badges:\n\n- [ ] This package extends beyond minimal compliance with standards in the following ways: (please describe)\n\n---\n\n### General Review\n\n#### Documentation\n\nThe package includes all the following forms of documentation:\n\n- [ ] **A statement of need** clearly stating problems the software is designed to solve and its target audience in README\n- [ ] **Installation instructions:** for the development version of package and any non-standard dependencies in README\n- [ ] **Community guidelines** including contribution guidelines in the README or CONTRIBUTING\n- [ ] The documentation is sufficient to enable general use of the package beyond one specific use case\n\nThe following sections of this template include questions intended to be used as guides to provide general, descriptive responses. Please remove this, and any subsequent lines that are not relevant or necessary for your final review.\n\n#### Algorithms\n\n- How well are algorithms encoded?\n- Is the choice of computer language appropriate for that algorithm, and/or envisioned use of package?\n- Are aspects of algorithmic scaling sufficiently documented and tested?\n- Are there any aspects of algorithmic implementation which could be improved?\n\n#### Testing\n\n- Regardless of actual coverage of tests, are there any fundamental software operations which are not sufficiently expressed in tests? \n- Is there a need for extended tests, or if extended tests exists, have they been implemented in an appropriate way, and are they appropriately documented?\n\n#### Visualisation (where appropriate)\n\n- Do visualisations aid the primary purposes of statistical interpretation of results?\n- Are there any aspects of visualisations which could risk statistical misinterpretation?\n\n#### Package Design\n\n- Is the package well designed for its intended purpose?\n- In relation to **External Design:** Do exported functions and the relationships between them enable general usage of the package? \n- In relation to **External Design:** Do exported functions best serve inter-operability with other packages?\n- In relation to **Internal Design:** Are algorithms implemented appropriately in terms of aspects such as efficiency, flexibility, generality, and accuracy? \n- In relation to **Internal Design:** Could ranges of admissible input structures, or form(s) of output structures, be expanded to enhance inter-operability with other packages?\n\n---\n\n- [ ] **Packaging guidelines**: The package conforms to the rOpenSci packaging guidelines\n\nEstimated hours spent reviewing:\n\n- [ ] Should the author(s) deem it appropriate, I agree to be acknowledged as a package reviewer (\"rev\" role) in the package DESCRIPTION file.",
    "crumbs": [
      "Package Development, Submission, and Review",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Guide for Reviewers</span>"
    ]
  },
  {
    "objectID": "standards.html",
    "href": "standards.html",
    "title": "6  Standards: Version 0.2.0",
    "section": "",
    "text": "6.1 General Standards for Statistical Software\nThis Chapter serves as the reference for rOpenSci’s standards for statistical software. Software accepted for peer-review must fit one or more of our categories, and thus all packages must comply with the General Standards listed in the first of the following sections, as well as at least one of the category-specific sets of standards listed in the subsequent sections.\nOur standards are open and intended to change and evolve in response to public feedback. Please contribute via the GitHub discussions pages for this book. We particularly encourage anybody preparing software for submission to discuss any aspects of our standards, including applicability, validity, phrasing, expectations, reasons for standards, and even the addition or removal of specific standards.\nThese general standards, and all category-specific standards that follow, are intended to serve as recommendations for best practices. Note in particular that many standards are written using the word “should” in explicit acknowledgement that adhering to such standards may not always be possible. All standards phrased in these terms are intended to be interpreted as applicable under such conditions as “Where possible”, or “Where applicable”. Developers are requested to note any standards which they deem not applicable to their software via the srr package, as described in Chapter 3.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-general",
    "href": "standards.html#standards-general",
    "title": "6  Standards: Version 0.2.0",
    "section": "",
    "text": "These standards refer to Data Types as the fundamental types defined by the R language itself. Information on these types can be seen by clicking here.\n\n\nThe R language defines the following data types:\n\nLogical\nInteger\nContinuous (class = \"numeric\" / typeof = \"double\")\nComplex\nString / character\n\nThe base R system also includes what are considered here to be direct extensions of fundamental types to include:\n\nFactor\nOrdered Factor\nDate/Time\n\nThe continuous type has a typeof of “double” because that represents the storage mode in the C representation of such objects, while the class as defined within R is referred to as “numeric”. While typeof is not the same as class, with reference to continuous variables, “numeric” may be considered identical to “double” throughout.\nThe term “character” is interpreted here to refer to a vector each element of which is an individual “character” object. The term “string” does not relate to any official R nomenclature, but is used here to refer for convenience to a character vector of length one; in other words, a “string” is the sole element of a single-length “character” vector.\n\n\n6.1.1 Documentation\n\n\nG1.0 Statistical Software should list at least one primary reference from published academic literature.\n\n\nWe consider that statistical software submitted under our system will either (i) implement or extend prior methods, in which case the primary reference will be to the most relevant published version(s) of prior methods; or (ii) be an implementation of some new method. In the second case, it will be expected that the software will eventually form the basis of an academic publication. Until that time, the most suitable reference for equivalent algorithms or implementations should be provided.\n\n\nG1.1 Statistical Software should document whether the algorithm(s) it implements are:\n\n\nThe first implementation of a novel algorithm; or\n\nThe first implementation within R of an algorithm which has previously been implemented in other languages or contexts; or\n\nAn improvement on other implementations of similar algorithms in R.\n\n\n\nThe second and third options additionally require references to comparable algorithms or implementations to be documented somewhere within the software, including references to all known implementations in other computer languages. (A common location for such is a statement of “Prior Art” or similar at the end of the main README document.)\n\n\nG1.2 Statistical Software should include a Life Cycle Statement describing current and anticipated future states of development.\n\n\nWe encourage these to placed within a repository’s CONTRIBUTING.md file, as in this example. A simple Life Cycle Statement may be formed by selecting one of the following four statements.\nThis package is\n\n    - In a stable state of development, with minimal subsequent development\n      envisioned.\n    - In a stable state of development, with active subsequent development\n      primarily in response to user feedback.\n    - In a stable state of development, with some degree of active subsequent\n      development as envisioned by the primary authors.\n    - In an initially stable state of development, with a great deal of active\n      subsequent development envisioned.\n\n6.1.1.1 Statistical Terminology\n\n\nG1.3 All statistical terminology should be clarified and unambiguously defined.\n\n\nDevelopers should not presume anywhere in the documentation of software that specific statistical terminology may be “generally understood”, and therefore not need explicit clarification. Even terms which many may consider sufficiently generic as to not require such clarification, such as “null hypotheses” or “confidence intervals”, will generally need explicit clarification. For example, both the estimation and interpretation of confidence intervals are dependent on distributional properties and associated assumptions. Any particular implementation of procedures to estimate or report on confidence intervals will accordingly reflect assumptions on distributional properties (among other aspects), both the nature and implications of which must be explicitly clarified.\n\n6.1.1.2 Function-level Documentation\n\n\nG1.4 Software should use roxygen2 to document all functions.\n\n\nG1.4a All internal (non-exported) functions should also be documented in standard roxygen2 format, along with a final @noRd tag to suppress automatic generation of .Rd files or @keywords internal if documentation is still desired.\n\n\n\n\n6.1.1.3 Supplementary Documentation\nThe following standards describe several forms of what might be considered “Supplementary Material”. While there are many places within an R package where such material may be included, common locations include vignettes, or in additional directories (such as data-raw) listed in .Rbuildignore to prevent inclusion within installed packages.\nWhere software supports a publication, all claims made in the publication with regard to software performance (for example, claims of algorithmic scaling or efficiency; or claims of accuracy), the following standard applies:\n\n\nG1.5 Software should include all code necessary to reproduce results which form the basis of performance claims made in associated publications.\n\n\nWhere claims regarding aspects of software performance are made with respect to other extant R packages, the following standard applies:\n\n\nG1.6 Software should include code necessary to compare performance claims with alternative implementations in other R packages.\n\n\n6.1.2 Input Structures\nThis section considers general standards for Input Structures. These standards may often effectively be addressed through implementing class structures, although this is not a general requirement. Developers are nevertheless encouraged to examine the guide to S3 vectors in the vctrs package as an example of the kind of assurances and validation checks that are possible with regard to input data. Systems like those demonstrated in that vignette provide a very effective way to ensure that software remains robust to diverse and unexpected classes and types of input data. Packages such checkmate enable direct and simple ways to check and assert input structures.\n\n6.1.2.1 Uni-variate (Vector) Input\nIt is important to note for univariate data that single values in R are vectors with a length of one, and that 1 is of exactly the same data type as 1:n. Given this, inputs expected to be univariate should:\n\n\nG2.0 Implement assertions on lengths of inputs, particularly through asserting that inputs expected to be single- or multi-valued are indeed so.\n\n\nG2.0a Provide explicit secondary documentation of any expectations on lengths of inputs\n\n\n\nG2.1 Implement assertions on types of inputs (see the initial point on nomenclature above).\n\n\nG2.1a Provide explicit secondary documentation of expectations on data types of all vector inputs.\n\n\n\n\nG2.2 Appropriately prohibit or restrict submission of multivariate input to parameters expected to be univariate.\n\n\nG2.3 For univariate character input:\n\n\nG2.3a Use match.arg() or equivalent where applicable to only permit expected values.\n\n\nG2.3b Either: use tolower() or equivalent to ensure input of character parameters is not case dependent; or explicitly document that parameters are strictly case-sensitive.\n\n\n\n\nG2.4 Provide appropriate mechanisms to convert between different data types, potentially including:\n\n\nG2.4a explicit conversion to integer via as.integer()\n\n\nG2.4b explicit conversion to continuous via as.numeric()\n\n\nG2.4c explicit conversion to character via as.character() (and not paste or paste0)\n\n\nG2.4d explicit conversion to factor via as.factor()\n\n\nG2.4e explicit conversion from factor via as...() functions\n\n\n\n\nG2.5 Where inputs are expected to be of factor type, secondary documentation should explicitly state whether these should be ordered or not, and those inputs should provide appropriate error or other routines to ensure inputs follow these expectations.\n\n\nA few packages implement R versions of “static type” forms common in other languages, whereby the type of a variable must be explicitly specified prior to assignment. Use of such approaches is encouraged, including but not restricted to approaches documented in packages such as vctrs, or the experimental package typed. One additional standard for vector input is:\n\n\nG2.6 Software which accepts one-dimensional input should ensure values are appropriately pre-processed regardless of class structures.\n\n\nThe units package provides a good example, in creating objects that may be treated as vectors, yet which have a class structure that does not inherit from the vector class. Using these objects as input often causes software to fail. The storage.mode of the underlying objects may nevertheless be examined, and the objects transformed or processed accordingly to ensure such inputs do not lead to errors.\n\n6.1.2.2 Tabular Input\nThis sub-section concerns input in “tabular data” forms, meaning the base R forms array, matrix, and data.frame, and other forms and classes derived from these. Tabular data generally have two dimensions, although may have more (such as for array objects). There is a primary distinction within R itself between array or matrix representations, and data.frame and associated representations. The former are restricted to storing data of a single uniform type (for example, all integer or all character values), whereas data.frame as associated representations (generally) store each column as a list item, allowing different columns to hold values of different types. Further noting that a matrix may, as of R version 4.0, be considered as a strictly two-dimensional array, tabular inputs for the purposes of these standards are considered to imply data represented in one or more of the following forms:\n\n\nmatrix form when referring to specifically two-dimensional data of one uniform type\n\narray form as a more general expression, or when referring to data that are not necessarily or strictly two-dimensional\ndata.frame\nExtensions such as\n\ntibble\ndata.table\ndomain-specific classes such as tsibble for time series, or sf for spatial data.\n\n\n\nBoth matrix and array forms are actually stored as vectors with a single storage.mode, and so all of the preceding standards G2.0–G2.5 apply. The other rectangular forms are not stored as vectors, and do not necessarily have a single storage.mode for all columns. These forms are referred to throughout these standards as “data.frame-type tabular forms”, which may be assumed to refer to data represented in either the base::data.frame format, and/or any of the classes listed in the final of the above points.\nGeneral Standards applicable to software which is intended to accept any one or more of these data.frame-type tabular inputs are then that:\n\n\nG2.7 Software should accept as input as many of the above standard tabular forms as possible, including extension to domain-specific forms.\n\n\nSoftware need not necessarily test abilities to accept different types of inputs, because that may require adding packages to the Suggests field of a package for that purpose alone. Nevertheless, software which somehow uses (through Depends or Suggests) any packages for representing tabular data should confirm in tests the ability to accept these types of input.\n\n\nG2.8 Software should provide appropriate conversion or dispatch routines as part of initial pre-processing to ensure that all other sub-functions of a package receive inputs of a single defined class or type.\n\n\nG2.9 Software should issue diagnostic messages for type conversion in which information is lost (such as conversion of variables from factor to character; standardisation of variable names; or removal of meta-data such as those associated with sf-format data) or added (such as insertion of variable or column names where none were provided).\n\n\nNote, for example, that an array may have column names which start with numeric values, but that a data.frame may not.\n\n\n  2\n1 1\n\n\n  X2\n1  1\n\n\nIf array or matrix class objects are accepted as input, then G2.8 implies that routines should be implemented to check for such conversion of column names.\nThe next standard concerns the following inconsistencies between three common tabular classes in regard the column extraction operator, [.\n\nExtracting a single column from a data.frame returns a vector by default, and a data.frame if drop = FALSE.\nExtracting a single column from a tibble returns a single-column tibble by default, and a vector if drop = TRUE.\nExtracting a single column from a data.table always returns a data.table, and the drop argument has no effect.\n\nGiven such inconsistencies,\n\n\nG2.10 Software should ensure that extraction or filtering of single columns from tabular inputs should not presume any particular default behaviour, and should ensure all column-extraction operations behave consistently regardless of the class of tabular data used as input.\n\n\nAdherence to the above standard G2.8 will ensure that any implicitly or explicitly assumed default behaviour will yield consistent results regardless of input classes.\nColumns of tabular inputs\nThe following standards apply to data.frame-like tabular objects (including all derived and otherwise compatible classes), and so do not apply to matrix or array objects.\n\n\nG2.11 Software should ensure that data.frame-like tabular objects which have columns which do not themselves have standard class attributes (typically, vector) are appropriately processed, and do not error without reason. This behaviour should be tested. Again, columns created by the units package provide a good test case.\n\n\nG2.12 Software should ensure that data.frame-like tabular objects which have list columns should ensure that those columns are appropriately pre-processed either through being removed, converted to equivalent vector columns where appropriate, or some other appropriate treatment such as an informative error. This behaviour should be tested.\n\n\n6.1.2.3 Missing or Undefined Values\n\n\nG2.13 Statistical Software should implement appropriate checks for missing data as part of initial pre-processing prior to passing data to analytic algorithms.\n\n\nG2.14 Where possible, all functions should provide options for users to specify how to handle missing (NA) data, with options minimally including:\n\n\nG2.14a error on missing data\n\n\nG2.14b ignore missing data with default warnings or messages issued\n\n\nG2.14c replace missing data with appropriately imputed values\n\n\n\n\nG2.15 Functions should never assume non-missingness, and should never pass data with potential missing values to any base routines with default na.rm = FALSE-type parameters (such as mean(), sd() or cor()).\n\n\nG2.16 All functions should also provide options to handle undefined values (e.g., NaN, Inf and -Inf), including potentially ignoring or removing such values.\n\n\n6.1.3 Algorithms\n\n\nG3.0 Statistical software should never compare floating point numbers for equality. All numeric equality comparisons should either ensure that they are made between integers, or use appropriate tolerances for approximate equality.\n\n\nThis standard applies to all computer languages included in any package. In R, values can be affirmed to be integers through is.integer(), or asserting that the storage.mode() of an object is “integer”. One way to compare numeric values with tolerance is with the all.equal() function, which accepts an additional tolerance parameter with a default for numeric comparison of sqrt(.Machine$double.eps), which is typically around e(-8–10). In other languages, including C and C++, comparisons of floating point numbers are commonly implemented by conditions such as if (abs(a - b) &lt; tol), where tol specifies the tolerance for equality.\nImportantly, R functions such as duplicated() and unique() rely on equality comparisons, and this standard extends to require that software should not apply any functions which themselves rely on equality comparisons to floating point numbers.\n\n\nG3.1 Statistical software which relies on covariance calculations should enable users to choose between different algorithms for calculating covariances, and should not rely solely on covariances from the stats::cov function.\n\n\nG3.1a The ability to use arbitrarily specified covariance methods should be documented (typically in examples or vignettes).\n\n\n\n\nEstimates of covariance can be very sensitive to outliers, and a variety of methods have been developed for “robust” estimates of covariance, implemented in such packages as rms, robust, and sandwich. Adhering to this standard merely requires an ability for a user to specify a particular covariance function, such as through an additional parameter. The stats::cov function can be used as a default, and additional packages such as the three listed here need not necessarily be listed as Imports to a package.\n\n6.1.4 Output Structures\n\n\nG4.0 Statistical Software which enables outputs to be written to local files should parse parameters specifying file names to ensure appropriate file suffices are automatically generated where not provided.\n\n\n6.1.5 Testing\nAll packages should follow rOpenSci standards on testing and continuous integration, including aiming for high test coverage. Extant R packages which may be useful for testing include testthat, tinytest, roxytest, and xpectr.\n\n6.1.5.1 Test Data Sets\n\n\nG5.0 Where applicable or practicable, tests should use standard data sets with known properties (for example, the NIST Standard Reference Datasets, or data sets provided by other widely-used R packages).\n\n\nG5.1 Data sets created within, and used to test, a package should be exported (or otherwise made generally available) so that users can confirm tests and run examples.\n\n\n6.1.5.2 Responses to Unexpected Input\n\n\nG5.2 Appropriate error and warning behaviour of all functions should be explicitly demonstrated through tests. In particular,\n\n\nG5.2a Every message produced within R code by stop(), warning(), message(), or equivalent should be unique\n\n\nG5.2b Explicit tests should demonstrate conditions which trigger every one of those messages, and should compare the result with expected values.\n\n\n\n\nG5.3 For functions which are expected to return objects containing no missing (NA) or undefined (NaN, Inf) values, the absence of any such values in return objects should be explicitly tested.\n\n\n6.1.5.3 Algorithm Tests\nFor testing statistical algorithms, tests should include tests of the following types:\n\n\nG5.4 Correctness tests to test that statistical algorithms produce expected results to some fixed test data sets (potentially through comparisons using binding frameworks such as RStata).\n\n\nG5.4a For new methods, it can be difficult to separate out correctness of the method from the correctness of the implementation, as there may not be reference for comparison. In this case, testing may be implemented against simple, trivial cases or against multiple implementations such as an initial R implementation compared with results from a C/C++ implementation.\n\n\nG5.4b For new implementations of existing methods, correctness tests should include tests against previous implementations. Such testing may explicitly call those implementations in testing, preferably from fixed-versions of other software, or use stored outputs from those where that is not possible.\n\n\nG5.4c Where applicable, stored values may be drawn from published paper outputs when applicable and where code from original implementations is not available\n\n\n\n\nG5.5 Correctness tests should be run with a fixed random seed\n\n\nG5.6 Parameter recovery tests to test that the implementation produce expected results given data with known properties. For instance, a linear regression algorithm should return expected coefficient values for a simulated data set generated from a linear model.\n\n\nG5.6a Parameter recovery tests should generally be expected to succeed within a defined tolerance rather than recovering exact values.\n\n\nG5.6b Parameter recovery tests should be run with multiple random seeds when either data simulation or the algorithm contains a random component. (When long-running, such tests may be part of an extended, rather than regular, test suite; see G5.10-4.12, below).\n\n\n\n\nNote that authors should ensure that they use at least v3 of the testthat package, which introduced a testthat_tolerance(), defaulting to the value defined by base::all_equal() of sqrt(.Machine$double.eps) on all expect_equal() expectations.\n\n\nG5.7 Algorithm performance tests to test that implementation performs as expected as properties of data change. For instance, a test may show that parameters approach correct estimates within tolerance as data size increases, or that convergence times decrease for higher convergence thresholds.\n\n\nG5.8 Edge condition tests to test that these conditions produce expected behaviour such as clear warnings or errors when confronted with data with extreme properties including but not limited to:\n\n\nG5.8a Zero-length data\n\n\nG5.8b Data of unsupported types (e.g., character or complex numbers in for functions designed only for numeric data)\n\n\nG5.8c Data with all-NA fields or columns or all identical fields or columns\n\n\nG5.8d Data outside the scope of the algorithm (for example, data with more fields (columns) than observations (rows) for some regression algorithms)\n\n\n\n\nG5.9 Noise susceptibility tests Packages should test for expected stochastic behaviour, such as through the following conditions:\n\n\nG5.9a Adding trivial noise (for example, at the scale of .Machine$double.eps) to data does not meaningfully change results\n\n\nG5.9b Running under different random seeds or initial conditions does not meaningfully change results\n\n\n\n\n6.1.5.4 Extended tests\nThorough testing of statistical software may require tests on large data sets, tests with many permutations, or other conditions leading to long-running tests. In such cases it may be neither possible nor advisable to execute tests continuously, or with every code change. Software should nevertheless test any and all conditions regardless of how long tests may take, and in doing so should adhere to the following standards:\n\n\nG5.10 Extended tests should included and run under a common framework with other tests but be switched on by flags such as as a &lt;MYPKG&gt;_EXTENDED_TESTS=\"true\" environment variable.\n\n\nThe extended tests can be then run automatically by GitHub Actions for example by adding the following to the env section of the workflow:\nMYPKG_EXTENDED_TESTS: ${{contains(github.event.head_commit.message, 'run-extended')}}\nExtended tests will then be run in response to any commit message which contains the phrase run-extended.\n\n\n\n\nG5.11 Where extended tests require large data sets or other assets, these should be provided for downloading and fetched as part of the testing workflow.\n\n\nG5.11a When any downloads of additional data necessary for extended tests fail, the tests themselves should not fail, rather be skipped and implicitly succeed with an appropriate diagnostic message.\n\n\n\n\nG5.12 Any conditions necessary to run extended tests such as platform requirements, memory, expected runtime, and artefacts produced that may need manual inspection, should be described in developer documentation such as a CONTRIBUTING.md or tests/README.md file.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-bayesian",
    "href": "standards.html#standards-bayesian",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.2 Bayesian and Monte Carlo Software",
    "text": "6.2 Bayesian and Monte Carlo Software\nBayesian and Monte Carlo software centres on quantitative estimation of components of Baye’s theorem, particularly on estimation or application of prior and/or posterior probability distributions. The procedures implemented to estimate the properties of such distributions are commonly based on random sampling procedures, hence referred to as “Monte Carlo” routines in reference to the random yet quantifiable nature of casino games. The scope of this category also includes algorithms which focus on sampling routines only, such as Markov-Chain Monte Carlo (MCMC) procedures, independent of application in Bayesian analyses.\nThe term “model” is understood with reference here to Bayesian software to refer to an encoded description of how parameters specifying aspects of one or more prior distributions are transformed into (properties of) one or more posterior distributions.\nSome examples of Bayesian and Monte Carlo software include:\n\nThe bayestestR package which “provides tools to describe … posterior distributions”\nThe ArviZ package python package for exploratory analyses of Bayesian models, particularly posterior distributions.\nThe GammaGompertzCR package, which features explicit diagnostics of MCMC convergence statistics.\nThe BayesianNetwork package, which is in many ways a wrapper package primarily serving a shiny app, and is also accordingly a package in the EDA category.\nThe fmcmc package, which is a “classic” MCMC package which directly provides its own implementation, and generates its own convergence statistics.\nThe rsimsum package which “summarise[s] results from Monte Carlo simulation studies”. Many of the statistics generated by this package are useful for assessing and comparing Bayesian and Monte Carlo software in general. (See also the MCMCvis package, with more of a focus on visualisation.)\nThe walkr package for “MCMC Sampling from Non-Negative Convex Polytopes”. This package is also indicative of the difficulties of deriving generally applicable assessments of software in this category, because MCMC sampling relies on fundamentally different inputs and outputs than many other MCMC routines.\n\nClick on the following link to view a demonstration Application of Bayesian and Monte Carlo Standards.\nBayesian and Monte Carlo Software (hereafter referred to for simplicity as “Bayesian Software”) is presumed to perform one or more of the following steps:\n\nDocument how to specify inputs including:\n\n1.1 Data\n1.2 Parameters determining prior distributions\n1.3 Parameters determining the computational processes\n\n\nAccept and validate all of forms of input\nApply data transformation and pre-processing steps\nApply one or more analytic algorithms, generally sampling algorithms used to generate estimates of posterior distributions\nReturn the result of that algorithmic application\nOffer additional functionality such as printing or summarising return results\n\nThis chapter details standards for each of these steps, each prefixed with “BS”.\n\n6.2.1 Documentation of Inputs\nPrior to actual standards for documentation of inputs, we note one terminological standard for Bayesian software which uses the term “hyperparameter”:\n\n\nBS1.0 Bayesian software which uses the term “hyperparameter” should explicitly clarify the meaning of that term in the context of that software.\n\n\nThis standard reflects the dual facts that this term is frequently used in Bayesian software, yet has no unambiguous definition or interpretation. The term “hyperparameter” is also used in other statistical contexts in ways that are often distinctly different from its common use in Bayesian analyses. Examples of the kinds of clarifications required to adhere to this standard include,\n\nHyperparameters refer here to parameters determining the form of prior distributions that conditionally depend on other parameters.\n\nSuch a clarification would then require further explicit distinction between “parameters” and “hyperparameters”. The remainder of these standards does not refer to “hyperparameters”, rather attempts to make explicit distinctions between different kinds of parameters, such as distributional or algorithmic control parameters. Beyond this standard, Bayesian Software should provide the following documentation of how to specify inputs:\n\n\nBS1.1 Descriptions of how to enter data, both in textual form and via code examples. Both of these should consider the simplest cases of single objects representing independent and dependent data, and potentially more complicated cases of multiple independent data inputs.\n\n\nBS1.2 Description of how to specify prior distributions, both in textual form describing the general principles of specifying prior distributions, along with more applied descriptions and examples, within:\n\n\nBS1.2a The main package README, either as textual description or example code\n\n\nBS1.2b At least one package vignette, both as general and applied textual descriptions, and example code\n\n\nBS1.2c Function-level documentation, preferably with code included in examples\n\n\n\n\nBS1.3 Description of all parameters which control the computational process (typically those determining aspects such as numbers and lengths of sampling processes, seeds used to start them, thinning parameters determining post-hoc sampling from simulated values, and convergence criteria). In particular:\n\n\nBS1.3a Bayesian Software should document, both in text and examples, how to use the output of previous simulations as starting points of subsequent simulations.\n\n\nBS1.3b Where applicable, Bayesian software should document, both in text and examples, how to use different sampling algorithms for a given model.\n\n\n\n\nBS1.4 For Bayesian Software which implements or otherwise enables convergence checkers, documentation should explicitly describe and provide examples of use with and without convergence checkers.\n\n\nBS1.5 For Bayesian Software which implements or otherwise enables multiple convergence checkers, differences between these should be explicitly tested.\n\n\n6.2.2 Input Data Structures and Validation\nThis section contains standards primarily intended to ensure that input data, including model specifications, are validated prior to passing through to the main computational algorithms.\n\n6.2.2.1 Input Data\nBayesian Software is commonly designed to accept generic one- or two-dimensional forms such as vector, matrix, or data.frame objects, for which the following standard applies.\n\n\nBS2.1 Bayesian Software should implement pre-processing routines to ensure all input data is dimensionally commensurate, for example by ensuring commensurate lengths of vectors or numbers of rows of tabular inputs.\n\n\nBS2.1a The effects of such routines should be tested.\n\n\n\n\n6.2.2.2 Prior Distributions, Model Specifications, and Distributional Parameters\nThe second set of standards in this section concern specification of prior distributions, model structures, or other equivalent ways of specifying hypothesised relationships among input data structures. R already has a diverse range of Bayesian Software with distinct approaches to this task, commonly either through specifying a model as a character vector representing an R function, or an external file either as R code, or encoded according to some alternative system (such as for rstan).\nBayesian Software should:\n\n\nBS2.2 Ensure that all appropriate validation and pre-processing of distributional parameters are implemented as distinct pre-processing steps prior to submitting to analytic routines, and especially prior to submitting to multiple parallel computational chains.\n\n\nBS2.3 Ensure that lengths of vectors of distributional parameters are checked, with no excess values silently discarded (unless such output is explicitly suppressed, as detailed below).\n\n\nBS2.4 Ensure that lengths of vectors of distributional parameters are commensurate with expected model input (see example immediately below)\n\n\nBS2.5 Where possible, implement pre-processing checks to validate appropriateness of numeric values submitted for distributional parameters; for example, by ensuring that distributional parameters defining second-order moments such as distributional variance or shape parameters, or any parameters which are logarithmically transformed, are non-negative.\n\n\nThe following example demonstrates how standards like the above (BS2.4-2.5) might be addressed. Consider the following function which defines a log-likelihood estimator for a linear regression, controlled via a vector of three distributional parameters, p:\nll &lt;- function (x, y, p) dnorm (y - (p[1] + x * p[2]), sd = p[3], log = TRUE)\nPre-processing stages should be used to determine:\n\nThat the dimensions of the input data, x and y, are commensurate (BS2.1); non-commensurate inputs should error by default.\nThe length of the vector p (BS2.3)\n\nThe latter task is not necessarily straightforward, because the definition of the function, ll(), will itself generally be part of the input to an actual Bayesian Software function. This functional input thus needs to be examined to determine expected lengths of hyperparameter vectors. The following code illustrates one way to achieve this, relying on utilities for parsing function calls in R, primarily through the getParseData function from the utils package. The parse data for a function can be extracted with the following line:\nx &lt;- getParseData (parse (text = deparse (ll)))\nThe object x is a data.frame of every R token (such as an expression, symbol, or operator) parsed from the function ll. The following section illustrates how this data can be used to determine the expected lengths of vector inputs to the function, ll().\n\nclick to see details\n\n\nInput arguments used to define parameter vectors in any R software are accessed through R’s standard vector access syntax of vec[i], for some element i of a vector vec. The parse data for such begins with the SYMBOL of vec, the [, a NUM_CONST for the value of i, and a closing ]. The following code can be used to extract elements of the parse data which match this pattern, and ultimately to extract the various values of i used to access members of vec.\nvector_length &lt;- function (x, i) {\n    xn &lt;- x [which (x$token %in% c (\"SYMBOL\", \"NUM_CONST\", \"'['\", \"']'\")), ]\n    # split resultant data.frame at first \"SYMBOL\" entry\n    xn &lt;- split (xn, cumsum (xn$token == \"SYMBOL\"))\n    # reduce to only those matching the above pattern\n    xn &lt;- xn [which (vapply (xn, function (j)\n                             j$text [1] == i & nrow (j) &gt; 3,\n                             logical (1)))]\n    ret &lt;- NA_integer_ # default return value\n    if (length (xn) &gt; 0) {\n        # get all values of NUM_CONST as integers\n        n &lt;- vapply (xn, function (j)\n                         as.integer (j$text [j$token == \"NUM_CONST\"] [1]),\n                         integer (1), USE.NAMES = FALSE)\n        # and return max of these\n        ret &lt;- max (n)\n    }\n    return (ret)\n}\nThat function can then be used to determine the length of any inputs which are used as hyperparameter vectors:\nll &lt;- function (p, x, y) dnorm (y - (p[1] + x * p[2]), sd = p[3], log = TRUE)\np &lt;- parse (text = deparse (ll))\nx &lt;- utils::getParseData (p)\n\n# extract the names of the parameters:\nparams &lt;- unique (x$text [x$token == \"SYMBOL\"])\nlens &lt;- vapply (params, function (i) vector_length (x, i), integer (1))\nlens\n#&gt;  y  p  x \n#&gt; NA  3 NA\nAnd the vector p is used as a hyperparameter vector containing three parameters. Any initial value vectors can then be examined to ensure that they have this same length.\n\nNot all Bayesian Software is designed to accept model inputs expressed as R code. The rstan package, for example, implements its own model specification language, and only allows distributional parameters to be named, and not addressed by index. While this largely avoids problems of mismatched lengths of parameter vectors, the software (at v2.21.1) does not ensure the existence of named parameters prior to starting the computational chains. This ultimately results in each chain generating an error when a model specification refers to a non-existent or undefined distributional parameter. Such controls should be part of a single pre-processing stage, and so should only generate a single error.\n\n6.2.2.3 Computational Parameters\nComputational parameters are considered here distinct from distributional parameters, and commonly passed to Bayesian functions to directly control computational processes. They typically include parameters controlling lengths of runs, lengths of burn-in periods, numbers of parallel computations, other parameters controlling how samples are to be generated, or convergence criteria. All Computational Parameters should be checked for general “sanity” prior to calling primary computational algorithms. The standards for such sanity checks include that Bayesian Software should:\n\n\nBS2.6 Check that values for computational parameters lie within plausible ranges.\n\n\nWhile admittedly not always possible to define, plausible ranges may be as simple as ensuring values are greater than zero. Where possible, checks should nevertheless ensure appropriate responses to extremely large values, for example by issuing diagnostic messages about likely long computational times. The following two sub-sections consider particular cases of computational parameters.\n\n6.2.2.4 Parameters Controlling Start Values\nBayesian software generally relies on sequential random sampling procedures, with each sequence uniquely determined by (among other aspects) the value at which it is started. Given that, Bayesian software should:\n\n\nBS2.7 Enable starting values to be explicitly controlled via one or more input parameters, including multiple values for software which implements or enables multiple computational “chains.”\n\n\nBS2.8 Enable results of previous runs to be used as starting points for subsequent runs.\n\n\nBayesian Software which implements or enables multiple computational chains should:\n\n\nBS2.9 Ensure each chain is started with a different seed by default.\n\n\nBS2.10 Issue diagnostic messages when identical seeds are passed to distinct computational chains.\n\n\nBS2.11 Software which accepts starting values as a vector should provide the parameter with a plural name: for example, “starting_values” and not “starting_value”.\n\n\nTo avoid potential confusion between separate parameters to control random seeds and starting values, we recommended a single “starting values” rather than “seeds” argument, with appropriate translation of these parameters into seeds where necessary.\n\n6.2.2.5 Output Verbosity\nAll Bayesian Software should implement computational parameters to control output verbosity. Bayesian computations are often time-consuming, and often performed as batch computations. The following standards should be adhered to in regard to output verbosity:\n\n\nBS2.12 Bayesian Software should implement at least one parameter controlling the verbosity of output, defaulting to verbose output of all appropriate messages, warnings, errors, and progress indicators.\n\n\nBS2.13 Bayesian Software should enable suppression of messages and progress indicators, while retaining verbosity of warnings and errors. This should be tested.\n\n\nBS2.14 Bayesian Software should enable suppression of warnings where appropriate. This should be tested.\n\n\nBS2.15 Bayesian Software should explicitly enable errors to be caught, and appropriately processed either through conversion to warnings, or otherwise captured in return values. This should be tested.\n\n\n6.2.3 Pre-processing and Data Transformation\n\n6.2.3.1 Missing Values\nIn additional to the General Standards for missing values (G2.13–2.16), and in particular G2.13, Bayesian Software should:\n\n\nBS3.0 Explicitly document assumptions made in regard to missing values; for example that data is assumed to contain no missing (NA, Inf) values, and that such values, or entire rows including any such values, will be automatically removed from input data.\n\n\n6.2.3.2 Perfect Collinearity\nWhere appropriate, Bayesian Software should:\n\n\nBS3.1 Implement pre-processing routines to diagnose perfect collinearity, and provide appropriate diagnostic messages or warnings\n\n\nBS3.2 Provide distinct routines for processing perfectly collinear data, potentially bypassing sampling algorithms\n\n\nAn appropriate test for BS3.2 would confirm that system.time() or equivalent timing expressions for perfectly collinear data should be less than equivalent routines called with non-collinear data. Alternatively, a test could ensure that perfectly collinear data passed to a function with a stopping criteria generated no results, while specifying a fixed number of iterations may generate results.\n\n6.2.4 Analytic Algorithms\nAs mentioned, analytic algorithms for Bayesian Software are commonly algorithms to simulate posterior distributions, and to draw samples from those simulations. Numerous extant R packages implement and offer sampling algorithms, and not all Bayesian Software will internally implement sampling algorithms. The following standards apply to packages which do implement internal sampling algorithms:\n\n\nBS4.0 Packages should document sampling algorithms (generally via literary citation, or reference to other software)\n\n\nBS4.1 Packages should provide explicit comparisons with external samplers which demonstrate intended advantage of implementation (generally via tests, vignettes, or both).\n\n\nRegardless of whether or not Bayesian Software implements internal sampling algorithms, it should:\n\n\nBS4.2 Implement at least one means to validate posterior estimates.\n\n\nAn example of posterior validation is the Simulation Based Calibration approach implemented in the rstan function sbc). (Note also that the BayesValidate package has not been updated for almost 15 years, so should not be directly used, although ideas from that package may be adapted for validation purposes.) Beyond this, where possible or applicable, Bayesian Software should:\n\n\nBS4.3 Implement or otherwise offer at least one type of convergence checker, and provide a documented reference for that implementation.\n\n\nBS4.4 Enable computations to be stopped on convergence (although not necessarily by default).\n\n\nBS4.5 Ensure that appropriate mechanisms are provided for models which do not converge.\n\n\nThis is often achieved by having default behaviour to stop after specified numbers of iterations regardless of convergence.\n\n\nBS4.6 Implement tests to confirm that results with convergence checker are statistically equivalent to results from equivalent fixed number of samples without convergence checking.\n\n\nBS4.7 Where convergence checkers are themselves parametrised, the effects of such parameters should also be tested. For threshold parameters, for example, lower values should result in longer sequence lengths.\n\n\n6.2.5 Return Values\nUnlike software in many other categories, Bayesian Software should generally return several kinds of distinct data, both the raw data derived from statistical algorithms, and associated metadata. Such distinct and generally disparate forms of data will be generally best combined into a single object through implementing a defined class structure, although other options are possible, including (re-)using extant class structures (see the CRAN Task view on Bayesian Inference for reference to other packages and class systems). Regardless of the precise form of return object, and whether or not defined class structures are used or implemented, the following standards apply:\n\n\nBS5.0 Return values should include starting value(s) or seed(s), including values for each sequence where multiple sequences are included\n\n\nBS5.1 Return values should include appropriate metadata on types (or classes) and dimensions of input data\n\n\nThe latter standard may also include returning a unique hash computed from the input data, to enable results to be uniquely associated with that input data. With regard to the input function, or alternative means of specifying prior distributions:\n\n\nBS5.2 Bayesian Software should either return the input function or prior distributional specification in the return object; or enable direct access to such via additional functions which accept the return object as single argument.\n\n\nWhere convergence checkers are implemented or provided:\n\n\nBS5.3 Bayesian Software should return convergence statistics or equivalent\n\n\nBS5.4 Where multiple checkers are enabled, Bayesian Software should return details of convergence checker used\n\n\nBS5.5 Appropriate diagnostic statistics to indicate absence of convergence should either be returned or immediately able to be accessed.\n\n\n6.2.6 Additional Functionality\nWith regard to additional methods implemented for, or dispatched on, return objects:\n\n\nBS6.0 Software should implement a default print method for return objects\n\n\nBS6.1 Software should implement a default plot method for return objects\n\n\nBS6.2 Software should provide and document straightforward abilities to plot sequences of posterior samples, with burn-in periods clearly distinguished\n\n\nBS6.3 Software should provide and document straightforward abilities to plot posterior distributional estimates\n\n\nBeyond these points:\n\n\nBS6.4 Software may provide summary methods for return objects\n\n\nBS6.5 Software may provide abilities to plot both sequences of posterior samples and distributional estimates together in single graphic\n\n\n6.2.7 Tests\n\n6.2.7.1 Parameter Recovery Tests\nBayesian software should implement the following parameter recovery tests:\n\n\nBS7.0 Software should demonstrate and confirm recovery of parametric estimates of a prior distribution\n\n\nBS7.1 Software should demonstrate and confirm recovery of a prior distribution in the absence of any additional data or information\n\n\nBS7.2 Software should demonstrate and confirm recovery of a expected posterior distribution given a specified prior and some input data\n\n\n6.2.7.2 Algorithmic Scaling Tests\n\n\nBS7.3 Bayesian software should include tests which demonstrate and confirm the scaling of algorithmic efficiency with sizes of input data.\n\n\nAn example of adhering to this standard would be documentation or tests which demonstrate or confirm that computation times increase approximately logarithmically with increasing sizes of input data.\n\n6.2.7.3 Scaling of Input to Output Data\n\n\nBS7.4 Bayesian software should implement tests which confirm that predicted or fitted values are on (approximately) the same scale as input values.\n\n\nBS7.4a The implications of any assumptions on scales on input objects should be explicitly tested in this context; for example that the scales of inputs which do not have means of zero will not be able to be recovered.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-eda",
    "href": "standards.html#standards-eda",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.3 Exploratory Data Analysis and Summary Statistics",
    "text": "6.3 Exploratory Data Analysis and Summary Statistics\nExploration is a part of all data analyses, and Exploratory Data Analysis (EDA) is not something that is entered into and exited from at some point prior to “real” analysis. Exploratory Analyses are also not strictly limited to Data, but may extend to exploration of Models of those data. The category could thus equally be termed, “Exploratory Data and Model Analysis”, yet we opt to utilise the standard acronym of EDA in this document.\nSummary statistics are generally intended to aid data exploration, and software providing summary statistics is also considered here as a form of EDA software. For simplicity, both kinds of software are referred to throughout these standards as “EDA software”, a phrase intended at all times to also encompass summary statistics software.\nThe category of EDA is somewhat different to many other categories considered here. Primary differences include:\n\nEDA software often has a strong focus upon visualization, which is a category which we have otherwise explicitly excluded from the scope of the project at the present stage.\nThe assessment of EDA software requires addressing more general questions than software in most other categories, notably including the important question of intended audience(s).\n\nExamples of EDA software include:\n\nA package rejected by rOpenSci as out-of-scope, gtsummary, which provides, “Presentation-ready data summary and analytic result tables.”\nThe smartEDA package (with accompanying JOSS paper) “for automated exploratory data analysis”. The package, “automatically selects the variables and performs the related descriptive statistics. Moreover, it also analyzes the information value, the weight of evidence, custom tables, summary statistics, and performs graphical techniques for both numeric and categorical variables.” This package is potentially as much a workflow package as it is a statistical reporting package, and illustrates the ambiguity between these two categories.\nThe modeLLtest package (with accompanying JOSS paper) is “An R Package for Unbiased Model Comparison using Cross Validation.” Its main functionality allows different statistical models to be compared, likely implying that this represents a kind of meta package.\nThe insight package (with accompanying JOSS paper) provides “a unified interface to access information from model objects in R,” with a strong focus on unified and consistent reporting of statistical results.\nThe arviz software for python (with accompanying JOSS paper) provides “a unified library for exploratory analysis of Bayesian models in Python.”\nThe iRF package (with accompanying JOSS paper) enables “extracting interactions from random forests”, yet also focusses primarily on enabling interpretation of random forests through reporting on interaction terms.\n\nClick on the following link to view a demonstration Application of Exploratory Data Analysis Standards.\nReflecting these considerations, the following standards are somewhat differently structured than equivalent standards developed to date for other categories, particularly through being more qualitative and abstract. In particular, while documentation is an important component of standards for all categories, clear and instructive documentation is of paramount importance for EDA Software, and so warrants its own sub-section within this document.\n\n6.3.1 Documentation Standards\nThe following refer to Primary Documentation, implying in main package README or vignette(s), and Secondary Documentation, implying function-level documentation.\nThe Primary Documentation (README and/or vignette(s)) of EDA software should:\n\n\nEA1.0 Identify one or more target audiences for whom the software is intended\n\n\nEA1.1 Identify the kinds of data the software is capable of analysing (see Kinds of Data* below).*\n\nEA1.2 Identify the kinds of questions the software is intended to help explore.\n\n\nImportant distinctions between kinds of questions include whether they are inferential, predictive, associative, causal, or representative of other modes of statistical enquiry. The Secondary Documentation (within individual functions) of EDA software should:\n\n\nEA1.3 Identify the kinds of data each function is intended to accept as input\n\n\n6.3.2 Input Data\nA further primary difference of EDA software from that of our other categories is that input data for statistical software may be generally presumed of one or more specific types, whereas EDA software often accepts data of more general and varied types. EDA software should aim to accept and appropriately transform as many diverse kinds of input data as possible, through addressing the following standards, considered in terms of the two cases of input data in uni- and multi-variate form. All of the general standards for kinds of input (G2.0 - G2.12) apply to input data for EDA Software.\n\n6.3.2.1 Index Columns\nThe following standards refer to an index column, which is understood to imply an explicitly named or identified column which can be used to provide a unique index index into any and all rows of that table. Index columns ensure the universal applicability of standard table join operations, such as those implemented via the dplyr package.\n\n\nEA2.0 EDA Software which accepts standard tabular data and implements or relies upon extensive table filter and join operations should utilise an index column system\n\n\nEA2.1 All values in an index column must be unique, and this uniqueness should be affirmed as a pre-processing step for all input data.\n\n\nEA2.2 Index columns should be explicitly identified, either:\n\n\nEA2.2a by using an appropriate class system, or\n\n\nEA2.2b through setting an attribute on a table, x, of attr(x, \"index\") &lt;- &lt;index_col_name&gt;.\n\n\n\n\nFor EDA software which either implements custom classes or explicitly sets attributes specifying index columns, these attributes should be used as the basis of all table join operations, and in particular:\n\n\nEA2.3 Table join operations should not be based on any assumed variable or column names\n\n\n6.3.2.2 Multi-tabular input\nEDA software designed to accept multi-tabular input should:\n\n\nEA2.4 Use and demand an explicit class system for such input (for example, via the DM package).\n\n\nEA2.5 Ensure all individual tables follow the above standards for Index Columns\n\n\n6.3.2.3 Classes and Sub-Classes\nClasses are understood here to be the classes define single input objects, while Sub-Classes refer to the class definitions of components of input objects (for example, of columns of an input data.frame). EDA software which is intended to receive input in general vector formats (see Uni-variate Input section of General Standards) should ensure that it complies with G2., so that vector input is appropriately processed regardless of input class. An additional standard for EDA software is that,\n\n\nEA2.6 Routines should appropriately process vector data regardless of additional attributes\n\n\nThe following code illustrates some ways by which “metadata” defining classes and additional attributes associated with a standard vector object may by modified.\nx &lt;- 1:10\nclass (x) &lt;- \"notvector\"\nattr (x, \"extra_attribute\") &lt;- \"another attribute\"\nattr (x, \"vector attribute\") &lt;- runif (5)\nattributes (x)\n#&gt; $class\n#&gt; [1] \"notvector\"\n#&gt; \n#&gt; $extra_attribute\n#&gt; [1] \"another attribute\"\n#&gt; \n#&gt; $`vector attribute`\n#&gt; [1] 0.03521663 0.49418081 0.60129563 0.75804346 0.16073301\nAll statistical software should appropriately deal with such input data, as exemplified by the storage.mode(), length(), and sum() functions of the base package, which return the appropriate values regardless of redefinition of class or additional attributes.\nstorage.mode (x)\n#&gt; [1] \"integer\"\nlength (x)\n#&gt; [1] 10\nsum (x)\n#&gt; [1] 55\nstorage.mode (sum (x))\n#&gt; [1] \"integer\"\nTabular inputs in data.frame class may contain columns which are themselves defined by custom classes, and which possess additional attributes. The ability of software to accept such inputs is covered by the Tabular Input section of the General Standards.\n\n6.3.3 Analytic Algorithms\nEDA software will generally not directly implement what might be considered as statistical algorithms in their own right. Where algorithms are implemented, the following standards apply.\n\n\nEA3.0 The algorithmic components of EDA Software should enable automated extraction and/or reporting of statistics as some sufficiently “meta” level (such as variable or model selection), for which previous or reference implementations require manual intervention.\n\n\nEA3.1 EDA software should enable standardised comparison of inputs, processes, models, or outputs which previous or reference implementations otherwise only enable in some comparably unstandardised form.\n\n\nBoth of these standards also relate to the following standards for output values, visualisation, and summary output.\n\n6.3.4 Return Results / Output Data\n\n\nEA4.0 EDA Software should ensure all return results have types which are consistent with input types.\n\n\nExamples of such compliance include ensuring that sum, min, or max values applied to integer-type vectors return integer values.\n\n\nEA4.1 EDA Software should implement parameters to enable explicit control of numeric precision\n\n\nEA4.2 The primary routines of EDA Software should return objects for which default print and plot methods give sensible results. Default summary methods may also be implemented.\n\n\n6.3.5 Visualization and Summary Output\nVisualization commonly represents one of the primary functions of EDA Software, and thus visualization output is given greater consideration in this category than in other categories in which visualization may nevertheless play an important role. In particular, one component of this sub-category is Summary Output, taken to refer to all forms of screen-based output beyond conventional graphical output, including tabular and other text-based forms. Standards for visualization itself are considered in the two primary sub-categories of static and dynamic visualization, where the latter includes interactive visualization.\nPrior to these individual sub-categories, we consider a few standards applicable to visualization in general, whether static or dynamic.\n\n\nEA5.0 Graphical presentation in EDA software should be as accessible as possible or practicable. In particular, EDA software should consider accessibility in terms of:\n\n\nEA5.0a Typeface sizes, which should default to sizes which explicitly enhance accessibility\n\n\nEA5.0b Default colour schemes, which should be carefully constructed to ensure accessibility.\n\n\n\n\nEA5.1 Any explicit specifications of typefaces which override default values provided through other packages (including the graphics package) should consider accessibility\n\n\n\n6.3.5.1 Summary and Screen-based Output\n\n\nEA5.2 Screen-based output should never rely on default print formatting of numeric types, rather should also use some version of round(., digits), formatC, sprintf, or similar functions for numeric formatting according the parameter described in EA4.1.\n\nEA5.3 Column-based summary statistics should always indicate the storage.mode, class, or equivalent defining attribute of each column.\n\n\nAn example of compliance with the latter standard is the print.tibble method of the tibble package.\n\n6.3.5.2 General Standards for Visualization (Static and Dynamic)\n\n\nEA5.4 All visualisations should ensure values are rounded sensibly (for example, via pretty() function).\n\n\nEA5.5 All visualisations should include units on all axes where such are specified or otherwise obtainable from input data or other routines.\n\n\n6.3.5.3 Dynamic Visualization\nDynamic visualization routines are commonly implemented as interfaces to javascript routines. Unless routines have been explicitly developed as an internal part of an R package, standards shall not be considered to apply to the code itself, rather only to decisions present as user-controlled parameters exposed within the R environment. That said, one standard may nevertheless be applied, which aims to maximise inter-operability between packages.\n\n\nEA5.6 Any packages which internally bundle libraries used for dynamic visualization and which are also bundled in other, pre-existing R packages, should explain the necessity and advantage of re-bundling that library.\n\n\n6.3.6 Testing\n\n6.3.6.1 Return Values\n\n\nEA6.0 Return values from all functions should be tested, including tests for the following characteristics:\n\n\nEA6.0a Classes and types of objects\n\n\nEA6.0b Dimensions of tabular objects\n\n\nEA6.0c Column names (or equivalent) of tabular objects\n\n\nEA6.0d Classes or types of all columns contained within data.frame-type tabular objects \n\n\nEA6.0e Values of single-valued objects; for numeric values either using testthat::expect_equal() or equivalent with a defined value for the tolerance parameter, or using round(...,     digits = x) with some defined value of x prior to testing equality.\n\n\n\n\n6.3.6.2 Graphical Output\n\n\nEA6.1 The properties of graphical output from EDA software should be explicitly tested, for example via the vdiffr package or equivalent.\n\n\nTests for graphical output are frequently only run as part of an extended test suite.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-ml",
    "href": "standards.html#standards-ml",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.4 Machine Learning Software",
    "text": "6.4 Machine Learning Software\nR has an extensive and diverse ecosystem of Machine Learning (ML) software which is very well described in the corresponding CRAN Task View. Unlike most other categories of statistical software considered here, the primary distinguishing feature of ML software is not (necessarily or directly) algorithmic, rather pertains to a workflow typical of machine learning tasks. In particular, we consider ML software to approach data analysis via the two primary steps of:\n\nPassing a set of training data to an algorithm in order to generate a candidate mapping between that data and some form of pre-specified output or response variable. Such mappings will be referred to here as “models”, with a single analysis of a single set of training data generating one model.\nPassing a set of test data to the model(s) generated by the first step in order to derive some measure of predictive accuracy for that model.\n\nA single ML task generally yields two distinct outputs:\n\nThe model derived in the first of the previous steps; and\nAssociated statistics of model performance, as evaluated within the context of the test data used to assess that performance.\n\nClick on the following link to view a demonstration Application of Machine Learning Software Standards.\nA Machine Learning Workflow\nGiven those initial considerations, we now attempt the difficult task of envisioning a typical standard workflow for inherently diverse ML software. The following workflow ought to be considered an “extensive” workflow, with shorter versions, and correspondingly more restricted sets of standards, possible dependent upon envisioned areas of application. For example, the workflow presumes input data to be too large to be stored as a single entity in local memory. Adaptation to situations in which all training data can be loaded into memory may mean that some of the following workflow stages, and therefore corresponding standards, may not apply.\nJust as typical workflows are potentially very diverse, so are outputs of ML software, which depend on areas of application and intended purpose of software. The following refers to the “desired output” of ML software, a phrase which is intentionally left non-specific, but which it intended to connote any and all forms of “response variable” and other “pre-specified outputs” such as categorical labels or validation data, along with outputs which may not necessarily be able to be pre-specified in simple uni- or multi-variate form, such as measures of distance between sets of training and validation data.\nSuch “desired outputs” are presumed to be quantified in terms of a “loss” or “cost” function (hereafter, simply “loss function”) quantifying some measure of distance between a model estimate (resulting from applying the model to one or more components of a training data set) and a pre-defined “valid” output (during training), or a test data set (following training).\nGiven the foregoing considerations, we consider a typical ML workflow to progress through (at least some of) the following steps:\n\n\nInput Data Specification Obtain a local copy of input data, often as multiple objects (either on-disk or in memory) in some suitably structured form such as in a series of sub-directories or accompanied by additional data defining the structural properties of input objects. Regardless of form, multiple objects are commonly given generic labels which distinguish between training and test data, along with optional additional categories and labels such as validation data used, for example, to determine accuracy of models applied to training data yet prior to testing.\n\nPre-Processing Define transformations of input data, including but not restricted to, broadcasting dimensions (as defined below) and standardising data ranges (typically to defined values of mean and standard deviation).\n\nModel and Algorithm Specification Specify the model and associated processes which will be applied to map the input data on to the desired output. This step minimally includes the following distinct stages (generally in no particular order):\n\nSpecify the kind of model which will be applied to the training data. ML software often allows the use of pre-trained models, in which case this this step includes downloading or otherwise obtaining a pre-trained model, along with specification of which aspects of those models are to be modified through application to a particular set of training and validation data.\nSpecify the kind of algorithm which will be used to explore the search space (for example some kind of gradient descent algorithm), along with parameters controlling how that algorithm will be applied (for example a learning rate, as defined above).\nSpecify the kind of loss function will be used to quantify distance between model estimates and desired output.\n\n\n\nModel Training Apply the specified model to the training data to generate a series of estimates from the specified loss function. This stage may also include specifying parameters such as stopping or exit criteria, and parameters controlling batch processing of input data. Moreover, this stage may involve retaining some of the following additional data:\n\nPotential “pre-processing” stages such as initial estimates of optimal learning rates (see above).\nDetails of summaries of actual paths taken through the search space towards convergence on local or global minimum.\n\n\n\nModel Output and Performance Measure the performance of the trained model when applied to the test data set, generally requiring the specification of a metric of model performance or accuracy.\n\nImportantly, ML workflows may be partly iterative. This may in turn potentially confound distinctions between training and test data, and accordingly confound expectations commonly placed upon statistical analyses of statistical independence of response variables. ML routines such as cross-validation repeatedly (re-)partition data between training and test sets. Resultant models can then not be considered to have been developed through application to any single set of truly “independent” data. In the context of the standards that follow, these considerations admit a potential lack of clarity in any notional categorical distinction between training and test data, and between model specification and training.\nThe preceding workflow mentioned a couple of concepts the interpretations of which in the context of these standards may be seen by clicking on the corresponding items below. Following that, we proceed to standards for ML software, enumerated and developed with reference to the preceding workflow steps. In order that the following standards initially adhere to the enumeration of workflow steps given above, more general standards pertaining to aspects such as documentation and testing are given following the initial five “workflow” standards.\n\nClick for a definition of broadcasting, referred to in Step 2, above.\n\n\nThe following definition comes from a vignette for the rray package named Broadcasting.\n\n\nBroadcasting is, “repeating the dimensions of one object to match the dimensions of another.”\n\nThis concept runs counter to aspects of standards in other categories, which often suggest that functions should error when passed input objects which do not have commensurate dimensions. Broadcasting is a pre-processing step which enables objects with incommensurate dimensions to be dimensionally reconciled.\nThe following demonstration is taken directly from the rray package (which is not currently on CRAN).\nBroadcasting is commonly employed in ML software because it enables ML operations to be implemented on objects with incommensurate dimensions. One example is image analysis, in which training data may all be dimensionally commensurate, yet test images may have different dimensions. Broadcasting allows data to be submitted to ML routines regardless of potentially incommensurate dimensions.\n\n\nClick for a definition of learning rate, referred to in Step 5, above.\n\n\n\n\nLearning Rate (generally) determines the step size used to search for local optima as a fraction of the local gradient.\n\nThis parameter is particularly important for training ML algorithms like neural networks, the results of which can be very sensitive to variations in learning rates. A useful overview of the importance of learning rates, and a useful approach to automatically determining appropriate values, is given in this blog post.\n\n\nPartly because of widespread and current relevance, the category of Machine Learning software is one for which there have been other notable attempts to develop standards. A particularly useful reference is the MLPerf organization which, among other activities, hosts several github repositories providing reference datasets and benchmark conditions for comparing performance aspects of ML software. While such reference or benchmark standards are not explicitly referred to in the current version of the following standards, we expect them to be gradually adapted and incorporated as we start to apply and refine our standards in application to software submitted to our review system.\n\n6.4.1 Input Data Specification\nMany of the following standards refer to the labelling of input data as “testing” or “training” data, along with potentially additional labels such as “validation” data. In regard to such labelling, the following two standards apply,\n\n\nML1.0 Documentation should make a clear conceptual distinction between training and test data (even where such may ultimately be confounded as described above.)\n\n\nML1.0a Where these terms are ultimately eschewed, these should nevertheless be used in initial documentation, along with clear explanation of, and justification for, alternative terminology.\n\n\n\n\nML1.1 Absent clear justification for alternative design decisions, input data should be expected to be labelled “test”, “training”, and, where applicable, “validation” data.\n\n\nML1.1a The presence and use of these labels should be explicitly confirmed via pre-processing steps (and tested in accordance with ML7.0, below).\n\n\nML1.1b Matches to expected labels should be case-insensitive and based on partial matching such that, for example, “Test”, “test”, or “testing” should all suffice.\n\n\n\n\nThe following three standards (ML1.2–ML1.4) represent three possible design intentions for ML software. Only one of these three will generally be applicable to any one piece of software, although it is nevertheless possible that more than one of these standards may apply. The first of these three standards applies to ML software which is intended to process, or capable of processing, input data as a single (generally tabular) object.\n\n\nML1.2 Training and test data sets for ML software should be able to be input as a single, generally tabular, data object, with the training and test data distinguished either by\n\nA specified variable containing, for example, TRUE/FALSE or 0/1 values, or which uses some other system such as missing (NA) values to denote test data); and/or\nAn additional parameter designating case or row numbers, or labels of test data.\n\n\n\nThe second of these three standards applies to ML software which is intended to process, or capable of processing, input data represented as multiple objects which exist in local memory.\n\n\nML1.3 Input data should be clearly partitioned between training and test data (for example, through having each passed as a distinct list item), or should enable an additional means of categorically distinguishing training from test data (such as via an additional parameter which provides explicit labels). Where applicable, distinction of validation and any other data should also accord with this standard.\n\n\nThe third of these three standards for data input applies to ML software for which data are expected to be input as references to multiple external objects, generally expected to be read from either local or remote connections.\n\n\nML1.4 Training and test data sets, along with other necessary components such as validation data sets, should be stored in their own distinctly labelled sub-directories (for distinct files), or according to an explicit and distinct labelling scheme (for example, for database connections). Labelling should in all cases adhere to ML1.1, above.\n\n\nThe following standard applies to all ML software regardless of the applicability or otherwise of the preceding three standards.\n\n\nML1.5 ML software should implement a single function which summarises the contents of test and training (and other) data sets, minimally including counts of numbers of cases, records, or files, and potentially extending to tables or summaries of file or data types, sizes, and other information (such as unique hashes for each component).\n\n\n\n6.4.1.1 Missing Values\nMissing data are handled differently by different ML routines, and it is also difficult to suggest generally applicable standards for pre-processing missing values in ML software. The General Standards for missing values (G2.13–G2.16) do not apply to Machine Learning software, in the place of which the following standards attempt to cover a practical range of typical approaches and applications.\n\n\nML1.6 ML software which does not admit missing values, and which expects no missing values, should implement explicit pre-processing routines to identify whether data has any missing values, and should generally error appropriately and informatively when passed data with missing values. In addition, ML software which does not admit missing values should:\n\n\nML1.6a Explain why missing values are not admitted.\n\n\nML1.6b Provide explicit examples (in function documentation, vignettes, or both) for how missing values may be imputed, rather than simply discarded.\n\n\n\n\nML1.7 ML software which admits missing values should clearly document how such values are processed.\n\n\nML1.7a Where missing values are imputed, software should offer multiple user-defined ways to impute missing data.\n\n\nML1.7b Where missing values are imputed, the precise imputation steps should also be explicitly documented, either in tests (see ML7.2 below), function documentation, or vignettes.\n\n\n\n\nML1.8 ML software should enable equal treatment of missing values for both training and test data, with optional user ability to control application to either one or both.\n\n\n6.4.2 Pre-processing\nAs reflected in the workflow envisioned at the outset, ML software operates somewhat differently to statistical software in many other categories. In particular, ML software often requires explicit specification of a workflow, including specification of input data (as per the standards of the preceding sub-section), and of both transformations and statistical models to be applied to those data. This section of standards refers exclusively to the transformation of input data as a pre-processing step prior to any specification of, or submission to, actual models.\n\n\nML2.0 A dedicated function should enable pre-processing steps to be defined and parametrized.\n\n\nML2.0a That function should return an object which can be directly submitted to a specified model (see section 3, below).\n\n\nML2.0b Absent explicit justification otherwise, that return object should have a defined class minimally intended to implement a default print method which summarizes the input data set (as per ML1.5 above) and associated transformations (see the following standard).\n\n\n\n\nStandards for most other categories of statistical software suggest that pre-processing routines should ensure that input data sets are commensurate, for example, through having equal numbers of cases or rows. In contrast, ML software is commonly intended to accept input data which can not be guaranteed to be dimensionally commensurate, such as software intended to process rectangular image files which may be of different sizes.\n\n\nML2.1 ML software which uses broadcasting to reconcile dimensionally incommensurate input data should offer an ability to at least optionally record transformations applied to each input file.\n\n\nBeyond broadcasting and dimensional transformations, the following standards apply to the pre-processing stages of ML software.\n\n\nML2.2 ML software which requires or relies upon numeric transformations of input data (such as change in mean values or variances) should allow optimal explicit specification of target values, rather than restricting transformations to default generic values only (such as transformations to z-scores).\n\n\nML2.2a Where the parameters have default values, reasons for those particular defaults should be explicitly described.\n\n\nML2.2b Any extended documentation (such as vignettes) which demonstrates the use of explicit values for numeric transformations should explicitly describe why particular values are used.\n\n\n\n\nFor all transformations applied to input data, whether of dimension (ML2.1) or scale (ML2.2),\n\n\nML2.3 The values associated with all transformations should be recorded in the object returned by the function described in the preceding standard (ML2.0).\n\n\nML2.4 Default values of all transformations should be explicitly documented, both in documentation of parameters where appropriate (such as for numeric transformations), and in extended documentation such as vignettes.\n\n\nML2.5 ML software should provide options to bypass or otherwise switch off all default transformations.\n\n\nML2.6 Where transformations are implemented via distinct functions, these should be exported to a package’s namespace so they can be applied in other contexts.\n\n\nML2.7 Where possible, documentation should be provided for how transformations may be reversed. For example, documentation may demonstrate how the values retained via ML2.3, above, can be used along with transformations either exported via ML2.6 or otherwise exemplified in demonstration code to independently transform data, and then to reverse those transformations.\n\n\n6.4.3 Model and Algorithm Specification\nA “model” in the context of ML software is understood to be a means of specifying a mapping between input and output data, generally applied to training and validation data. Model specification is the step of specifying how such a mapping is to be constructed. The specification of what the values of such a model actually are occurs through training the model, and is described in the following sub-section. These standards also refer to control parameters which specify how models are trained. These parameters commonly include values specifying numbers of iterations, training rates, and parameters controlling algorithmic processes such as re-sampling or cross-validation.\n\n\nML3.0 Model specification should be implemented as a distinct stage subsequent to specification of pre-processing routines (see Section 2, above) and prior to actual model fitting or training (see Section 4, below). In particular,\n\n\nML3.0a A dedicated function should enable models to be specified without actually fitting or training them, or if this (ML3) and the following (ML4) stages are controlled by a single function, that function should have a parameter enabling models to be specified yet not fitted (for example, nofit = FALSE).\n\n\nML3.0b That function should accept as input the objects produced by the previous Input Data Specification stage, and defined according to ML2.0, above.\n\n\nML3.0c The function described above (ML3.0a) should return an object which can be directly trained as described in the following sub-section (ML4).\n\n\nML3.0d That return object should have a defined class minimally intended to implement a default print method which summarises the model specification, including values of all relevant parameters.\n\n\n\n\nML3.1 ML software should allow the use of both untrained models, specified through model parameters only, as well as pre-trained models. Use of the latter commonly entails an ability to submit a previously-trained model object to the function defined according to ML3.0a, above.\n\n\nML3.2 ML software should enable different models to be applied to the object specifying data inputs and transformations (see sub-sections 1–2, above) without needing to re-define those preceding steps.\n\n\nA function fulfilling ML3.0–3.2 might, for example, permit the following arguments:\n\n\ndata: Input data specification constructed according to ML1\n\n\nmodel: An optional previously-trained model\n\ncontrol: A list of parameters controlling how the model algorithm is to be applied during the subsequent training phase (ML4).\n\nA function with the arguments defined above would fulfil the preceding three standards, because the data stage would represent the output of ML1, while the model stage would allow for different pre-trained models to be submitted using the same data and associated specifications (ML3.1). The provision of a separate .data argument would fulfil ML3.2 by allowing one or both model or control parameters to be re-defined while submitting the same data object.\n\n\nML3.3 Where ML software implements its own distinct classes of model objects, the properties and behaviours of those specific classes of objects should be explicitly compared with objects produced by other ML software. In particular, where possible, ML software should provide extended documentation (as vignettes or equivalent) comparing model objects with those from other ML software, noting both unique abilities and restrictions of any implemented classes.\n\n\nML3.4 Where training rates are used, ML software should provide explicit documentation both in all functions which use training rates, and in extended form such as vignettes, of the importance of, and/or sensitivity to, different values of training rates. In particular,\n\n\nML3.4a Unless explicitly justified otherwise, ML software should offer abilities to automatically determine appropriate or optimal training rates, either as distinct pre-processing stages, or as implicit stages of model training.\n\n\nML3.4b ML software which provides default values for training rates should clearly document anticipated restrictions of validity of those default values; for example through clear suggestions that user-determined and -specified values may generally be necessary or preferable.\n\n\n\n\n\n6.4.3.1 Control Parameters\nControl parameters are considered here to specify how a model is to be applied to a set of training data. These are generally distinct from parameters specifying the actual model (such as model architecture). While we recommend that control parameters be submitted as items of a single named list, this is neither a firm expectation nor an explicit part of the current standards.\n\n\nML3.5 Parameters controlling optimization algorithms should minimally include:\n\n\nML3.5a Specification of the type of algorithm used to explore the search space (commonly, for example, some kind of gradient descent algorithm)\n\n\nML3.5b The kind of loss function used to assess distance between model estimates and desired output.\n\n\n\n\nML3.6 Unless explicitly justified otherwise (for example because ML software under consideration is an implementation of one specific algorithm), ML software should:\n\n\nML3.6a Implement or otherwise permit usage of multiple ways of exploring search space\n\n\nML3.6b Implement or otherwise permit usage of multiple loss functions.\n\n\n\n\n6.4.3.2 CPU and GPU processing\nML software often involves manipulation of large numbers of rectangular arrays for which graphics processing units (GPUs) are often more efficient than central processing units (CPUs). ML software thus commonly offers options to train models using either CPUs or GPUs. While these standards do not currently suggest any particular design choice in this regard, we do note the following:\n\n\nML3.7 For ML software in which algorithms are coded in C++, user-controlled use of either CPUs or GPUs (on NVIDIA processors at least) should be implemented through direct use of libcudacxx.\n\n\nThis library can be “switched on” through activating a single C++ header file to switch from CPU to GPU.\n\n6.4.4 Model Training\nModel training is the stage of the ML workflow envisioned here in which the actual computation is performed by applying a model specified according to ML3 to data specified according to ML1 and ML2.\n\n\nML4.0 ML software should generally implement a unified single-function interface to model training, able to receive as input a model specified according to all preceding standards. In particular, models with categorically different specifications, such as different model architectures or optimization algorithms, should be able to be submitted to the same model training function.\n\n\nML4.1 ML software should at least optionally retain explicit information on paths taken as an optimizer advances towards minimal loss. Such information should minimally include:\n\n\nML4.1a Specification of all model-internal parameters, or equivalent hashed representation.\n\n\nML4.1b The value of the loss function at each point\n\n\nML4.1c Information used to advance to next point, for example quantification of local gradient.\n\n\n\n\nML4.2 The subsequent extraction of information retained according to the preceding standard should be explicitly documented, including through example code.\n\n\n\n6.4.4.1 Batch Processing\nThe following standards apply to ML software which implements batch processing, commonly to train models on data sets too large to be loaded in their entirety into memory.\n\n\nML4.3 All parameters controlling batch processing and associated terminology should be explicitly documented, and it should not, for example, be presumed that users will understand the definition of “epoch” as implemented in any particular ML software.\n\n\nAccording to that standard, it would for example be inappropriate to have a parameter, nepochs, described as “Number of epochs used in model training”. Rather, the definition and particular implementation of “epoch” must be explicitly defined.\n\n\nML4.4 Explicit guidance should be provided on selection of appropriate values for parameter controlling batch processing, for example, on trade-offs between batch sizes and numbers of epochs (with both terms provided as Control Parameters in accordance with the preceding standard, ML3).\n\n\nML4.5 ML software may optionally include a function to estimate likely time to train a specified model, through estimating initial timings from a small sample of the full batch.\n\n\nML4.6 ML software should by default provide explicit information on the progress of batch jobs (even where those jobs may be implemented in parallel on GPUs). That information may be optionally suppressed through additional parameters.\n\n\n6.4.4.2 Re-sampling\nAs described at the outset, ML software does not always rely on pre-specified and categorical distinctions between training and test data. For example, models may be fit to what is effectively one single data set in which specified cases or rows are used as training data, and the remainder as test data. Re-sampling generally refers to the practice of re-defining categorical distinctions between training and test data. One training run accordingly connotes training a model on one particular set of training data and then applying that model to the specified set of test data. Re-sampling starts that process anew, through constructing an alternative categorical partition between test and training data.\nEven where test and training data are distinguished by more than a simple data-internal category (such as a labelling column), for example, by being stored in distinctly-named sub-directories, re-sampling may be implemented by effectively shuffling data between training and test sub-directories.\n\n\nML4.7 ML software should provide an ability to combine results from multiple re-sampling iterations using a single parameter specifying numbers of iterations.\n\n\nML4.8 Absent any additional specification, re-sampling algorithms should by default partition data according to proportions of original test and training data.\n\n\nML4.8a Re-sampling routines of ML software should nevertheless offer an ability to explicitly control or override such default proportions of test and training data.\n\n\n\n\n6.4.5 Model Output and Performance\nModel output is considered here as a stage distinct from model performance. Model output refers to the end result of model training (ML4), while model performance involves the assessment of a trained model against a test data set. The present section first describes standards for model output, which are standards guiding the form of a model trained according to the preceding standards (ML4). Model Performance is then considered as a separate stage.\n\n6.4.5.1 Model Output\n\n\nML5.0 The result of applying the training processes described above should be contained within a single model object returned by the function defined according to ML4.0, above. Even where the output reflects application to a test data set, the resultant object need not include any information on model performance (see ML5.3–ML5.4, below).\n\n\nML5.0a That object should either have its own class, or extend some previously-defined class.\n\n\nML5.0b That class should have a defined print method which summarises important aspects of the model object, including but not limited to summaries of input data and algorithmic control parameters.\n\n\n\n\nML5.1 As for the untrained model objects produced according to the above standards, and in particular as a direct extension of ML3.3, the properties and behaviours of trained models produced by ML software should be explicitly compared with equivalent objects produced by other ML software. (Such comparison will generally be done in terms of comparing model performance, as described in the following standard ML5.3–ML5.4).\n\n\nML5.2 The structure and functionality of objects representing trained ML models should be thoroughly documented. In particular,\n\n\nML5.2a Either all functionality extending from the class of model object should be explicitly documented, or a method for listing or otherwise accessing all associated functionality explicitly documented and demonstrated in example code.\n\n\nML5.2b Documentation should include examples of how to save and re-load trained model objects for their re-use in accordance with ML3.1, above.\n\n\nML5.2c Where general functions for saving or serializing objects, such as saveRDS are not appropriate for storing local copies of trained models, an explicit function should be provided for that purpose, and should be demonstrated with example code.\n\n\n\n\nThe R6 system for representing classes in R is an example of a system with explicit functionality, all components of which are accessible by a simple ls() call. Adherence to ML5.2a would nevertheless require explicit description of the ability of ls() to supply a list of all functions associated with an object. The mlr package, for example, uses R6 classes, yet neither explicitly describes the use of ls() to list all associated functions, nor explicitly lists those functions.\n\n6.4.5.2 Model Performance\nModel performance refers to the quantitative assessment of a trained model when applied to a set of test data.\n\n\nML5.3 Assessment of model performance should be implemented as one or more functions distinct from model training.\n\n\nML5.4 Model performance should be able to be assessed according to a variety of metrics.\n\n\nML5.4a All model performance metrics represented by functions internal to a package must be clearly and distinctly documented.\n\n\nML5.4b It should be possible to submit custom metrics to a model assessment function, and the ability to do so should be clearly documented including through example code.\n\n\n\n\nThe remaining sub-sections specify general standards beyond the preceding workflow-specific ones.\n\n6.4.6 Documentation\n\n\nML6.0 Descriptions of ML software should make explicit reference to a workflow which separates training and testing stages, and which clearly indicates a need for distinct training and test data sets.\n\n\nThe following standard applies to packages which are intended or other able to only encompass a restricted subset of the six primary workflow steps enumerated at the outset. Envisioned here are packages explicitly intended to aid one particular aspect of the general workflow envisioned here, such as implementations of ML optimization functions, or specific loss measures.\n\n\nML6.1 ML software intentionally designed to address only a restricted subset of the workflow described here should clearly document how it can be embedded within a typical full ML workflow in the sense considered here.\n\n\nML6.1a Such demonstrations should include and contrast embedding within a full workflow using at least two other packages to implement that workflow.\n\n\n\n\n6.4.7 Testing\n\n6.4.7.1 Input Data\n\n\nML7.0 Test should explicitly confirm partial and case-insensitive matching of “test”, “train”, and, where applicable, “validation” data.\n\n\nML7.1 Tests should demonstrate effects of different numeric scaling of input data (see ML2.2).\n\n\nML7.2 For software which imputes missing data, tests should compare internal imputation with explicit code which directly implements imputation steps (even where such imputation is a single-step implemented via some external package). These tests serve as an explicit reference for how imputation is performed.\n\n\n6.4.7.2 Model Classes\nThe following standard applies to models in both untrained and trained forms, considered to be the respective outputs of the preceding standards ML3 and ML4.\n\n\nML7.3 Where model objects are implemented as distinct classes, tests should explicitly compare the functionality of these classes with functionality of equivalent classes for ML model objects from other packages.\n\n\nML7.3a These tests should explicitly identify restrictions on the functionality of model objects in comparison with those of other packages.\n\n\nML7.3b These tests should explicitly identify functional advantages and unique abilities of the model objects in comparison with those of other packages.\n\n\n\n\n6.4.7.3 Model Training\n\n\nML7.4 ML software should explicit document the effects of different training rates, and in particular should demonstrate divergence from optima with inappropriate training rates.\n\n\nML7.5 ML software which implements routines to determine optimal training rates (see ML3.4, above) should implement tests to confirm the optimality of resultant values.\n\n\nML7.6 ML software which implement independent training “epochs” should demonstrate in tests the effects of lesser versus greater numbers of epochs.\n\n\nML7.7 ML software should explicitly test different optimization algorithms, even where software is intended to implement one specific algorithm.\n\n\nML7.8 ML software should explicitly test different loss functions, even where software is intended to implement one specific measure of loss.\n\n\nML7.9 Tests should explicitly compare all possible combinations in categorical differences in model architecture, such as different model architectures with same optimization algorithms, same model architectures with different optimization algorithms, and differences in both.\n\n\nML7.9a Such combinations will generally be formed from multiple categorical factors, for which explicit use of functions such as expand.grid() is recommended.\n\n\n\n\nThe following example illustrates:\n\n\n    Var1 Var2  Var3\n1  archA optA costA\n2  archB optA costA\n3  archA optB costA\n4  archB optB costA\n5  archA optC costA\n6  archB optC costA\n7  archA optA costB\n8  archB optA costB\n9  archA optB costB\n10 archB optB costB\n11 archA optC costB\n12 archB optC costB\n13 archA optA costC\n14 archB optA costC\n15 archA optB costC\n16 archB optB costC\n17 archA optC costC\n18 archB optC costC\n\n\nAll possible combinations of these categorical parameters could then be tested by iterating over the rows of that output.\n\n\nML7.10 The successful extraction of information on paths taken by optimizers (see ML5.1, above), should be tested, including testing the general properties, but not necessarily actual values of, such data.\n\n\n6.4.7.4 Model Performance\n\n\nML7.11 All performance metrics available for a given class of trained model should be thoroughly tested and compared.\n\n\nML7.11a Tests which compare metrics should do so over a range of inputs (generally implying differently trained models) to demonstrate relative advantages and disadvantages of different metrics.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-regression",
    "href": "standards.html#standards-regression",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.5 Regression and Supervised Learning",
    "text": "6.5 Regression and Supervised Learning\nThis sub-section details standards for Regression and Supervised Learning Software – referred to from here on for simplicity as “Regression Software”. Regression Software implements algorithms which aim to construct or analyse one or more mappings between two defined data sets (for example, a set of “independent” data, \\(X\\), and a set of “dependent” data, \\(Y\\)). In contrast, the analogous category of Unsupervised Learning Software aims to construct or analyse one or more mappings between a defined set of input or independent data, and a second set of “output” data which are not necessarily known or given prior to the analysis.\nCommon purposes of Regression Software are to fit models to estimate relationships or to make predictions between specified inputs and outputs. Regression Software includes tools with inferential or predictive foci, Bayesian, frequentist, or probability-free Machine Learning (ML) approaches, parametric or or non-parametric approaches, discrete outputs (such as in classification tasks) or continuous outputs, and models and algorithms specific to applications or data such as time series or spatial data. In many cases other standards specific to these subcategories may apply.\nExamples of the diversity of Regression and Unsupervised Learning software include the following.\n\n\nxrnet to perform “hierarchical regularized regression to incorporate external data”, where “external data” in this case refers to structured meta-data as applied to genomic features.\n\nsurvPen is, “an R package for hazard and excess hazard modelling with multidimensional penalized splines”\n\nareal is, “an R package for areal weighted interpolation”.\n\nChiRP is a package for “Chinese Restaurant Process mixtures for regression and clustering”, which implements a class of non-parametric Bayesian Monte Carlo models.\n\nklrfome is a package for, “kernel logistic regression on focal mean embeddings,” with a specific and exclusive application to the prediction of likely archaeological sites.\n\ngravity is a package for “estimation methods for gravity models in R,” where “gravity models” refers to models of spatial interactions between point locations based on the properties of those locations.\n\ncompboost is an example of an R package for gradient boosting, which is inherently a regression-based technique, and so standards for regression software ought to consider such applications.\n\nungroup is, “an R package for efficient estimation of smooth distributions from coarsely binned data.” As such, this package is an example of regression-based software for which the input data are (effectively) categorical. The package is primarily intended to implement a particular method for “unbinning” the data, and so represents a particular class of interpolation methods.\n\nregistr is a package for “registration for exponential family functional data,” where registration in this context is effectively an interpolation method applied within a functional data analysis context.\n\nggeffects for “tidy data frames of marginal effects from regression models.” This package aims to make statistics quantifying marginal effects readily understandable, and so implements a standard (tidyverse-based) methodology for representing and visualising statistics relating to marginal effects.\n\nClick on the following link to view a demonstration Application of Regression and Supervised Learning Standards.\nThe following standards are divided among several sub-categories, with each standard prefixed with “RE”.\n\n6.5.1 Input data structures and validation\n\n\nRE1.0 Regression Software should enable models to be specified via a formula interface, unless reasons for not doing so are explicitly documented.\n\n\nRE1.1 Regression Software should document how formula interfaces are converted to matrix representations of input data.\n\n\nSee Max Kuhn’s RStudio blog post for examples of how to implement and describe such conversions.\n\n\nRE1.2 Regression Software should document expected format (types or classes) for inputting predictor variables, including descriptions of types or classes which are not accepted.\n\n\nExamples documentation addressing this standard include clarifying that software accepts only numeric inputs in vector or matrix form, or that all inputs must be in data.frame form with both column and row names.\n\n\nRE1.3 Regression Software which passes or otherwise transforms aspects of input data onto output structures should ensure that those output structures retain all relevant aspects of input data, notably including row and column names, and potentially information from other attributes().\n\n\nRE1.3a Where otherwise relevant information is not transferred, this should be explicitly documented.\n\n\n\n\nThis standard reflects the common process in regression software of transforming a rectangular input structure into a modified version which includes additional columns of model fits or predictions. Software which constructs such modified versions anew often copies numeric values from input columns, and may implicitly drop additional information such as attributes. This standard requires all such information to be retained.\n\n\nRE1.4 Regression Software should document any assumptions made with regard to input data; for example distributional assumptions, or assumptions that predictor data have mean values of zero. Implications of violations of these assumptions should be both documented and tested.\n\n\n6.5.2 Pre-processing and Variable Transformation\n\n\nRE2.0 Regression Software should document any transformations applied to input data, for example conversion of label-values to factor, and should provide ways to explicitly avoid any default transformations (with error or warning conditions where appropriate).\n\n\nRE2.1 Regression Software should implement explicit parameters controlling the processing of missing values, ideally distinguishing NA or NaN values from Inf values (for example, through use of na.omit() and related functions from the stats package).\n\n\nNote that fulfilling this standard ensures compliance with all General Standard for missing values (G2.13–G2.16).\n\n\nRE2.2 Regression Software should provide different options for processing missing values in predictor and response data. For example, it should be possible to fit a model with no missing predictor data in order to generate values for all associated response points, even where submitted response values may be missing.\n\n\nRE2.3 Where applicable, Regression Software should enable data to be centred (for example, through converting to zero-mean equivalent values; or to z-scores) or offset (for example, to zero-intercept equivalent values) via additional parameters, with the effects of any such parameters clearly documented and tested.\n\n\nRE2.4 Regression Software should implement pre-processing routines to identify whether aspects of input data are perfectly collinear, notably including:\n\n\nRE2.4a Perfect collinearity among predictor variables\n\n\nRE2.4b Perfect collinearity between independent and dependent variables\n\n\n\n\nThese pre-processing routines should also be tested as described below.\n\n6.5.3 Algorithms\nThe following standards apply to the model fitting algorithms of Regression Software which implement or rely on iterative algorithms which are expected to converge to generate model statistics. Regression Software which implements or relies on iterative convergence algorithms should:\n\n\nRE3.0 Issue appropriate warnings or other diagnostic messages for models which fail to converge.\n\n\nRE3.1 Enable such messages to be optionally suppressed, yet should ensure that the resultant model object nevertheless includes sufficient data to identify lack of convergence.\n\n\nRE3.2 Ensure that convergence thresholds have sensible default values, demonstrated through explicit documentation.\n\n\nRE3.3 Allow explicit setting of convergence thresholds, unless reasons against doing so are explicitly documented.\n\n\n6.5.4 Return Results\n\n\nRE4.0 Regression Software should return some form of “model” object, generally through using or modifying existing class structures for model objects (such as lm, glm, or model objects from other packages), or creating a new class of model objects.\n\n\nRE4.1 Regression Software may enable an ability to generate a model object without actually fitting values. This may be useful for controlling batch processing of computationally intensive fitting algorithms.\n\n\n\n6.5.4.1 Accessor Methods\nRegression Software should provide functions to access or extract as much of the following kinds of model data as possible or practicable. Access should ideally rely on class-specific methods which extend, or implement otherwise equivalent versions of, the methods from the stats package which are named in parentheses in each of the following standards.\nModel objects should include, or otherwise enable effectively immediate access to the following descriptors. It is acknowledged that not all regression models can sensibly provide access to these descriptors, yet should include access provisions to all those that are applicable.\n\n\nRE4.2 Model coefficients (via coef() / coefficients())\n\n\nRE4.3 Confidence intervals on those coefficients (via confint())\n\n\nRE4.4 The specification of the model, generally as a formula (via formula())\n\n\nRE4.5 Numbers of observations submitted to model (via nobs())\n\n\nRE4.6 The variance-covariance matrix of the model parameters (via vcov())\n\n\nRE4.7 Where appropriate, convergence statistics\n\n\nNote that compliance with RE4.6 should also heed General Standard G3.1 in offering user control over covariance algorithms. Regression Software should further provide simple and direct methods to return or otherwise access the following form of data and metadata, where the latter includes information on any transformations which may have been applied to the data prior to submission to modelling routines.\n\n\nRE4.8 Response variables, and associated “metadata” where applicable.\n\n\nRE4.9 Modelled values of response variables.\n\n\nRE4.10 Model Residuals, including sufficient documentation to enable interpretation of residuals, and to enable users to submit residuals to their own tests.\n\n\nRE4.11 Goodness-of-fit and other statistics associated such as effect sizes with model coefficients.\n\n\nRE4.12 Where appropriate, functions used to transform input data, and associated inverse transform functions.\n\n\nRegression software may additionally opt to provide simple and direct methods to return or otherwise access the following:\n\n\nRE4.13 Predictor variables, and associated “metadata” where applicable.\n\n\n6.5.4.2 Prediction, Extrapolation, and Forecasting\nNot all regression software is intended to, or can, provide distinct abilities to extrapolate or forecast. Moreover, identifying cases in which a regression model is used to extrapolate or forecast may often be a non-trivial exercise. It may nevertheless be possible, for example when input data used to construct a model are unidimensional, and data on which a prediction is to be based extend beyond the range used to construct the model. Where reasonably unambiguous identification of extrapolation or forecasting using a model is possible, the following standards apply:\n\n\nRE4.14 Where possible, values should also be provided for extrapolation or forecast errors.\n\n\nRE4.15 Sufficient documentation and/or testing should be provided to demonstrate that forecast errors, confidence intervals, or equivalent values increase with forecast horizons.\n\n\nDistinct from extrapolation or forecasting abilities, the following standard applies to regression software which relies on, or otherwise provides abilities to process, categorical grouping variables:\n\n\nRE4.16 Regression Software which models distinct responses for different categorical groups should include the ability to submit new groups to predict() methods.\n\n\n6.5.4.3 Reporting Return Results\n\n\nRE4.17 Model objects returned by Regression Software should implement or appropriately extend a default print method which provides an on-screen summary of model (input) parameters and (output) coefficients.\n\n\nRE4.18 Regression Software may also implement summary methods for model objects, and in particular should implement distinct summary methods for any cases in which calculation of summary statistics is computationally non-trivial (for example, for bootstrapped estimates of confidence intervals).\n\n\n6.5.5 Documentation\nBeyond the General Standards for documentation, Regression Software should explicitly describe the following aspects, and ideally provide extended documentation including summary graphical reports of:\n\n\nRE5.0 Scaling relationships between sizes of input data (numbers of observations, with potential extension to numbers of variables/columns) and speed of algorithm.\n\n\n6.5.6 Visualization\n\n\nRE6.0 Model objects returned by Regression Software (see RE4) should have default plot methods, either through explicit implementation, extension of methods for existing model objects, or through ensuring default methods work appropriately.\n\n\nRE6.1 Where the default plot method is NOT a generic plot method dispatched on the class of return objects (that is, through an S3-type plot.&lt;myclass&gt; function or equivalent), that method dispatch (or equivalent) should nevertheless exist in order to explicitly direct users to the appropriate function.\n\n\nRE6.2 The default plot method should produce a plot of the fitted values of the model, with optional visualisation of confidence intervals or equivalent.\n\n\nThe following standard applies only to software fulfilling RE4.14-4.15, and the conditions described prior to those standards.\n\n\nRE6.3 Where a model object is used to generate a forecast (for example, through a predict() method), the default plot method should provide clear visual distinction between modelled (interpolated) and forecast (extrapolated) values.\n\n\n6.5.7 Testing\n\n6.5.7.1 Input Data\nTests for Regression Software should include the following conditions and cases:\n\n\nRE7.0 Tests with noiseless, exact relationships between predictor (independent) data.\n\n\nRE7.0a In particular, these tests should confirm ability to reject perfectly noiseless input data.\n\n\n\nRE7.1 Tests with noiseless, exact relationships between predictor (independent) and response (dependent) data.\n\n\nRE7.1a In particular, these tests should confirm that model fitting is at least as fast or (preferably) faster than testing with equivalent noisy data (see RE2.4b).\n\n\n\n\n6.5.7.2 Return Results\nTests for Regression Software should\n\n\nRE7.2 Demonstrate that output objects retain aspects of input data such as row or case names (see RE1.3).\n\nRE7.3 Demonstrate and test expected behaviour when objects returned from regression software are submitted to the accessor methods of RE4.2–RE4.7.\n\nRE7.4 Extending directly from RE4.15, where appropriate, tests should demonstrate and confirm that forecast errors, confidence intervals, or equivalent values increase with forecast horizons.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-spatial",
    "href": "standards.html#standards-spatial",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.6 Spatial Software",
    "text": "6.6 Spatial Software\nStandards for spatial software begin with a consideration and standardisation of domains of applicability. Following that we proceed to standards according to which spatial software is presumed to perform one or more of the following steps:\n\nAccept and validate input data\nApply one or more analytic algorithms\nReturn the result of that algorithmic application\nOffer additional functionality such as printing or summarising return results\nTesting\n\nEach standard for spatial software is prefixed with “SP”.\n\n6.6.1 Spatial Domains\nMany developers of spatial software in R, including many of those those featured on the CRAN Task view on “Analysis of Spatial Data”, have been primarily focussed on geographic data; that is, data quantifying positions, structures, and relationships on the Earth and other planets. Spatial analyses are nevertheless both broader and more general than geography alone. In particular, spatial software may be geometric – that is, concerned with positions, structures, and relationships in space in any general or specific sense, not necessarily confined to geographic systems alone.\nIt is important to distinguish these two domains because many algorithms and procedures devised in one of these two domains are not necessarily (directly) applicable in the other, most commonly because geometric algorithms presume space to be rectilinear or Cartesian, while geographic algorithms (generally) presume it be have a specific curvilinear form (commonly spherical or elliptical). Algorithms designed for Cartesian space may not be directly applicable in curvilinear space, and vice-versa.\nMoreover, spatial software and algorithms might be intended to apply in spaces of arbitrary dimensionality. The phrase “Cartesian” refers to any space of arbitrary dimensionality in which all dimensions are orthogonal and described by straight lines; dimensions in a curvilinear space or arbitrary dimensionality are described by curved lines. A planar geometry is a two-dimensional Cartesian space; a spherical geometry is a two- (or maybe three-)dimensional curvilinear space.\nOne of the earliest and still most widely used R spatial packages, spatstat (first released 2002), describes itself as, “[f]ocused mainly on two-dimensional point patterns, including multitype/marked points, in any spatial region.” Routines from this package are thus generally applicable to two-dimensional Cartesian data only, even through the final phrase might be interpreted to indicate a comprehensive generality. spatstat routines may not necessarily give accurate results when applied in curvilinear space.\nThese considerations motivate the first standard for spatial software:\n\n\nSP1.0 Spatial software should explicitly indicate its domain of applicability, and in particular distinguish whether the software may be applied in Cartesian/rectilinear/geometric domains, curvilinear/geographic domains, or both.\n\n\nWe encourage the use of clear and unambiguous phrases such as “planar”, “spherical”, “Cartesian”, “rectilinear” or “curvilinear”, along with clear indications of dimensionality such as “two-” or “three-dimensional.” Concepts of dimensionality should be interpreted to refer explicitly to the dimensionality of independent spatial coordinates. Elevation is a third spatial dimension, and time may also be considered an additional dimension. Beyond those two, other attributes measured at spatial locations do not represent additional dimensions.\n\n\nSP1.1 Spatial software should explicitly indicate its dimensional domain of applicability, in particular through identifying whether it is applicable to two or three dimensions only, or whether there are any other restrictions on dimensionality.\n\n\nThese considerations of domains of applicability permeate much of the ensuring standards, which distinguish “geometric software” from “geographic software”, where these phrases are to be interpreted as shorthand references to software intended for use in the respective domains.\n\n6.6.2 Input data structures and validation\nInput validation is an important software task, and an important part of our standards. While there are many ways to approach validation, the class systems of R offer a particularly convenient and effective means. For Spatial Software in particular, a range of class systems have been developed, for which we refer to the CRAN Task view on “Analysis of Spatial Data”. Software which uses and relies on defined classes can often validate input through affirming appropriate class(es). Software which does not use or rely on class systems will generally need specific routines to validate input data structures.\nAs for our standards for Time-Series Software, these standards for Spatial Software also suggest that software should use explicit class systems designed and intended for spatial data. New packages may implement new class systems for spatial data, and these may even be as simple as appending a class attribute to a matrix of coordinates. The primary motivation of the following standard is nevertheless to encourage and enhance inter-operability with the rich system of classes for spatial data in R.\n\n\nSP2.0 Spatial software should only accept input data of one or more classes explicitly developed to represent such data.\n\n\nSP2.0a Where new classes are implemented, conversion to other common classes for spatial data in R should be documented.\n\n\nSP2.0b Class systems should ensure that functions error appropriately, rather than merely warning, in response to data from inappropriate spatial domains.\n\n\n\n\nSpatial Workflows, Packages, and Classes\nSpatial software encompasses an enormous diversity, yet workflows implemented by spatial software often share much in common. In particular, coordinate reference systems used to precisely relate pairs of coordinates to precise locations in a curvilinear space, and in particular to the Earth’s ellipsoid, need to be able to be compared and transformed regardless of the specificities of individual software. This ubiquitous need has fostered the development of the PROJ library for representing and transforming spatial coordinates. Several other libraries have been built on top or or alongside that, notably including the GDAL (“Geospatial Data Abstraction Library”) and GEOS (“Geometry Engine, Open Source”) libraries. These libraries are used by, and integrated within, most geographical spatial software commonly used today, and will likely continue to be used.\nWhile not a standard in itself, it is expected that spatial software should not, absent very convincing and explicit justification, attempt to reconstruct aspects of these generic libraries. Given that, the following standards aim to ensure that spatial software remains as compatible as possible with workflows established by preceding packages which have aimed to expose and integrate as much of the functionality of these generic libraries as possible. The use of specific class systems for spatial data, and the workflows encapsulated in associated packages, ensures maximal ongoing compatibility with these libraries and with spatial workflows in general.\nNotable class systems and associated packages in R include sp, sf, and raster, and more recent extensions such as stars, terra, and s2. With regard to these packages, the following single standard applies, because the maintainer of sp has made it clear that new software should build upon sf, not sp.\n\n\nSP2.1 Spatial Software should not use the sp package, rather should use sf.\n\n\nMore generally,\n\n\nSP2.2 Geographical Spatial Software should ensure maximal compatibility with established packages and workflows, minimally through:\n\n\nSP2.2a Clear and extensive documentation demonstrating how routines from that software may be embedded within, or otherwise adapted to, workflows which rely on these established packages; and\n\n\nSP2.2b Tests which clearly demonstrate that routines from that software may be successfully translated into forms and workflows which rely on these established packages.\n\n\n\n\nThis standard is further refined in a number of subsequent standards concerning documentation and testing.\n\n\nSP2.3 Software which accepts spatial input data in any standard format established in other R packages (such as any of the formats able to be read by GDAL, and therefore by the sf package) should include example and test code which load those data in spatial formats, rather than R-specific binary formats such as .Rds.\n\n\nSee the sf vignette on “Reading, Writing and Converting Simple Features” for useful examples.\nCoordinate Reference Systems\nAs described above, one of the primary reasons for the development of classes in Spatial Software is to represent the coordinate reference systems in which data are represented, and to ensure compatibility with the PROJ system and other generic spatial libraries. The PROJ standards and associated software library have been recently (2020) updated (to version number 7) with “breaking changes” that are not backwards-compatible with previous versions, and in particular with the long-standing version 4. The details and implications of these changes within the context of spatial software in R can be examined in this blog entry on r-spatial.org, and in this vignette for the rgdal package. The “breaking” nature of these updates partly reflects analogous “breaking changes” associated with updates in the “Well-Known Text” (WKT) system for representing coordinate reference systems.\nThe following standard applies to software which directly or indirectly relies on geographic data which uses or relies upon coordinate reference systems.\n\n\nSP2.4 Geographical Spatial Software should be compliant with version 6 or larger of PROJ, and with WKT2 representations. The primary implication, described in detail in the articles linked to above, is that:\n\n\nSP2.4a Software should not permit coordinate reference systems to be represented merely by so-called “PROJ4-strings”, but should use at least WKT2.\n\n\n\n\nGeneral Input Structures\nNew spatial software may nevertheless eschew these prior packages and classes in favour of implementing new classes. Whether or not prior classes are used or expected, geographic software should accord as much as possible with the principles of these prior systems by according with the following standards:\n\n\nSP2.5 Class systems for input data must contain meta data on associated coordinate reference systems.\n\n\nSP2.5a Software which implements new classes to input spatial data (or the spatial components of more general data) should provide an ability to convert such input objects into alternative spatial classes such as those listed above.\n\n\n\n\nSP2.6 Spatial Software should explicitly document the types and classes of input data able to be passed to each function.\n\n\nSP2.7 Spatial Software should implement validation routines to confirm that inputs are of acceptable classes (or represented in otherwise appropriate ways for software which does not use class systems).\n\n\nSP2.8 Spatial Software should implement a single pre-processing routine to validate input data, and to appropriately transform it to a single uniform type to be passed to all subsequent data-processing functions.\n\n\nSP2.9 The pre-processing function described above should maintain those metadata attributes of input data which are relevant or important to core algorithms or return values.\n\n\n6.6.3 Algorithms\nThe following standards will be conditionally applicable to some but not all spatial software. Procedures for standards deemed not applicable to a particular piece of software are described in the srr package.\n\n\nSP3.0 Spatial software which considers spatial neighbours should enable user control over neighbourhood forms and sizes. In particular:\n\n\nSP3.0a Neighbours (able to be expressed) on regular grids should be able to be considered in both rectangular only, or rectangular and diagonal (respectively “rook” and “queen” by analogy to chess).\n\n\nSP3.0b Neighbourhoods in irregular spaces should be minimally able to be controlled via an integer number of neighbours, an area (or equivalent distance defining an area) in which to include neighbours, or otherwise equivalent user-controlled value.\n\n\n\n\nSP3.1 Spatial software which considers spatial neighbours should wherever possible enable neighbour contributions to be weighted by distance (or other continuous weighting variable), and not rely exclusively on a uniform-weight rectangular cut-off.\n\n\nSP3.2 Spatial software which relies on sampling from input data (even if only of spatial coordinates) should enable sampling procedures to be based on local spatial densities of those input data.\n\n\nAn example of software which would not adhere to SP3.2 would be where input data were a simple matrix of spatial coordinates, and sampling were implemented using the sample() function to randomly select elements of those input data (like sample(nrow(xy), n)). In the context of an example based on the sample() function, adhering to the standard would require including an additional prob vector where each point was weighted by the local density of surrounding points. Doing so would lead to higher probabilities of samples being taken from central clusters of higher densities than from outlying extreme points. Note that the standard merely suggests that software should enable such density-based samples to be taken, not that it must, or even necessarily should by default.\nAlgorithms for spatial software are often related to other categories of statistical software, and it is anticipated that spatial software will commonly also be subject to standards from these other categories. Nevertheless, because spatial analyses frequently face unique challenges, some of these category-specific standards also have extension standards when applied to spatial software. The following standards will be applicable for any spatial software which also fits any of the other listed categories of statistical software.\nRegression Software\n\n\nSP3.3 Spatial regression software should explicitly quantify and distinguish autocovariant or autoregressive processes from those covariant or regressive processes not directly related to spatial structure alone.\n\n\nUnsupervised Learning Software\nThe following standard applies to any spatial unsupervised learning software which uses clustering algorithms.\n\n\nSP3.4 Where possible, spatial clustering software should avoid using standard non-spatial clustering algorithms in which spatial proximity is merely represented by an additional weighting factor in favour of explicitly spatial algorithms.\n\n\nMachine Learning Software\nOne common application in which machine learning algorithms are applied to spatial software is in analyses of raster images. The first of the following standards applies because the individual cells or pixels of these raster images represent fixed spatial coordinates. (This standard also renders ML2.1 inapplicable).\n\n\nSP3.5 Spatial machine learning software should ensure that broadcasting procedures for reconciling inputs of different dimensions are not applied.\n\nA definition of broadcasting is given at the end of the introduction to corresponding Machine Learning Standards, just above Input Data Specification.\n\n\nSP3.6 Spatial machine learning software should document (and, where possible, test) the potential effects of different sampling procedures\n\n\nA simple example might be to provide examples or extended documentation which compares the effects of sampling both test and training data from the same spatial region versus sampling them from distinct regions. Although there are no comparable General Standard for Machine Learning Software, procedures for sampling spatial data may have particularly pronounced effects on results, and this standard attempts to foster a “best practice” of documenting how such effects may arise with a given piece of software.\nA more concrete example may be to demonstrate a particular technique for generating distinct test and training data such as spatial partitioning (Muenchow n.d.; Brenning 2012; Schratz et al. 2019; Valavi et al. 2019). There may nevertheless be cases in which such sampling from a common spatial region is appropriate, for example for software intended to analyse or model temporally-structured spatial data for which a more appropriate distinction might be temporal rather than spatial. Adherence to this standard merely requires that the potential for any such confounding effects be explicitly documented (and possibly tested as well).\n\n6.6.4 Return Results\nFor (functions within) Spatial Software which return spatial data:\n\n\nSP4.0 Return values should either:\n\n\nSP4.0a Be in same class as input data, or\n\n\nSP4.0b Be in a unique, preferably class-defined, format.\n\n\n\n\nSP4.1 Any aspects of input data which are included in output data (either directly, or in some transformed form) and which contain units should ensure those same units are maintained in return values.\n\n\nSP4.2 The type and class of all return values should be explicitly documented.\n\n\n6.6.5 Visualization\nSpatial Software which returns objects in a custom class structure explicitly designed to represent or include spatial data should:\n\n\nSP5.0 Implement default plot methods for any implemented class system.\n\n\nSP5.1 Implement appropriate placement of variables along x- and y-axes.\n\n\nSP5.2 Ensure that axis labels include appropriate units.\n\n\nAn example of SP5.1 might be ensuring that longitude is placed on the x-axis, latitude on the y, although standard orientations may depend on coordinate reference systems and other aspects of data and software design. The preceding three standards will generally not apply to software which returns objects in a custom class structure yet which is not inherently spatial.\nSpatial Software which returns objects with geographical coordinates should:\n\n\nSP5.3 Offer an ability to generate interactive (generally html-based) visualisations of results.\n\n\n6.6.6 Testing\nThe following standards apply to all Spatial Software which is intended or able to be applied to data represented in curvilinear systems, notably including all geographical data. The only Spatial Software to which the following standards do not (necessarily) apply would be software explicitly intended to be applied exclusively to Cartesian spatial data, and which ensured appropriate rejection of curvilinear data according to SP2.0b.\nRound-Trip Tests\n\n\nSP6.0 Software which implements routines for transforming coordinates of input data should include tests which demonstrate ability to recover the original coordinates.\n\n\nThis standard is applicable to any software which implements any routines for coordinate transformations, even if those routines are implemented via PROJ. Conversely, software which has no routines for coordinate transformations need not adhere to SP6.0, even if that software relies on PROJ for other purposes.\n\n\nSP6.1 All functions which can be applied to both Cartesian and curvilinear data should be tested through application to both.\n\n\nSP6.1a Functions which may yield inaccurate results when applied to data in one or the other forms (such as the preceding examples of centroids and buffers from ellipsoidal data) should test that results from inappropriate application of those functions are indeed less accurate.\n\n\nSP6.1b Functions which yield accurate results regardless of whether input data are rectilinear or curvilinear should demonstrate equivalent accuracy in both cases, and should also demonstrate how equivalent results may be obtained through first explicitly transforming input data.\n\n\n\n\nExtreme Geographical Coordinates\n\n\nSP6.2 Geographical Software should include tests with extreme geographical coordinates, minimally including extension to polar extremes of +/-90 degrees.\n\n\nWhile such tests should generally confirm that software generates reliable results to such extreme coordinates, software which is unable to generate reliable results to such inputs should nevertheless include tests to indicate both approximate bounds of reliability, and the expected characteristics of unreliable results.\nThe remaining standards for testing Spatial Software extend directly from the preceding Algorithmic Standards (SP3), with the same sub-section headings used here.\n\n\nSP6.3 Spatial Software which considers spatial neighbours should explicitly test all possible ways of defining them, and should explicitly compare quantitative effects of different ways of defining neighbours.\n\n\nSP6.4 Spatial Software which considers spatial neighbours should explicitly test effects of different schemes to weight neighbours by spatial proximity.\n\n\nUnsupervised Learning Software\n\n\nSP6.5 Spatial Unsupervised Learning Software which uses clustering algorithms should implement tests which explicitly compare results with equivalent results obtained with a non-spatial clustering algorithm.\n\n\nMachine Learning Software\n\n\nSP6.6 *Spatial Machine Learning Software should implement tests which explicitly demonstrate the detrimental consequences of sampling test and training data from the same spatial region, rather than from spatially distinct regions.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-time-series",
    "href": "standards.html#standards-time-series",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.7 Time Series Software",
    "text": "6.7 Time Series Software\nThe category of Time Series software is arguably easier to define than the preceding categories, and represents any software the primary input of which is intended to be temporally structured data. Importantly, while “temporally structured” may often imply temporally ordered, this need not necessarily be the case. The primary definition of temporally structured data is that they possess some kind of index which can be used to extract temporal relationships.\nTime series software is presumed to perform one or more of the following steps:\n\nAccept and validate input data\nApply data transformation and pre-processing steps\nApply one or more analytic algorithms\nReturn the result of that algorithmic application\nOffer additional functionality such as printing or summarising return results\n\nThis document details standards for each of these steps, each prefixed with “TS”.\n\n6.7.1 Input data structures and validation\nInput validation is an important software task, and an important part of our standards. While there are many ways to approach validation, the class systems of R offer a particularly convenient and effective means. For Time Series Software in particular, a range of class systems have been developed, for which we refer to the section “Time Series Classes” in the CRAN Task view on Time Series Analysis”, and the class-conversion package tsbox. Software which uses and relies on defined classes can often validate input through affirming appropriate class(es). Software which does not use or rely on class systems will generally need specific routines to validate input data structures. In particular, because of the long history of time series software in R, and the variety of class systems for representing time series data, new time series packages should accept as many different classes of input as possible by according with the following standards:\n\n\nTS1.0 Time Series Software should use and rely on explicit class systems developed for representing time series data, and should not permit generic, non-time-series input\n\n\nThe core algorithms of time-series software are often ultimately applied to simple vector objects, and some time series software accepts simple vector inputs, assuming these to represent temporally sequential data. Permitting such generic inputs nevertheless prevents any such assumptions from being asserted or tested. Missing values pose particular problems in this regard. A simple na.omit() call or similar will shorten the length of the vector by removing any NA values, and will change the explicit temporal relationship between elements. The use of explicit classes for time series generally ensures an ability to explicitly assert properties such as strict temporal regularity, and to control for any deviation from expected properties.\n\n\nTS1.1 Time Series Software should explicitly document the types and classes of input data able to be passed to each function.\n\n\nSuch documentation should include a demonstration of how to input data in at least one commonly used class for time-series such as ts.\n\n\nTS1.2 Time Series Software should implement validation routines to confirm that inputs are of acceptable classes (or represented in otherwise appropriate ways for software which does not use class systems).\n\n\nTS1.3 Time Series Software should implement a single pre-processing routine to validate input data, and to appropriately transform it to a single uniform type to be passed to all subsequent data-processing functions (the tsbox package provides one convenient approach for this).\n\n\nTS1.4 The pre-processing function described above should maintain all time- or date-based components or attributes of input data.\n\n\nFor Time Series Software which relies on or implements custom classes or types for representing time-series data, the following standards should be adhered to:\n\n\nTS1.5 The software should ensure strict ordering of the time, frequency, or equivalent ordering index variable.\n\n\nTS1.6 Any violations of ordering should be caught in the pre-processing stages of all functions.\n\n\n\n6.7.1.1 Time Intervals and Relative Time\nWhile most common packages and classes for time series data assume absolute temporal scales such as those represented in POSIX classes for dates or times, time series may also be quantified on relative scales where the temporal index variable quantifies intervals rather than absolute times or dates. Many analytic routines which accept time series inputs in absolute form are also appropriately applied to analogous data in relative form, and thus many packages should accept time series inputs both in absolute and relative forms. Software which can or should accept times series inputs in relative form should:\n\n\nTS1.7 Accept inputs defined via the units package for attributing SI units to R vectors.\n\n\nTS1.8 Where time intervals or periods may be days or months, be explicit about the system used to represent such, particularly regarding whether a calendar system is used, or whether a year is presumed to have 365 days, 365.2422 days, or some other value.\n\n\n6.7.2 Pre-processing and Variable Transformation\n\n6.7.2.1 Missing Data\nOne critical pre-processing step for Time Series Software is the appropriate handling of missing data. It is convenient to distinguish between implicit and explicit missing data. For regular time series, explicit missing data may be represented by NA values, while for irregular time series, implicit missing data may be represented by missing rows. The difference is demonstrated in the following table.\n\n\nMissing Values\n\n\n\n\nTime\n\n\nvalue\n\n\n\n\n08:43\n\n\n0.71\n\n\n\n\n08:44\n\n\nNA\n\n\n\n\n08:45\n\n\n0.28\n\n\n\n\n08:47\n\n\n0.34\n\n\n\n\n08:48\n\n\n0.07\n\n\n\n\nThe value for 08:46 is implicitly missing, while the value for 08:44 is explicitly missing. These two forms of missingness may connote different things, and may require different forms of pre-processing. With this in mind, and beyond the General Standards for missing data (G2.13–G2.16), the following standards apply:\n\n\nTS2.0 Time Series Software which presumes or requires regular data should only allow explicit missing values, and should issue appropriate diagnostic messages, potentially including errors, in response to any implicit missing values.\n\n\nTS2.1 Where possible, all functions should provide options for users to specify how to handle missing data, with options minimally including:\n\n\nTS2.1a *error on missing data; or.\n\nTS2.1b warn or ignore missing data, and proceed to analyse irregular data, ensuring that results from function calls with regular yet missing data return identical values to submitting equivalent irregular data with no missing values; or\n\n\nTS2.1c replace missing data with appropriately imputed values.\n\n\n\n\nThis latter standard is a modified version of General Standard G2.14, with additional requirements via TS2.1b.\n\n6.7.2.2 Stationarity\nTime Series Software should explicitly document assumptions or requirements made with respect to the stationarity or otherwise of all input data. In particular, any (sub-)functions which assume or rely on stationarity should:\n\n\nTS2.2 *Consider stationarity of all relevant moments\n\ntypically first (mean) and second (variance) order, or otherwise document why such consideration may be restricted to lower orders only.*\n\n\n\nTS2.3 Explicitly document all assumptions and/or requirements of stationarity\n\n\nTS2.4 Implement appropriate checks for all relevant forms of stationarity, and either:\n\n\nTS2.4a issue diagnostic messages or warnings; or\n\n\nTS2.4b enable or advise on appropriate transformations to ensure stationarity.\n\n\n\n\nThe two options in the last point (TS2.4b) respectively translate to enabling transformations to ensure stationarity by providing appropriate routines, generally triggered by some function parameter, or advising on appropriate transformations, for example by directing users to additional functions able to implement appropriate transformations.\n\n6.7.2.3 Auto-Covariance Matrices\nWhere auto-covariance matrices are constructed or otherwise used within or as input to functions, they should:\n\n\nTS2.5 Incorporate a system to ensure that both row and column orders follow the same ordering as the underlying time series data. This may, for example, be done by including the index attribute of the time series data as an attribute of the auto-covariance matrix.\n\n\nTS2.6 Where applicable, auto-covariance matrices should also include specification of appropriate units.\n\n\nGeneral Standard G3.1 also applies to all Time Series Software which constructs or uses auto-covariance matrices.\n\n6.7.3 Analytic Algorithms\nAnalytic algorithms are considered here to reflect the core analytic components of Time Series Software. These may be many and varied, and we explicitly consider only a small subset here.\n\n6.7.3.1 Forecasting\nStatistical software which implements forecasting routines should:\n\n\nTS3.0 Provide tests to demonstrate at least one case in which errors widen appropriately with forecast horizon.\n\n\nTS3.1 If possible, provide at least one test which violates TS3.0\n\n\nTS3.2 Document the general drivers of forecast errors or horizons, as demonstrated via the particular cases of TS3.0 and TS3.1\n\n\nTS3.3 Either:\n\n\nTS3.3a Document, preferable via an example, how to trim forecast values based on a specified error margin or equivalent; or\n\n\nTS3.3b Provide an explicit mechanism to trim forecast values to a specified error margin, either via an explicit post-processing function, or via an input parameter to a primary analytic function.\n\n\n\n\n6.7.4 Return Results\nFor (functions within) Time Series Software which return time series data:\n\n\nTS4.0 Return values should either:\n\n\nTS4.0a Be in same class as input data, for example by using the tsbox package to re-convert from standard internal format (see 1.4, above); or\n\n\nTS4.0b Be in a unique, preferably class-defined, format.\n\n\n\n\nTS4.1 Any units included as attributes of input data should also be included within return values.\n\n\nTS4.2 The type and class of all return values should be explicitly documented.\n\n\nFor (functions within) Time Series Software which return data other than direct series:\n\n\nTS4.3 Return values should explicitly include all appropriate units and/or time scales\n\n\n\n6.7.4.1 Data Transformation\nTime Series Software which internally implements routines for transforming data to achieve stationarity and which returns forecast values should:\n\n\nTS4.4 Document the effect of any such transformations on forecast data, including potential effects on both first- and second-order estimates.\n\n\nTS4.5 In decreasing order of preference, either:\n\n\nTS4.5a Provide explicit routines or options to back-transform data commensurate with original, non-stationary input data\n\n\nTS4.5b Demonstrate how data may be back-transformed to a form commensurate with original, non-stationary input data.\n\n\nTS4.5c Document associated limitations on forecast values\n\n\n\n\n6.7.4.2 Forecasting\nWhere Time Series Software implements or otherwise enables forecasting abilities, it should return one of the following three kinds of information. These are presented in decreasing order of preference, such that software should strive to return the first kind of object, failing that the second, and only the third as a last resort.\n\n\nTS4.6 Time Series Software which implements or otherwise enables forecasting should return either:\n\n\nTS4.6a A distribution object, for example via one of the many packages described in the CRAN Task View on Probability Distributions (or the new distributional package as used in the fable package for time-series forecasting).\n\n\nTS4.6b For each variable to be forecast, predicted values equivalent to first- and second-order moments (for example, mean and standard error values).\n\n\nTS4.6c Some more general indication of error associated with forecast estimates.\n\n\n\n\nBeyond these particular standards for return objects, Time Series Software which implements or otherwise enables forecasting should:\n\n\nTS4.7 Ensure that forecast (modelled) values are clearly distinguished from observed (model or input) values, either (in this case in no order of preference) by\n\n\nTS4.7a Returning forecast values alone\n\n\nTS4.7b Returning distinct list items for model and forecast values\n\n\nTS4.7c Combining model and forecast values into a single return object with an appropriate additional column clearly distinguishing the two kinds of data.\n\n\n\n\n6.7.5 Visualization\nTime Series Software should:\n\n\nTS5.0 Implement default plot methods for any implemented class system.\n\n\nTS5.1 When representing results in temporal domain(s), ensure that one axis is clearly labelled “time” (or equivalent), with continuous units.\n\n\nTS5.2 Default to placing the “time” (or equivalent) variable on the horizontal axis.\n\n\nTS5.3 Ensure that units of the time, frequency, or index variable are printed by default on the axis.\n\n\nTS5.4 For frequency visualization, abscissa spanning \\([-\\pi, \\pi]\\) should be avoided in favour of positive units of \\([0, 2\\pi]\\) or \\([0, 0.5]\\), in all cases with appropriate additional explanation of units.\n\n\nTS5.5 Provide options to determine whether plots of data with missing values should generate continuous or broken lines.\n\n\nFor the results of forecast operations, Time Series Software should\n\n\nTS5.6 By default indicate distributional limits of forecast on plot\n\n\nTS5.7 By default include model (input) values in plot, as well as forecast (output) values\n\n\nTS5.8 By default provide clear visual distinction between model (input) values and forecast (output) values.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-unsupervised",
    "href": "standards.html#standards-unsupervised",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.8 Dimensionality Reduction, Clustering, and Unsupervised Learning",
    "text": "6.8 Dimensionality Reduction, Clustering, and Unsupervised Learning\nThis sub-section details standards for Dimensionality Reduction, Clustering, and Unsupervised Learning Software – referred to from here on for simplicity as “Unsupervised Learning Software”. Software in this category is distinguished from Regression Software though the latter aiming to construct or analyse one or more mappings between two defined data sets (for example, a set of “independent” data, \\(X\\), and a set of “dependent” data, “Y”), whereas Unsupervised Learning Software aims to construct or analyse one or more mappings between a defined set of input or independent data, and a second set of “output” data which are not necessarily known or given prior to the analysis. A key distinction in Unsupervised Learning Software and Algorithms is between that for which output data represent (generally numerical) transformations of the input data set, and that for which output data are discrete labels applied to the input data. Examples of the former type include dimensionality reduction and ordination software and algorithms, and examples of the latter include clustering and discrete partitioning software and algorithms.\nSome examples of Dimensionality Reduction, Clustering, and Unsupervised Learning software include:\n\n\nivis implements a dimensionality reduction technique using a “Siamese Neural Network architecture.\n\ntsfeaturex is a package to automate “time series feature extraction,” which also provides an example of a package for which both input and output data are generally incomparable with most other packages in this category.\n\niRF is another example of a generally incomparable package within this category, here one for which the features extracted are the most distinct predictive features extracted from repeated iterations of random forest algorithms.\n\ncompboost is a package for component-wise gradient boosting which may be sufficient general to potentially allow general application to problems addressed by several packages in this category.\nThe iml package may offer usable functionality for devising general assessments of software within this category, through offering a “toolbox for making machine learning models interpretable” in a “model agnostic” way.\n\nClick on the following link to view a demonstration Application of Dimensionality Reduction, Clustering, and Unsupervised Learning Standards.\n\n6.8.1 Input Data Structures and Validation\n\n\nUL1.0 Unsupervised Learning Software should explicitly document expected format (types or classes) for input data, including descriptions of types or classes which are not accepted; for example, specification that software accepts only numeric inputs in vector or matrix form, or that all inputs must be in data.frame form with both column and row names.\n\n\nUL1.1 Unsupervised Learning Software should provide distinct sub-routines to assert that all input data is of the expected form, and issue informative error messages when incompatible data are submitted.\n\n\nThe following code demonstrates an example of a routine from the base stats package which fails to meet this standard.\n\n#&gt; Error in if (is.na(n) || n &gt; 65536L) stop(\"size cannot be NA nor exceed 65536\"): missing value where TRUE/FALSE needed\n\nThe latter fails, yet issues an uninformative error message that clearly indicates a failure to provide sufficient checks on the class of input data.\n\n\nUL1.2 Unsupervised learning which uses row or column names to label output objects should assert that input data have non-default row or column names, and issue an informative message when these are not provided.\n\n\nSuch messages need not necessarily be provided by default, but should at least be optionally available.\n\nClick here for examples of checks for whether row and column names have generic default values.\n\n\nThe data.frame function inserts default row and column names where these are not explicitly specified.\n\n#&gt;   X1 X2\n#&gt; 1  1  6\n#&gt; 2  2  7\n#&gt; 3  3  8\n#&gt; 4  4  9\n#&gt; 5  5 10\n\nGeneric row names are almost always simple integer sequences, which the following condition confirms.\n\n#&gt; [1] TRUE\n\nGeneric column names may come in a variety of formats. The following code uses a grep expression to match any number of characters plus an optional leading zero followed by a generic sequence of column numbers, appropriate for matching column names produced by generic construction of data.frame objects.\n\n#&gt; [1] TRUE\n\nMessages should be issued in both of these cases.\n\n\nThe following code illustrates that the hclust function does not implement any such checks or assertions, rather it silently returns an object with default labels.\n\n#&gt; [1] \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n\n\nUL1.3 Unsupervised Learning Software should transfer all relevant aspects of input data, notably including row and column names, and potentially information from other attributes(), to corresponding aspects of return objects.\n\n\nUL1.3a Where otherwise relevant information is not transferred, this should be explicitly documented.\n\n\n\n\nAn example of a function according with UL1.3 is stats::cutree()\n\n#&gt;    Alabama     Alaska    Arizona   Arkansas California   Colorado \n#&gt;          1          2          3          4          5          4\n\nThe row names of USArrests are transferred to the output object. In contrast, some routines from the cluster package do not comply with this standard:\n\n#&gt; [1] 1 2 3 4 3 4\n\nThe case labels are not appropriately carried through to the object returned by agnes() to enable them to be transferred within cutree(). (The labels are transferred to the object returned by agnes, just not in a way that enables cutree to inherit them.)\n\n\nUL1.4 Unsupervised Learning Software should document any assumptions made with regard to input data; for example assumptions about distributional forms or locations (such as that data are centred or on approximately equivalent distributional scales). Implications of violations of these assumptions should be both documented and tested, in particular:\n\n\nUL1.4a Software which responds qualitatively differently to input data which has components on markedly different scales should explicitly document such differences, and implications of submitting such data.\n\n\nUL1.4b Examples or other documentation should not use scale() or equivalent transformations without explaining why scale is applied, and explicitly illustrating and contrasting the consequences of not applying such transformations.\n\n\n\n\n6.8.2 Pre-processing and Variable Transformation\n\n\nUL2.0 Routines likely to give unreliable or irreproducible results in response to violations of assumptions regarding input data (see UL1.4) should implement pre-processing steps to diagnose potential violations, and issue appropriately informative messages, and/or include parameters to enable suitable transformations to be applied.\n\n\nExample of compliance with this standard are the documentation entries for the center and scale. parameters of the stats::prcomp() function.\n\n\nUL2.1 Unsupervised Learning Software should document any transformations applied to input data, for example conversion of label-values to factor, and should provide ways to explicitly avoid any default transformations (with error or warning conditions where appropriate).\n\n\nUL2.2 Unsupervised Learning Software which accepts missing values in input data should implement explicit parameters controlling the processing of missing values, ideally distinguishing NA or NaN values from Inf values.\n\n\nThis standard applies beyond General Standards G2.13–G2.16, through the additional requirement of implementing explicit parameters.\n\n\nUL2.3 Unsupervised Learning Software should implement pre-processing routines to identify whether aspects of input data are perfectly collinear.\n\n\n6.8.3 Algorithms\n\n6.8.3.1 Labelling\n\n\nUL3.0 Algorithms which apply sequential labels to input data (such as clustering or partitioning algorithms) should ensure that the sequence follows decreasing group sizes (so labels of “1”, “a”, or “A” describe the largest group, “2”, “b”, or “B” the second largest, and so on.)\n\n\nNote that the stats::cutree() function does not accord with this standard:\n\n#&gt; \n#&gt;  1  2  3  4  5  6  7  8  9 10 \n#&gt;  3  3  3  6  5 10  2  5  5  8\n\nThe cutree() function applies arbitrary integer labels to the groups, yet the order of labels is not related to the order of group sizes.\n\n\nUL3.1 Dimensionality reduction or equivalent algorithms which label dimensions should ensure that that sequences of labels follows decreasing “importance” (for example, eigenvalues or variance contributions).\n\n\nThe stats::prcomp function accords with this standard:\n\n#&gt; Importance of first k=5 (out of 21) components:\n#&gt;                              PC1       PC2       PC3       PC4       PC5\n#&gt; Standard deviation     2529.6298 2157.3434 1459.4839 551.68183 369.10901\n#&gt; Proportion of Variance    0.4591    0.3339    0.1528   0.02184   0.00977\n#&gt; Cumulative Proportion     0.4591    0.7930    0.9458   0.96764   0.97741\n\nThe proportion of variance explained by each component decreasing with increasing numeric labelling of the components.\n\n\nUL3.2 Unsupervised Learning Software for which input data does not generally include labels (such as array-like data with no row names) should provide an additional parameter to enable cases to be labelled.\n\n\n6.8.3.2 Prediction\n\n\nUL3.3 Where applicable, Unsupervised Learning Software should implement routines to predict the properties (such as numerical ordinates, or cluster memberships) of additional new data without re-running the entire algorithm.\n\n\nWhile many algorithms such as Hierarchical clustering can not (readily) be used to predict memberships of new data, other algorithms can nevertheless be applied to perform this task. The following demonstrates how the output of stats::hclust can be used to predict membership of new data using the class:knn() function. (This is intended to illustrate only one of many possible approaches.)\n\n#&gt; [1] 2 2 1 1 2\n#&gt; Levels: 1 2 3\n\nThe stats::prcomp() function implements its own predict() method which conforms to this standard:\n\n#&gt;                      PC1        PC2        PC3       PC4\n#&gt; North Carolina 165.17494 -30.693263 -11.682811  1.304563\n#&gt; Maryland       129.44401  -4.132644  -2.161693  1.258237\n#&gt; Ohio           -49.51994  12.748248   2.104966 -2.777463\n#&gt; Colorado        35.78896  14.023774  12.869816  1.233391\n#&gt; Georgia         41.28054  -7.203986   3.987152 -7.818416\n\n\n6.8.3.3 Group Distributions and Associated Statistics\nMany unsupervised learning algorithms serve to label, categorise, or partition data. Software which performs any of these tasks will commonly output some kind of labelling or grouping schemes. The above example of principal components illustrates that the return object records the standard deviations associated with each component:\n\n#&gt; Standard deviations (1, .., p=4):\n#&gt; [1] 83.732400 14.212402  6.489426  2.482790\n#&gt; \n#&gt; Rotation (n x k) = (4 x 4):\n#&gt;                 PC1         PC2         PC3         PC4\n#&gt; Murder   0.04170432 -0.04482166  0.07989066 -0.99492173\n#&gt; Assault  0.99522128 -0.05876003 -0.06756974  0.03893830\n#&gt; UrbanPop 0.04633575  0.97685748 -0.20054629 -0.05816914\n#&gt; Rape     0.07515550  0.20071807  0.97408059  0.07232502\n#&gt; Importance of components:\n#&gt;                            PC1      PC2    PC3     PC4\n#&gt; Standard deviation     83.7324 14.21240 6.4894 2.48279\n#&gt; Proportion of Variance  0.9655  0.02782 0.0058 0.00085\n#&gt; Cumulative Proportion   0.9655  0.99335 0.9991 1.00000\n\nSuch output accords with the following standard:\n\n\nUL3.4 Objects returned from Unsupervised Learning Software which labels, categorise, or partitions data into discrete groups should include, or provide immediate access to, quantitative information on intra-group variances or equivalent, as well as on inter-group relationships where applicable.\n\n\nThe above example of principal components is one where there are no inter-group relationships, and so that standard is fulfilled by providing information on intra-group variances alone. Discrete clustering algorithms, in contrast, yield results for which inter-group relationships are meaningful, and such relationships can generally be meaningfully provided. The hclust() routine, like many clustering routines, simply returns a scheme for devising an arbitrary number of clusters, and so can not meaningfully provide variances or relationships between such. The cutree() function, however, does yield defined numbers of clusters, yet devoid of any quantitative information on variances or equivalent.\n\n#&gt;  Named int [1:50] 1 1 1 2 1 2 3 1 4 2 ...\n#&gt;  - attr(*, \"names\")= chr [1:50] \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n\nCompare that with the output of a largely equivalent routine, the clara() function from the cluster package.\n\n#&gt;       size  max_diss   av_diss isolation\n#&gt;  [1,]    4 24.708298 14.284874 1.4837745\n#&gt;  [2,]    6 28.857755 16.759943 1.7329563\n#&gt;  [3,]    6 44.640565 23.718040 0.9677229\n#&gt;  [4,]    6 28.005892 17.382196 0.8442061\n#&gt;  [5,]    6 15.901258  9.363471 1.1037219\n#&gt;  [6,]    7 29.407822 14.817031 0.9080598\n#&gt;  [7,]    4 11.764353  6.781659 0.8165753\n#&gt;  [8,]    3  8.766984  5.768183 0.3547323\n#&gt;  [9,]    3 18.848077 10.101505 0.7176276\n#&gt; [10,]    5 16.477257  8.468541 0.6273603\n\nThat object contains information on dissimilarities between each observation and cluster medoids, which in the context of UL3.4 is “information on intra-group variances or equivalent”. Moreover, inter-group information is also available as the “silhouette” of the clustering scheme.\n\n6.8.4 Return Results\n\n\nUL4.0 Unsupervised Learning Software should return some form of “model” object, generally through using or modifying existing class structures for model objects, or creating a new class of model objects.\n\n\nUL4.1 Unsupervised Learning Software may enable an ability to generate a model object without actually fitting values. This may be useful for controlling batch processing of computationally intensive fitting algorithms.\n\n\nUL4.2 The return object from Unsupervised Learning Software should include, or otherwise enable immediate extraction of, all parameters used to control the algorithm used.\n\n\n\n6.8.4.1 Reporting Return Results\n\n\nUL4.3 Model objects returned by Unsupervised Learning Software should implement or appropriately extend a default print method which provides an on-screen summary of model (input) parameters and methods used to generate results. The print method may also summarise statistical aspects of the output data or results.\n\n\nUL4.3a The default print method should always ensure only a restricted number of rows of any result matrices or equivalent are printed to the screen.\n\n\n\n\nThe prcomp objects returned from the function of the same name include potential large matrices of component coordinates which are by default printed in their entirety to the screen. This is because the default print behaviour for most tabular objects in R (matrix, data.frame, and objects from the Matrix package, for example) is to print objects in their entirety (limited only by such options as getOption(\"max.print\"), which determines maximal numbers of printed objects, such as lines of data.frame objects). Such default behaviour ought be avoided, particularly in Unsupervised Learning Software which commonly returns objects containing large numbers of numeric entries.\n\n\nUL4.4 Unsupervised Learning Software should also implement summary methods for model objects which should summarise the primary statistics used in generating the model (such as numbers of observations, parameters of methods applied). The summary method may also provide summary statistics from the resultant model.\n\n\n6.8.5 Documentation\n\n6.8.6 Visualization\n\n\nUL6.0 Objects returned by Unsupervised Learning Software should have default plot methods, either through explicit implementation, extension of methods for existing model objects, through ensuring default methods work appropriately, or through explicit reference to helper packages such as factoextra and associated functions.\n\n\nUL6.1 Where the default plot method is NOT a generic plot method dispatched on the class of return objects (that is, through an S3-type plot.&lt;myclass&gt; function or equivalent), that method dispatch (or equivalent) should nevertheless exist in order to explicitly direct users to the appropriate function.\n\n\nUL6.2 Where default plot methods include labelling components of return objects (such as cluster labels), routines should ensure that labels are automatically placed to ensure readability, and/or that appropriate diagnostic messages are issued where readability is likely to be compromised (for example, through attempting to place too many labels).\n\n\n6.8.7 Testing\nUnsupervised Learning Software should test the following properties and behaviours:\n\n\nUL7.0 Inappropriate types of input data are rejected with expected error messages.\n\n\n\n6.8.7.1 Input Scaling\nThe following tests should be implement for Unsupervised Learning Software for which inputs are presumed or required to be scaled in any particular ways (such as having mean values of zero).\n\n\nUL7.1 Tests should demonstrate that violations of assumed input properties yield unreliable or invalid outputs, and should clarify how such unreliability or invalidity is manifest through the properties of returned objects.\n\n\n6.8.7.2 Output Labelling\nWith regard to labelling of output data, tests for Unsupervised Learning Software should:\n\n\nUL7.2 Demonstrate that labels placed on output data follow decreasing group sizes (UL3.0)\n\n\nUL7.3 *Demonstrate that labels on input data are propagated to, or may be recovered from, output data.\n\n6.8.7.3 Prediction\nWith regard to prediction, tests for Unsupervised Learning Software should:\n\n\nUL7.4 Demonstrate that submission of new data to a previously fitted model can generate results more efficiently than initial model fitting.\n\n\n6.8.7.4 Batch Processing\nFor Unsupervised Learning Software which implements batch processing routines:\n\n\nUL7.5 Batch processing routines should be explicitly tested, commonly via extended tests (see G4.10–G4.12).\n\n\nUL7.5a Tests of batch processing routines should demonstrate that equivalent results are obtained from direct (non-batch) processing.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "standards.html#standards-distributions",
    "href": "standards.html#standards-distributions",
    "title": "6  Standards: Version 0.2.0",
    "section": "\n6.9 Probability Distributions",
    "text": "6.9 Probability Distributions\nThis sub-section details standards for Software which represents, transforms, or otherwise processes probability distributions. Unlike most other categories of standards, packages which fit in this category will also generally be expected to fit into at least one other category of statistical software. Reflecting that expectation, standards for probability distributions will be expected to only pertain to some (potentially small) portion of code in any package.\nPackages which utilise distributional functions to extract uni- or multi-variate estimates as a final algorithmic step, for example to provide numeric probability estimates, are not considered probability distributions software, and are not required to comply with these standards.\nThese standards apply to any package which performs operations on probability distributions. Operations include, but are not limited to, transformation, representation, convolution, integration, inversion, fitting, or re-scaling. The definition of probability distributions software ultimately depends on the notion of an “operation,” and it is ultimately up to package authors, in conversation with reviewers, to decide whether or not these Probability Distribution Standards might apply. If in doubt, the same principle applies here as to all other categories of standards: If at least half of the following standards apply, or could conceivably be applied, to a package, then it should be considered a probability distributions package.\n\n6.9.1 Documentation\n\n\nPD1.0 Software should provide references justifying choice and usage of particular probability distributions.\n\n\nThis standard applies, for example, to all cases where results of some algorithm are assumed to comply with some “known” statistical distribution, and are accordingly transformed or summarised. Software should then provide references demonstrating that such distributional properties may indeed be assumed to apply. This standard will not apply to any routines for general processing of probability distributions.\n\n6.9.2 Packages for Representing Distributions\nThese standards encourage the use of packages for general representation of probability distributions, especially as this allows distributional assumptions to be readily tested, refined, and updated, rather than remaining hard-coded and effectively fixed. The CRAN Task View on Probability Distributions has a sub-section under the “Miscellaneous” heading on Unified interface to handle distributions. Packages mentioned in that sub-section include:\n\nThe core stats package distributed wtih base R;\nThe distr family of packages, which offer an extremely powerful and flexibility range of S4-class objects for representing and manipulating probability distributions;\nThe distributions3 and distributional packages for representing and manipulating probability distributions as S3 objects; and\nThe distr6 package for distributions as R6 objects.\n\nThe follow standard should be adhered to where possible:\n\n\nPD2.0 Where possible, software should represent probability distributions using a package for general representation.\n\n\nAny one package will generally only be able to fulfil either this or the preceding standard (PD1.0): it will either use a particular distribution, and thus need to adhere to PD1.0, or it will treat distributions more generally, and thus need to adhere to PD2.0.\n\n6.9.3 Algorithms\n\n\nPD3.0 Manipulation of probability distributions should very generally be analytic, with numeric manipulations only implemented with clear justification (ideally including references).\n\n\nAn exemplary discussion of conditions under which numeric manipulations may be considered is provided in the Analytical and Numerical Methods vignette of the distr6 package.\n\n\nPD3.1 Operations on probability distributions should generally be contained within separate functions which themselves accept the names of the distributions as one input parameter.\n\n\nThis standard enables assumptions on distributions to be readily tested and updated, and applies even to packages which use only one single and specific distribution in accordance with PD1.0. The names of distributions are generally best passed as single character values, processed via calls like do.call(get(dist_name), list(args)) (although many other approaches are also possible). This standard is also important for the testing standards which follow.\n\n6.9.3.1 Optimisation algorithms\nThe following standard applies to operations on probability distributions which require calls to optimisation algorithms such as optimize(), optim(), or any equivalent numerical optimisation routines from stats or other packages.\n\n\nPD3.2 Use of optimisation routines to estimate parameters from probability distributions should explicitly specify and explain values of all parameters, including all uses of default parameters.\n\n\nPD3.3 Return objects which include values generated from optimisation algorithms should include information on optimisation algorithm and performance, minimally including the name of the algorithm used, the convergence tolerance, and the number of iterations.\n\n\nSee below for additional testing standards which also apply to probability distribution packages which use optimisation algorithms.\n\n6.9.3.2 Integration algorithms\n\n\nPD3.4 Use of routines to integrate probability distributions should explicitly document conditions under which integrals are expected to remain stable, and ideally include pre-processing checks for potentially unstable behaviour.\n\n\nPD3.5 Integration routines should only rely on discrete summation where such use can be justified (for example, through providing a literature reference), in which case the following applies:\n\n\nPD3.5a Use of discrete summation to approximate integrals must demonstrate that the Reimann sum has a finite limit (or, equivalently, must explicitly describe the conditions under which the sum may be expected to be finite).\n\n\n\n\nSee below for additional testing standards which also apply to probability distribution packages which use integration algorithms.\n\n6.9.4 Fitting Distributions\nFitting distributions is an important component of many statistical analyses, yet R currently has only two packages for general distributional fitting: fitdistrplus and fitteR. The field of distributional fitting is currently in very active development, and there are no notably “stable” approaches nor widely-used algorithms. This is reflected in the almost complete lack of mention of distributional fitting in the CRAN Task View on Probability Distributions. The very last point in the current version of that Task View describes “Parameter Estimation”, and links to both of these packages.\nGiven this dynamically evolving nature of code and algorithms for distributional fitting, this book currently provides no standards for this aspect. We nevertheless encourage any authors using or implementing distributional fitting procedures to help develop standards, for which we recommend use of the GitHub discussions channel for these standards.\n\n6.9.5 Testing\nThe following standards refer and apply to functions which process probability distributions, meaning functions defined in accordance with PD2.1, above. Such functions are referred to in the following standards as probability distribution functions.\n\n\nPD4.0 The numeric outputs of probability distribution functions should be tested, not just output structures. These tests should generally be tests for numeric equality.\n\n\nNumeric equality should always be tested within a defined tolerance (see General Standard G3.0).\n\n\nPD4.1 Tests for numeric equality should compare the output of of probability distribution functions with the output of code which explicitly demonstrates how such values are derived (generally defined in the same location in test files).\n\n\nA test fulfilling this standard will thus serve the dual purpose of testing the numeric results of a probability distribution function, and enabling anybody reading the test file to understand how those numeric results are derived.\n\n\nPD4.2 All functions constructed in accordance with PD2.1 - that is, which use a fixed distribution, and which name that distribution as an input parameter - should be tested using at least two different distributions.\n\n\nA package may justifiably rely on one single kind of probability distribution. Adherence to this standard would then require that the function notionally accept one other distribution as well, with a test then reflecting an expectation that results generated with this alternative distribution will differ somehow.\n\n6.9.5.1 Testing Optimisation and Integration Algorithms\nThe following standards only apply to packages which use either optimisation or integration algorithms (or both), and so comply with PD3.2 and PD3_3 for optimisation, or with PD3.4 and PD3_5 for integration.\n\n\nPD4.3 Tests of optimisation or integration algorithms should compare default results with results generated with alternative values for every parameter, including all parameters for the chosen algorithm (whether exposed as function inputs or not).\n\n\nThe following applies to any procedures other than simple one-dimensional optimisation or integration via routines such as stats::optimize() or stats::integrate().\n\n\nPD4.4 Tests of optimisation or integration algorithms should compare equivalent results generated with at least one alternative algorithm.\n\n\nUse of the stats::optim() function, for example, would already meet this standard through complying with the previous PD4.3, because optim() includes a method parameter naming one of several available optimisation methods. Many optimisation and integration routines nevertheless implement a single method, in which case adherence to this standard would require testing results against equivalent results generated via at least one alternative method.\n\n\n\n\nBrenning, A. 2012. “Spatial Cross-Validation and Bootstrap for the Assessment of Prediction Rules in Remote Sensing: The R Package Sperrorest.” In 2012 IEEE International Geoscience and Remote Sensing Symposium, 5372–75. https://doi.org/10.1109/IGARSS.2012.6352393.\n\n\nMuenchow, Jannes, Jakub Nowosad. n.d. Chapter 11 Statistical Learning  Geocomputation with R. Accessed March 10, 2021. https://geocompr.robinlovelace.net/.\n\n\nSchratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and Alexander Brenning. 2019. “Hyperparameter Tuning and Performance Assessment of Statistical and Machine-Learning Algorithms Using Spatial Data.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002.\n\n\nValavi, Roozbeh, Jane Elith, José J. Lahoz‐Monfort, and Gurutzeta Guillera‐Arroita. 2019. “blockCV: An r Package for Generating Spatially or Environmentally Separated Folds for k-Fold Cross-Validation of Species Distribution Models.” Methods in Ecology and Evolution 10 (2): 225–32. https://doi.org/https://doi.org/10.1111/2041-210X.13107.",
    "crumbs": [
      "Standards",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Standards: Version {{< var version >}}</span>"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "\n7  Appendices\n",
    "section": "",
    "text": "7.1 Notes on Scope and the Python Statistical Ecosystem\nTwo factors may be usefully noted in this regard:\nThe relative importance of python is powerfully reflected in temporal trends from the stackoverflow developer survey from the previous three years, with results shown in the following graphic.\nPython is not only more used and more loved than R, but both statistics for python have consistently grown at a faster rate over the past three years as have equivalent statistics for R.\nBoth languages nevertheless have relative well-defined standards for software packaging, python via the Python Package Index (pypi), and R via CRAN. In contrast to CRAN, which runs its own checks on all packages on a daily basis, there are no automatic checks for pypi packages, and almost any form of package that minimally conforms to the standards may be submitted. This much lower effective barrier to entry likely partially contributes to the far greater numbers of pypi (656,580) than CRAN (22,395) packages.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "appendix.html#python",
    "href": "appendix.html#python",
    "title": "\n7  Appendices\n",
    "section": "",
    "text": "The potential number of python packages for statistical analyses is likely to be relatively more restricted than relative numbers of R packages. Taking as indicative presentations at the previous three Joint Statistical Meetings (JSMs; 2018-2020), no python packages were referred to in any abstract, while 32 R packages were presented, along with two meta-platforms for R packages. Presentations at the Symposium of Data Science and Statistics (SDSS) for 2018-19 similarly including numerous presentations of R packages, along with presentation of three python packages. It may accordingly be expected that potential expansion to include python packages will demand relatively very little time or effort compared with that devoted to R packages as the primary software scope.\nIn spite of the above, the community of python users is enormously greater, reflected in the currently 656,580 packages compared with 22,395 packages on CRAN, or over 29 times as many python packages. Similarly, 41.7% of all respondents to the 2019 stackoverflow developer survey nominated python as their most popular language, compared with only 5.8% who nominated R.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "appendix.html#appendix-categories",
    "href": "appendix.html#appendix-categories",
    "title": "\n7  Appendices\n",
    "section": "\n7.2 Empirical Derivation of Categories",
    "text": "7.2 Empirical Derivation of Categories\nWe attempted to derive a realistic categorisation through using empirical data from several sources of potential software submissions, including all apparently “statistical” R packages published in the Journal of Open Source Software (JOSS), packages published in the Journal of Statistical Software, software presented at the 2018 and 2019 Joint Statistical Meetings (JSM), and Symposia on Data Science and Statistics (SDSS), well as CRAN task views. We have also compiled a list of the descriptions of all packages rejected by rOpenSci as being out of current scope because of current inability to consider statistical packages, along with a selection of recent statistical R packages accepted by JOSS. (The full list of all R package published by JOSS can be viewed at https://joss.theoj.org/papers//in/R).\nWe allocated one or more key words (or phrases) to each abstract, and use the frequencies and inter-connections between these to inform the following categorisation are represented in the interactive graphic (also included in the Appendix), itself derived from analyses of abstracts from all statistical software submitted to both rOpenSci and JOSS. (Several additional analyses and graphical representations of these raw data are included an auxiliary github repository.) The primary nodes that emerge from these empirical analyses (with associated relative sizes in parentheses) are shown in the following table.\n\n\n\nMost frequent key words from all JOSS abstracts (N = 92) for statistical software. Proportions are scaled per abstract, with each abstract generally having multiple key words, and so sum of proportions exceeds one.\n\nn\nterm\nproportion\n\n\n\n1\nML\n0.133\n\n\n2\nstatistical indices and scores\n0.111\n\n\n3\nvisualization\n0.111\n\n\n4\ndimensionality reduction\n0.100\n\n\n5\nprobability distributions\n0.100\n\n\n6\nregression\n0.100\n\n\n7\nwrapper\n0.100\n\n\n8\nestimates\n0.089\n\n\n9\nMonte Carlo\n0.089\n\n\n10\nBayesian\n0.078\n\n\n11\ncategorical variables\n0.078\n\n\n12\nEDA\n0.078\n\n\n13\nnetworks\n0.078\n\n\n14\nsummary statistics\n0.067\n\n\n15\nsurvival\n0.067\n\n\n16\nworkflow\n0.067\n\n\n\n\n\nThe top key words and their inter-relationships within the main network diagram were used to distinguish the following primary categories representing all terms which appear in over 5% of all abstracts, along with the two additional categories of “spatial” and “education”. We have excluded the key word “Estimates” as being too generic to usefully inform standards, and have also collected a few strongly-connected terms into single categories.\n\n\n\nProposed categorisation of statistical software, with corresponding proportions of all JOSS software matching each category\n\n\n\n\n\n\n\nn\nterm\nproprtion\ncomment\n\n\n\n1\nBayesian & Monte Carlo\n0.167\n\n\n\n2\ndimensionality reduction & feature selection\n0.144\nCommonly as a result of ML algorithms\n\n\n3\nML\n0.133\n\n\n\n4\nregression/splines/interpolation\n0.133\nIncluding function data analysis\n\n\n5\nstatistical indices and scores\n0.111\nSoftware generally intended to produce specific indices or scores as statistical output\n\n\n6\nvisualization\n0.111\n\n\n\n7\nprobability distributions\n0.100\nIncluding kernel densities, likelihood estimates and estimators, and sampling routines\n\n\n8\nwrapper\n0.100\n\n\n\n9\nExploratory Data Analysis (EDA)\n0.078\nIncluding information statistics such as Akaike’s criterion, and techniques such as random forests. Often related to workflow software.\n\n\n10\ncategorical variables\n0.078\nIncluding latent variables, and those output from ML algorithms. Note also that method for dimensionality reduction (such as clustering) often transform data to categorical forms.\n\n\n11\nnetworks\n0.078\n\n\n\n12\nsummary statistics\n0.067\nPrimarily related in the empirical data to regression and survival analyses, yet clearly a distinct category of its own.\n\n\n13\nsurvival\n0.067\nstrongly related to EDA, yet differing in being strictly descriptive of software outputs whereas EDA may include routines to explore data inputs and other pre-output stages of analysis.\n\n\n14\nworkflow\n0.067\nOften related to EDA, and very commonly also to ML.\n\n\n15\nspatial\n0.033\nAlso an important intermediate node connecting several other nodes, yet defining its own distinct cluster reflecting a distinct area of expertise.\n\n\n16\neducation\n0.044\n\n\n\n\n\n\nThe full network diagram can then be reduced down to these categories only, with interconnections weighted by all first- and second-order interconnections between intermediate categories, to give the following, simplified diagram (in which “scores” denotes “statistical indices and scores”; with the diagram best inspected by dragging individual nodes to see their connections to others).\n\n\nWarning: `clusters()` was deprecated in igraph 2.0.0.\nℹ Please use `components()` instead.\n\n\n\n\n\n\n\n\nStandards considered under any of the ensuing categories must be developed with reference to inter-relationships between categories, and in particular to potential ambiguity within and between any categorisation. An example of such ambiguity, and of potential difficulties associated with categorisation, is the category of “network” software which appropriate describes the grapherator package (with accompanying JOSS paper) which is effectively a distribution generator for data represented in a particular format that happens to represent a graph; and three JSM presentations, one on network-based clustering of high-dimensional data, one on community structure in dynamic networks and one on Gaussian graphical models. Standards derived for network software must accommodate such diversity of applications, and must accommodate software for which the “network” category may pertain only to some relatively minor aspect, while the primary algorithms or routines may not be related to network software in any direct way.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "appendix.html#appendix-keywords",
    "href": "appendix.html#appendix-keywords",
    "title": "\n7  Appendices\n",
    "section": "\n7.3 Analysis of statistical software keywords",
    "text": "7.3 Analysis of statistical software keywords\nThe JOSS conducts its own peer review process, and publishes textual descriptions of accepted software. Each piece of software then has its own web page on the journal’s site, on which the text is presented as a compiled .pdf-format document, along with links to the open review, as well as to the software repository. The published document must be included within the software repository in a file named paper.md, which enables automatic extraction and analysis of these text descriptions of software. Rather than attempt a comprehensive, and unavoidably subjective, categorization of software, these textual descriptions were used to identify key words or phrases (hereafter, “keywords”) which encapsulated the purpose, function, or other general descriptive elements of each piece of software. Each paper generally yielded multiple keywords. Extracting these from all papers judged to be potentially in scope allowed for the construction of a network of topics, in which the nodes were the key words and phrases, and the connections between any pair of nodes reflected the number of times those two keywords co-occurred across all papers.\nWe extracted all papers accepted and published by JOSS (217 at the time of writing in early 2020), and manually determined which of these were broadly statistical, reducing the total to 92. We then read through the contents of each of these, and recorded as many keywords as possible for each paper. The resultant network is shown in the following interactive graphic, in which nodes are scaled by numbers of occurrences, and edges by numbers of co-occurrences. (Or click here for full-screen version with link to code.)\n\n\n\n\n\n\nSuch a network visualization enables immediate identification of more and less central concepts including, in our case, several that we may not otherwise have conceived of as having been potentially in scope. We then used this network to define our set of key “in scope” concepts. This figure also reveals that many of these keywords are somewhat “lower level” than the kinds of concepts we might otherwise have used to define scoping categories. For example, keywords such as “likelihood” or “probability” are not likely to be useful in defining actual categories of statistical software, yet they turned out to lie at the centres of relatively well-defined groups of related keywords.\nWe also examined the forms of both input and output data for each of the 92 pieces of software described in these JOSS papers, and constructed an additional graph directionally relating these different data formats.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "appendix.html#appendix-other-software-standards",
    "href": "appendix.html#appendix-other-software-standards",
    "title": "\n7  Appendices\n",
    "section": "\n7.4 Other Software Standards",
    "text": "7.4 Other Software Standards\nAmong the noteworthy instances of software standards, the following are particularly relevant:\n\nThe Core Infrastructure Initiative’s Best Practices Badge, which is granted to software meeting an extensive list of criteria. This list of criteria provides a singularly useful reference for software standards.\nThe Software Sustainability Institute’s Software Evaluation Guide, in particular their guide to Criteria-based software evaluation, which considers two primary categories of Usability and Sustainability and Maintainability, each of which is divided into numerous sub-categories. The guide identifies numerous concrete criteria for each sub-category, explicitly detailed below in order to provide an example of the kind of standards that might be adapted and developed for application to the present project.\nThe Transparent Statistics Guidelines, by the “HCI (Human Computer Interaction) Working Group”. While currently only in its beginning phases, that document aims to provide concrete guidance on “transparent statistical communication.” If its development continues, it is likely to provide useful guidelines on best practices for how statistical software produces and reports results.\nThe more technical considerations of the Object Management Group’s Automated Source Code CISQ Maintainability Measure (where CISQ refers to the Consortium for IT Software Quality). This guide describes a number of measures which can be automatically extracted and used to quantify the maintainability of source code. None of these measures are not already considered in one or both of the preceding two documents, but the identification of measures particularly amenable to automated assessment provides a particularly useful reference.\n\nThere is also rOpenSci’s guide on package development, maintenance, and peer review, which provides standards of this type for R packages, primarily within its first chapter. Another notable example is the tidyverse design guide, and the section on Conventions for R Modeling Packages which provides guidance for model-fitting APIs.\nSpecific standards for neural network algorithms have also been developed as part of a google 2019 Summer Of Code project, resulting in a dedicated R package, NNbenchmark, and accompanying results—their so-called “notebooks”—of applying their benchmarks to a suite of neural network packages.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendices</span>"
    ]
  },
  {
    "objectID": "appendix.html#bibliography",
    "href": "appendix.html#bibliography",
    "title": "\n7  Appendices\n",
    "section": "\n7.5 Bibliography",
    "text": "7.5 Bibliography\n\n\nBrenning, A. 2012. “Spatial Cross-Validation and Bootstrap for the\nAssessment of Prediction Rules in Remote Sensing: The\nR Package Sperrorest.” In 2012 IEEE\nInternational Geoscience and\nRemote Sensing Symposium,\n5372–75. https://doi.org/10.1109/IGARSS.2012.6352393.\n\n\nEstivill-Castro, Vladimir. 2002. “Why so Many Clustering\nAlgorithms: A Position Paper.” ACM SIGKDD Explorations\nNewsletter 4 (1): 65–75. https://doi.org/10.1145/568574.568575.\n\n\nMuenchow, Jannes, Jakub Nowosad. n.d. Chapter 11\nStatistical Learning \nGeocomputation with R. Accessed March 10,\n2021. https://geocompr.robinlovelace.net/.\n\n\nSchratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and\nAlexander Brenning. 2019. “Hyperparameter Tuning and Performance\nAssessment of Statistical and Machine-Learning Algorithms Using Spatial\nData.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002.\n\n\nValavi, Roozbeh, Jane Elith, José J. Lahoz‐Monfort, and Gurutzeta\nGuillera‐Arroita. 2019. “blockCV:\nAn r Package for Generating Spatially or Environmentally\nSeparated Folds for k-Fold Cross-Validation of Species Distribution\nModels.” Methods in Ecology and Evolution 10 (2):\n225–32. https://doi.org/https://doi.org/10.1111/2041-210X.13107.",
    "crumbs": [
      "Appendix",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Appendices</span>"
    ]
  }
]